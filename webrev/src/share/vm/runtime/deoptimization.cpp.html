<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/runtime/deoptimization.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2016, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "code/codeCache.hpp"
  28 #include "code/debugInfoRec.hpp"
  29 #include "code/nmethod.hpp"
  30 #include "code/pcDesc.hpp"
  31 #include "code/scopeDesc.hpp"
  32 #include "interpreter/bytecode.hpp"
  33 #include "interpreter/interpreter.hpp"
  34 #include "interpreter/oopMapCache.hpp"
  35 #include "memory/allocation.inline.hpp"
  36 #include "memory/oopFactory.hpp"
  37 #include "memory/resourceArea.hpp"
  38 #include "oops/method.hpp"
  39 #include "oops/objArrayOop.inline.hpp"
  40 #include "oops/oop.inline.hpp"
  41 #include "oops/fieldStreams.hpp"
  42 #include "oops/verifyOopClosure.hpp"
  43 #include "prims/jvmtiThreadState.hpp"
  44 #include "runtime/biasedLocking.hpp"
  45 #include "runtime/compilationPolicy.hpp"
  46 #include "runtime/deoptimization.hpp"
  47 #include "runtime/interfaceSupport.hpp"
  48 #include "runtime/sharedRuntime.hpp"
  49 #include "runtime/signature.hpp"
  50 #include "runtime/stubRoutines.hpp"
  51 #include "runtime/thread.hpp"
  52 #include "runtime/vframe.hpp"
  53 #include "runtime/vframeArray.hpp"
  54 #include "runtime/vframe_hp.hpp"
  55 #include "utilities/events.hpp"
  56 #include "utilities/xmlstream.hpp"
  57 
  58 #if INCLUDE_JVMCI
  59 #include "jvmci/jvmciRuntime.hpp"
  60 #include "jvmci/jvmciJavaClasses.hpp"
  61 #endif
  62 
  63 
  64 bool DeoptimizationMarker::_is_active = false;
  65 
  66 Deoptimization::UnrollBlock::UnrollBlock(int  size_of_deoptimized_frame,
  67                                          int  caller_adjustment,
  68                                          int  caller_actual_parameters,
  69                                          int  number_of_frames,
  70                                          intptr_t* frame_sizes,
  71                                          address* frame_pcs,
  72                                          BasicType return_type,
  73                                          int exec_mode) {
  74   _size_of_deoptimized_frame = size_of_deoptimized_frame;
  75   _caller_adjustment         = caller_adjustment;
  76   _caller_actual_parameters  = caller_actual_parameters;
  77   _number_of_frames          = number_of_frames;
  78   _frame_sizes               = frame_sizes;
  79   _frame_pcs                 = frame_pcs;
  80   _register_block            = NEW_C_HEAP_ARRAY(intptr_t, RegisterMap::reg_count * 2, mtCompiler);
  81   _return_type               = return_type;
  82   _initial_info              = 0;
  83   // PD (x86 only)
  84   _counter_temp              = 0;
  85   _unpack_kind               = exec_mode;
  86   _sender_sp_temp            = 0;
  87 
  88   _total_frame_sizes         = size_of_frames();
  89   assert(exec_mode &gt;= 0 &amp;&amp; exec_mode &lt; Unpack_LIMIT, "Unexpected exec_mode");
  90 }
  91 
  92 
  93 Deoptimization::UnrollBlock::~UnrollBlock() {
  94   FREE_C_HEAP_ARRAY(intptr_t, _frame_sizes);
  95   FREE_C_HEAP_ARRAY(intptr_t, _frame_pcs);
  96   FREE_C_HEAP_ARRAY(intptr_t, _register_block);
  97 }
  98 
  99 
 100 intptr_t* Deoptimization::UnrollBlock::value_addr_at(int register_number) const {
 101   assert(register_number &lt; RegisterMap::reg_count, "checking register number");
 102   return &amp;_register_block[register_number * 2];
 103 }
 104 
 105 
 106 
 107 int Deoptimization::UnrollBlock::size_of_frames() const {
 108   // Acount first for the adjustment of the initial frame
 109   int result = _caller_adjustment;
 110   for (int index = 0; index &lt; number_of_frames(); index++) {
 111     result += frame_sizes()[index];
 112   }
 113   return result;
 114 }
 115 
 116 
 117 void Deoptimization::UnrollBlock::print() {
 118   ttyLocker ttyl;
 119   tty-&gt;print_cr("UnrollBlock");
 120   tty-&gt;print_cr("  size_of_deoptimized_frame = %d", _size_of_deoptimized_frame);
 121   tty-&gt;print(   "  frame_sizes: ");
 122   for (int index = 0; index &lt; number_of_frames(); index++) {
 123     tty-&gt;print(INTX_FORMAT " ", frame_sizes()[index]);
 124   }
 125   tty-&gt;cr();
 126 }
 127 
 128 
 129 // In order to make fetch_unroll_info work properly with escape
 130 // analysis, The method was changed from JRT_LEAF to JRT_BLOCK_ENTRY and
 131 // ResetNoHandleMark and HandleMark were removed from it. The actual reallocation
 132 // of previously eliminated objects occurs in realloc_objects, which is
 133 // called from the method fetch_unroll_info_helper below.
 134 JRT_BLOCK_ENTRY(Deoptimization::UnrollBlock*, Deoptimization::fetch_unroll_info(JavaThread* thread, int exec_mode))
 135   // It is actually ok to allocate handles in a leaf method. It causes no safepoints,
 136   // but makes the entry a little slower. There is however a little dance we have to
 137   // do in debug mode to get around the NoHandleMark code in the JRT_LEAF macro
 138 
 139   // fetch_unroll_info() is called at the beginning of the deoptimization
 140   // handler. Note this fact before we start generating temporary frames
 141   // that can confuse an asynchronous stack walker. This counter is
 142   // decremented at the end of unpack_frames().
 143   if (TraceDeoptimization) {
 144     tty-&gt;print_cr("Deoptimizing thread " INTPTR_FORMAT, p2i(thread));
 145   }
 146   thread-&gt;inc_in_deopt_handler();
 147 
 148   return fetch_unroll_info_helper(thread, exec_mode);
 149 JRT_END
 150 
 151 
 152 // This is factored, since it is both called from a JRT_LEAF (deoptimization) and a JRT_ENTRY (uncommon_trap)
 153 Deoptimization::UnrollBlock* Deoptimization::fetch_unroll_info_helper(JavaThread* thread, int exec_mode) {
 154 
 155   // Note: there is a safepoint safety issue here. No matter whether we enter
 156   // via vanilla deopt or uncommon trap we MUST NOT stop at a safepoint once
 157   // the vframeArray is created.
 158   //
 159 
 160   // Allocate our special deoptimization ResourceMark
 161   DeoptResourceMark* dmark = new DeoptResourceMark(thread);
 162   assert(thread-&gt;deopt_mark() == NULL, "Pending deopt!");
 163   thread-&gt;set_deopt_mark(dmark);
 164 
 165   frame stub_frame = thread-&gt;last_frame(); // Makes stack walkable as side effect
 166   RegisterMap map(thread, true);
 167   RegisterMap dummy_map(thread, false);
 168   // Now get the deoptee with a valid map
 169   frame deoptee = stub_frame.sender(&amp;map);
 170   // Set the deoptee nmethod
 171   assert(thread-&gt;deopt_nmethod() == NULL, "Pending deopt!");
 172   thread-&gt;set_deopt_nmethod(deoptee.cb()-&gt;as_nmethod_or_null());
 173   bool skip_internal = thread-&gt;deopt_nmethod() != NULL &amp;&amp; !thread-&gt;deopt_nmethod()-&gt;compiler()-&gt;is_jvmci();
 174 
 175   if (VerifyStack) {
 176     thread-&gt;validate_frame_layout();
 177   }
 178 
 179   // Create a growable array of VFrames where each VFrame represents an inlined
 180   // Java frame.  This storage is allocated with the usual system arena.
 181   assert(deoptee.is_compiled_frame(), "Wrong frame type");
 182   GrowableArray&lt;compiledVFrame*&gt;* chunk = new GrowableArray&lt;compiledVFrame*&gt;(10);
 183   vframe* vf = vframe::new_vframe(&amp;deoptee, &amp;map, thread);
 184   while (!vf-&gt;is_top()) {
 185     assert(vf-&gt;is_compiled_frame(), "Wrong frame type");
 186     chunk-&gt;push(compiledVFrame::cast(vf));
 187     vf = vf-&gt;sender();
 188   }
 189   assert(vf-&gt;is_compiled_frame(), "Wrong frame type");
 190   chunk-&gt;push(compiledVFrame::cast(vf));
 191 
 192   ScopeDesc* trap_scope = chunk-&gt;at(0)-&gt;scope();
 193   Handle exceptionObject;
 194   if (trap_scope-&gt;rethrow_exception()) {
 195     if (PrintDeoptimizationDetails) {
 196       tty-&gt;print_cr("Exception to be rethrown in the interpreter for method %s::%s at bci %d", trap_scope-&gt;method()-&gt;method_holder()-&gt;name()-&gt;as_C_string(), trap_scope-&gt;method()-&gt;name()-&gt;as_C_string(), trap_scope-&gt;bci());
 197     }
 198     GrowableArray&lt;ScopeValue*&gt;* expressions = trap_scope-&gt;expressions();
 199     guarantee(expressions != NULL &amp;&amp; expressions-&gt;length() &gt; 0, "must have exception to throw");
 200     ScopeValue* topOfStack = expressions-&gt;top();
 201     exceptionObject = StackValue::create_stack_value(&amp;deoptee, &amp;map, topOfStack)-&gt;get_obj();
 202     assert(exceptionObject() != NULL, "exception oop can not be null");
 203   }
 204 
 205   bool realloc_failures = false;
 206 
 207 #if defined(COMPILER2) || INCLUDE_JVMCI
 208   // Reallocate the non-escaping objects and restore their fields. Then
 209   // relock objects if synchronization on them was eliminated.
 210 #ifndef INCLUDE_JVMCI
 211   if (DoEscapeAnalysis || EliminateNestedLocks) {
 212     if (EliminateAllocations) {
 213 #endif // INCLUDE_JVMCI
 214       assert (chunk-&gt;at(0)-&gt;scope() != NULL,"expect only compiled java frames");
 215       GrowableArray&lt;ScopeValue*&gt;* objects = chunk-&gt;at(0)-&gt;scope()-&gt;objects();
 216 
 217       // The flag return_oop() indicates call sites which return oop
 218       // in compiled code. Such sites include java method calls,
 219       // runtime calls (for example, used to allocate new objects/arrays
 220       // on slow code path) and any other calls generated in compiled code.
 221       // It is not guaranteed that we can get such information here only
 222       // by analyzing bytecode in deoptimized frames. This is why this flag
 223       // is set during method compilation (see Compile::Process_OopMap_Node()).
 224       // If the previous frame was popped, we don't have a result.
 225       bool save_oop_result = chunk-&gt;at(0)-&gt;scope()-&gt;return_oop() &amp;&amp; !thread-&gt;popframe_forcing_deopt_reexecution();
 226       Handle return_value;
 227       if (save_oop_result) {
 228         // Reallocation may trigger GC. If deoptimization happened on return from
 229         // call which returns oop we need to save it since it is not in oopmap.
 230         oop result = deoptee.saved_oop_result(&amp;map);
 231         assert(result == NULL || result-&gt;is_oop(), "must be oop");
 232         return_value = Handle(thread, result);
 233         assert(Universe::heap()-&gt;is_in_or_null(result), "must be heap pointer");
 234         if (TraceDeoptimization) {
 235           ttyLocker ttyl;
 236           tty-&gt;print_cr("SAVED OOP RESULT " INTPTR_FORMAT " in thread " INTPTR_FORMAT, p2i(result), p2i(thread));
 237         }
 238       }
 239       if (objects != NULL) {
 240         JRT_BLOCK
 241           realloc_failures = realloc_objects(thread, &amp;deoptee, objects, THREAD);
 242         JRT_END
 243         reassign_fields(&amp;deoptee, &amp;map, objects, realloc_failures, skip_internal);
 244 #ifndef PRODUCT
 245         if (TraceDeoptimization) {
 246           ttyLocker ttyl;
 247           tty-&gt;print_cr("REALLOC OBJECTS in thread " INTPTR_FORMAT, p2i(thread));
 248           print_objects(objects, realloc_failures);
 249         }
 250 #endif
 251       }
 252       if (save_oop_result) {
 253         // Restore result.
 254         deoptee.set_saved_oop_result(&amp;map, return_value());
 255       }
 256 #ifndef INCLUDE_JVMCI
 257     }
 258     if (EliminateLocks) {
 259 #endif // INCLUDE_JVMCI
 260 #ifndef PRODUCT
 261       bool first = true;
 262 #endif
 263       for (int i = 0; i &lt; chunk-&gt;length(); i++) {
 264         compiledVFrame* cvf = chunk-&gt;at(i);
 265         assert (cvf-&gt;scope() != NULL,"expect only compiled java frames");
 266         GrowableArray&lt;MonitorInfo*&gt;* monitors = cvf-&gt;monitors();
 267         if (monitors-&gt;is_nonempty()) {
 268           relock_objects(monitors, thread, realloc_failures);
 269 #ifndef PRODUCT
 270           if (PrintDeoptimizationDetails) {
 271             ttyLocker ttyl;
 272             for (int j = 0; j &lt; monitors-&gt;length(); j++) {
 273               MonitorInfo* mi = monitors-&gt;at(j);
 274               if (mi-&gt;eliminated()) {
 275                 if (first) {
 276                   first = false;
 277                   tty-&gt;print_cr("RELOCK OBJECTS in thread " INTPTR_FORMAT, p2i(thread));
 278                 }
 279                 if (mi-&gt;owner_is_scalar_replaced()) {
 280                   Klass* k = java_lang_Class::as_Klass(mi-&gt;owner_klass());
 281                   tty-&gt;print_cr("     failed reallocation for klass %s", k-&gt;external_name());
 282                 } else {
 283                   tty-&gt;print_cr("     object &lt;" INTPTR_FORMAT "&gt; locked", p2i(mi-&gt;owner()));
 284                 }
 285               }
 286             }
 287           }
 288 #endif // !PRODUCT
 289         }
 290       }
 291 #ifndef INCLUDE_JVMCI
 292     }
 293   }
 294 #endif // INCLUDE_JVMCI
 295 #endif // COMPILER2 || INCLUDE_JVMCI
 296 
 297   // Ensure that no safepoint is taken after pointers have been stored
 298   // in fields of rematerialized objects.  If a safepoint occurs from here on
 299   // out the java state residing in the vframeArray will be missed.
 300   NoSafepointVerifier no_safepoint;
 301 
 302   vframeArray* array = create_vframeArray(thread, deoptee, &amp;map, chunk, realloc_failures);
 303 #if defined(COMPILER2) || INCLUDE_JVMCI
 304   if (realloc_failures) {
 305     pop_frames_failed_reallocs(thread, array);
 306   }
 307 #endif
 308 
 309   assert(thread-&gt;vframe_array_head() == NULL, "Pending deopt!");
 310   thread-&gt;set_vframe_array_head(array);
 311 
 312   // Now that the vframeArray has been created if we have any deferred local writes
 313   // added by jvmti then we can free up that structure as the data is now in the
 314   // vframeArray
 315 
 316   if (thread-&gt;deferred_locals() != NULL) {
 317     GrowableArray&lt;jvmtiDeferredLocalVariableSet*&gt;* list = thread-&gt;deferred_locals();
 318     int i = 0;
 319     do {
 320       // Because of inlining we could have multiple vframes for a single frame
 321       // and several of the vframes could have deferred writes. Find them all.
 322       if (list-&gt;at(i)-&gt;id() == array-&gt;original().id()) {
 323         jvmtiDeferredLocalVariableSet* dlv = list-&gt;at(i);
 324         list-&gt;remove_at(i);
 325         // individual jvmtiDeferredLocalVariableSet are CHeapObj's
 326         delete dlv;
 327       } else {
 328         i++;
 329       }
 330     } while ( i &lt; list-&gt;length() );
 331     if (list-&gt;length() == 0) {
 332       thread-&gt;set_deferred_locals(NULL);
 333       // free the list and elements back to C heap.
 334       delete list;
 335     }
 336 
 337   }
 338 
 339 #ifndef SHARK
 340   // Compute the caller frame based on the sender sp of stub_frame and stored frame sizes info.
 341   CodeBlob* cb = stub_frame.cb();
 342   // Verify we have the right vframeArray
 343   assert(cb-&gt;frame_size() &gt;= 0, "Unexpected frame size");
 344   intptr_t* unpack_sp = stub_frame.sp() + cb-&gt;frame_size();
 345 
 346   // If the deopt call site is a MethodHandle invoke call site we have
 347   // to adjust the unpack_sp.
 348   nmethod* deoptee_nm = deoptee.cb()-&gt;as_nmethod_or_null();
 349   if (deoptee_nm != NULL &amp;&amp; deoptee_nm-&gt;is_method_handle_return(deoptee.pc()))
 350     unpack_sp = deoptee.unextended_sp();
 351 
 352 #ifdef ASSERT
 353   assert(cb-&gt;is_deoptimization_stub() ||
 354          cb-&gt;is_uncommon_trap_stub() ||
 355          strcmp("Stub&lt;DeoptimizationStub.deoptimizationHandler&gt;", cb-&gt;name()) == 0 ||
 356          strcmp("Stub&lt;UncommonTrapStub.uncommonTrapHandler&gt;", cb-&gt;name()) == 0,
 357          "unexpected code blob: %s", cb-&gt;name());
 358 #endif
 359 #else
 360   intptr_t* unpack_sp = stub_frame.sender(&amp;dummy_map).unextended_sp();
 361 #endif // !SHARK
 362 
 363   // This is a guarantee instead of an assert because if vframe doesn't match
 364   // we will unpack the wrong deoptimized frame and wind up in strange places
 365   // where it will be very difficult to figure out what went wrong. Better
 366   // to die an early death here than some very obscure death later when the
 367   // trail is cold.
 368   // Note: on ia64 this guarantee can be fooled by frames with no memory stack
 369   // in that it will fail to detect a problem when there is one. This needs
 370   // more work in tiger timeframe.
 371   guarantee(array-&gt;unextended_sp() == unpack_sp, "vframe_array_head must contain the vframeArray to unpack");
 372 
 373   int number_of_frames = array-&gt;frames();
 374 
 375   // Compute the vframes' sizes.  Note that frame_sizes[] entries are ordered from outermost to innermost
 376   // virtual activation, which is the reverse of the elements in the vframes array.
 377   intptr_t* frame_sizes = NEW_C_HEAP_ARRAY(intptr_t, number_of_frames, mtCompiler);
 378   // +1 because we always have an interpreter return address for the final slot.
 379   address* frame_pcs = NEW_C_HEAP_ARRAY(address, number_of_frames + 1, mtCompiler);
 380   int popframe_extra_args = 0;
 381   // Create an interpreter return address for the stub to use as its return
 382   // address so the skeletal frames are perfectly walkable
 383   frame_pcs[number_of_frames] = Interpreter::deopt_entry(vtos, 0);
 384 
 385   // PopFrame requires that the preserved incoming arguments from the recently-popped topmost
 386   // activation be put back on the expression stack of the caller for reexecution
 387   if (JvmtiExport::can_pop_frame() &amp;&amp; thread-&gt;popframe_forcing_deopt_reexecution()) {
 388     popframe_extra_args = in_words(thread-&gt;popframe_preserved_args_size_in_words());
 389   }
 390 
 391   // Find the current pc for sender of the deoptee. Since the sender may have been deoptimized
 392   // itself since the deoptee vframeArray was created we must get a fresh value of the pc rather
 393   // than simply use array-&gt;sender.pc(). This requires us to walk the current set of frames
 394   //
 395   frame deopt_sender = stub_frame.sender(&amp;dummy_map); // First is the deoptee frame
 396   deopt_sender = deopt_sender.sender(&amp;dummy_map);     // Now deoptee caller
 397 
 398   // It's possible that the number of parameters at the call site is
 399   // different than number of arguments in the callee when method
 400   // handles are used.  If the caller is interpreted get the real
 401   // value so that the proper amount of space can be added to it's
 402   // frame.
 403   bool caller_was_method_handle = false;
 404   if (deopt_sender.is_interpreted_frame()) {
 405     methodHandle method = deopt_sender.interpreter_frame_method();
 406     Bytecode_invoke cur = Bytecode_invoke_check(method, deopt_sender.interpreter_frame_bci());
 407     if (cur.is_invokedynamic() || cur.is_invokehandle()) {
 408       // Method handle invokes may involve fairly arbitrary chains of
 409       // calls so it's impossible to know how much actual space the
 410       // caller has for locals.
 411       caller_was_method_handle = true;
 412     }
 413   }
 414 
 415   //
 416   // frame_sizes/frame_pcs[0] oldest frame (int or c2i)
 417   // frame_sizes/frame_pcs[1] next oldest frame (int)
 418   // frame_sizes/frame_pcs[n] youngest frame (int)
 419   //
 420   // Now a pc in frame_pcs is actually the return address to the frame's caller (a frame
 421   // owns the space for the return address to it's caller).  Confusing ain't it.
 422   //
 423   // The vframe array can address vframes with indices running from
 424   // 0.._frames-1. Index  0 is the youngest frame and _frame - 1 is the oldest (root) frame.
 425   // When we create the skeletal frames we need the oldest frame to be in the zero slot
 426   // in the frame_sizes/frame_pcs so the assembly code can do a trivial walk.
 427   // so things look a little strange in this loop.
 428   //
 429   int callee_parameters = 0;
 430   int callee_locals = 0;
 431   for (int index = 0; index &lt; array-&gt;frames(); index++ ) {
 432     // frame[number_of_frames - 1 ] = on_stack_size(youngest)
 433     // frame[number_of_frames - 2 ] = on_stack_size(sender(youngest))
 434     // frame[number_of_frames - 3 ] = on_stack_size(sender(sender(youngest)))
 435     frame_sizes[number_of_frames - 1 - index] = BytesPerWord * array-&gt;element(index)-&gt;on_stack_size(callee_parameters,
 436                                                                                                     callee_locals,
 437                                                                                                     index == 0,
 438                                                                                                     popframe_extra_args);
 439     // This pc doesn't have to be perfect just good enough to identify the frame
 440     // as interpreted so the skeleton frame will be walkable
 441     // The correct pc will be set when the skeleton frame is completely filled out
 442     // The final pc we store in the loop is wrong and will be overwritten below
 443     frame_pcs[number_of_frames - 1 - index ] = Interpreter::deopt_entry(vtos, 0) - frame::pc_return_offset;
 444 
 445     callee_parameters = array-&gt;element(index)-&gt;method()-&gt;size_of_parameters();
 446     callee_locals = array-&gt;element(index)-&gt;method()-&gt;max_locals();
 447     popframe_extra_args = 0;
 448   }
 449 
 450   // Compute whether the root vframe returns a float or double value.
 451   BasicType return_type;
 452   {
 453     HandleMark hm;
 454     methodHandle method(thread, array-&gt;element(0)-&gt;method());
 455     Bytecode_invoke invoke = Bytecode_invoke_check(method, array-&gt;element(0)-&gt;bci());
 456     return_type = invoke.is_valid() ? invoke.result_type() : T_ILLEGAL;
 457   }
 458 
 459   // Compute information for handling adapters and adjusting the frame size of the caller.
 460   int caller_adjustment = 0;
 461 
 462   // Compute the amount the oldest interpreter frame will have to adjust
 463   // its caller's stack by. If the caller is a compiled frame then
 464   // we pretend that the callee has no parameters so that the
 465   // extension counts for the full amount of locals and not just
 466   // locals-parms. This is because without a c2i adapter the parm
 467   // area as created by the compiled frame will not be usable by
 468   // the interpreter. (Depending on the calling convention there
 469   // may not even be enough space).
 470 
 471   // QQQ I'd rather see this pushed down into last_frame_adjust
 472   // and have it take the sender (aka caller).
 473 
 474   if (deopt_sender.is_compiled_frame() || caller_was_method_handle) {
 475     caller_adjustment = last_frame_adjust(0, callee_locals);
 476   } else if (callee_locals &gt; callee_parameters) {
 477     // The caller frame may need extending to accommodate
 478     // non-parameter locals of the first unpacked interpreted frame.
 479     // Compute that adjustment.
 480     caller_adjustment = last_frame_adjust(callee_parameters, callee_locals);
 481   }
 482 
 483   // If the sender is deoptimized the we must retrieve the address of the handler
 484   // since the frame will "magically" show the original pc before the deopt
 485   // and we'd undo the deopt.
 486 
 487   frame_pcs[0] = deopt_sender.raw_pc();
 488 
 489 #ifndef SHARK
 490   assert(CodeCache::find_blob_unsafe(frame_pcs[0]) != NULL, "bad pc");
 491 #endif // SHARK
 492 
 493 #ifdef INCLUDE_JVMCI
 494   if (exceptionObject() != NULL) {
 495     thread-&gt;set_exception_oop(exceptionObject());
 496     exec_mode = Unpack_exception;
 497   }
 498 #endif
 499 
 500   UnrollBlock* info = new UnrollBlock(array-&gt;frame_size() * BytesPerWord,
 501                                       caller_adjustment * BytesPerWord,
 502                                       caller_was_method_handle ? 0 : callee_parameters,
 503                                       number_of_frames,
 504                                       frame_sizes,
 505                                       frame_pcs,
 506                                       return_type,
 507                                       exec_mode);
 508   // On some platforms, we need a way to pass some platform dependent
 509   // information to the unpacking code so the skeletal frames come out
 510   // correct (initial fp value, unextended sp, ...)
 511   info-&gt;set_initial_info((intptr_t) array-&gt;sender().initial_deoptimization_info());
 512 
 513   if (array-&gt;frames() &gt; 1) {
 514     if (VerifyStack &amp;&amp; TraceDeoptimization) {
 515       ttyLocker ttyl;
 516       tty-&gt;print_cr("Deoptimizing method containing inlining");
 517     }
 518   }
 519 
 520   array-&gt;set_unroll_block(info);
 521   return info;
 522 }
 523 
 524 // Called to cleanup deoptimization data structures in normal case
 525 // after unpacking to stack and when stack overflow error occurs
 526 void Deoptimization::cleanup_deopt_info(JavaThread *thread,
 527                                         vframeArray *array) {
 528 
 529   // Get array if coming from exception
 530   if (array == NULL) {
 531     array = thread-&gt;vframe_array_head();
 532   }
 533   thread-&gt;set_vframe_array_head(NULL);
 534 
 535   // Free the previous UnrollBlock
 536   vframeArray* old_array = thread-&gt;vframe_array_last();
 537   thread-&gt;set_vframe_array_last(array);
 538 
 539   if (old_array != NULL) {
 540     UnrollBlock* old_info = old_array-&gt;unroll_block();
 541     old_array-&gt;set_unroll_block(NULL);
 542     delete old_info;
 543     delete old_array;
 544   }
 545 
 546   // Deallocate any resource creating in this routine and any ResourceObjs allocated
 547   // inside the vframeArray (StackValueCollections)
 548 
 549   delete thread-&gt;deopt_mark();
 550   thread-&gt;set_deopt_mark(NULL);
 551   thread-&gt;set_deopt_nmethod(NULL);
 552 
 553 
 554   if (JvmtiExport::can_pop_frame()) {
 555 #ifndef CC_INTERP
 556     // Regardless of whether we entered this routine with the pending
 557     // popframe condition bit set, we should always clear it now
 558     thread-&gt;clear_popframe_condition();
 559 #else
 560     // C++ interpreter will clear has_pending_popframe when it enters
 561     // with method_resume. For deopt_resume2 we clear it now.
 562     if (thread-&gt;popframe_forcing_deopt_reexecution())
 563         thread-&gt;clear_popframe_condition();
 564 #endif /* CC_INTERP */
 565   }
 566 
 567   // unpack_frames() is called at the end of the deoptimization handler
 568   // and (in C2) at the end of the uncommon trap handler. Note this fact
 569   // so that an asynchronous stack walker can work again. This counter is
 570   // incremented at the beginning of fetch_unroll_info() and (in C2) at
 571   // the beginning of uncommon_trap().
 572   thread-&gt;dec_in_deopt_handler();
 573 }
 574 
 575 // Moved from cpu directories because none of the cpus has callee save values.
 576 // If a cpu implements callee save values, move this to deoptimization_&lt;cpu&gt;.cpp.
 577 void Deoptimization::unwind_callee_save_values(frame* f, vframeArray* vframe_array) {
 578 
 579   // This code is sort of the equivalent of C2IAdapter::setup_stack_frame back in
 580   // the days we had adapter frames. When we deoptimize a situation where a
 581   // compiled caller calls a compiled caller will have registers it expects
 582   // to survive the call to the callee. If we deoptimize the callee the only
 583   // way we can restore these registers is to have the oldest interpreter
 584   // frame that we create restore these values. That is what this routine
 585   // will accomplish.
 586 
 587   // At the moment we have modified c2 to not have any callee save registers
 588   // so this problem does not exist and this routine is just a place holder.
 589 
 590   assert(f-&gt;is_interpreted_frame(), "must be interpreted");
 591 }
 592 
 593 // Return BasicType of value being returned
 594 JRT_LEAF(BasicType, Deoptimization::unpack_frames(JavaThread* thread, int exec_mode))
 595 
 596   // We are already active int he special DeoptResourceMark any ResourceObj's we
 597   // allocate will be freed at the end of the routine.
 598 
 599   // It is actually ok to allocate handles in a leaf method. It causes no safepoints,
 600   // but makes the entry a little slower. There is however a little dance we have to
 601   // do in debug mode to get around the NoHandleMark code in the JRT_LEAF macro
 602   ResetNoHandleMark rnhm; // No-op in release/product versions
 603   HandleMark hm;
 604 
 605   frame stub_frame = thread-&gt;last_frame();
 606 
 607   // Since the frame to unpack is the top frame of this thread, the vframe_array_head
 608   // must point to the vframeArray for the unpack frame.
 609   vframeArray* array = thread-&gt;vframe_array_head();
 610 
 611 #ifndef PRODUCT
 612   if (TraceDeoptimization) {
 613     ttyLocker ttyl;
 614     tty-&gt;print_cr("DEOPT UNPACKING thread " INTPTR_FORMAT " vframeArray " INTPTR_FORMAT " mode %d",
 615                   p2i(thread), p2i(array), exec_mode);
 616   }
 617 #endif
 618   Events::log(thread, "DEOPT UNPACKING pc=" INTPTR_FORMAT " sp=" INTPTR_FORMAT " mode %d",
 619               p2i(stub_frame.pc()), p2i(stub_frame.sp()), exec_mode);
 620 
 621   UnrollBlock* info = array-&gt;unroll_block();
 622 
 623   // Unpack the interpreter frames and any adapter frame (c2 only) we might create.
 624   array-&gt;unpack_to_stack(stub_frame, exec_mode, info-&gt;caller_actual_parameters());
 625 
 626   BasicType bt = info-&gt;return_type();
 627 
 628   // If we have an exception pending, claim that the return type is an oop
 629   // so the deopt_blob does not overwrite the exception_oop.
 630 
 631   if (exec_mode == Unpack_exception)
 632     bt = T_OBJECT;
 633 
 634   // Cleanup thread deopt data
 635   cleanup_deopt_info(thread, array);
 636 
 637 #ifndef PRODUCT
 638   if (VerifyStack) {
 639     ResourceMark res_mark;
 640 
 641     thread-&gt;validate_frame_layout();
 642 
 643     // Verify that the just-unpacked frames match the interpreter's
 644     // notions of expression stack and locals
 645     vframeArray* cur_array = thread-&gt;vframe_array_last();
 646     RegisterMap rm(thread, false);
 647     rm.set_include_argument_oops(false);
 648     bool is_top_frame = true;
 649     int callee_size_of_parameters = 0;
 650     int callee_max_locals = 0;
 651     for (int i = 0; i &lt; cur_array-&gt;frames(); i++) {
 652       vframeArrayElement* el = cur_array-&gt;element(i);
 653       frame* iframe = el-&gt;iframe();
 654       guarantee(iframe-&gt;is_interpreted_frame(), "Wrong frame type");
 655 
 656       // Get the oop map for this bci
 657       InterpreterOopMap mask;
 658       int cur_invoke_parameter_size = 0;
 659       bool try_next_mask = false;
 660       int next_mask_expression_stack_size = -1;
 661       int top_frame_expression_stack_adjustment = 0;
 662       methodHandle mh(thread, iframe-&gt;interpreter_frame_method());
 663       OopMapCache::compute_one_oop_map(mh, iframe-&gt;interpreter_frame_bci(), &amp;mask);
 664       BytecodeStream str(mh);
 665       str.set_start(iframe-&gt;interpreter_frame_bci());
 666       int max_bci = mh-&gt;code_size();
 667       // Get to the next bytecode if possible
 668       assert(str.bci() &lt; max_bci, "bci in interpreter frame out of bounds");
 669       // Check to see if we can grab the number of outgoing arguments
 670       // at an uncommon trap for an invoke (where the compiler
 671       // generates debug info before the invoke has executed)
 672       Bytecodes::Code cur_code = str.next();
 673       if (cur_code == Bytecodes::_invokevirtual   ||
 674           cur_code == Bytecodes::_invokespecial   ||
 675           cur_code == Bytecodes::_invokestatic    ||
 676           cur_code == Bytecodes::_invokeinterface ||
 677           cur_code == Bytecodes::_invokedynamic) {
 678         Bytecode_invoke invoke(mh, iframe-&gt;interpreter_frame_bci());
 679         Symbol* signature = invoke.signature();
 680         ArgumentSizeComputer asc(signature);
 681         cur_invoke_parameter_size = asc.size();
 682         if (invoke.has_receiver()) {
 683           // Add in receiver
 684           ++cur_invoke_parameter_size;
 685         }
 686         if (i != 0 &amp;&amp; !invoke.is_invokedynamic() &amp;&amp; MethodHandles::has_member_arg(invoke.klass(), invoke.name())) {
 687           callee_size_of_parameters++;
 688         }
 689       }
 690       if (str.bci() &lt; max_bci) {
 691         Bytecodes::Code bc = str.next();
 692         if (bc &gt;= 0) {
 693           // The interpreter oop map generator reports results before
 694           // the current bytecode has executed except in the case of
 695           // calls. It seems to be hard to tell whether the compiler
 696           // has emitted debug information matching the "state before"
 697           // a given bytecode or the state after, so we try both
 698           switch (cur_code) {
 699             case Bytecodes::_invokevirtual:
 700             case Bytecodes::_invokespecial:
 701             case Bytecodes::_invokestatic:
 702             case Bytecodes::_invokeinterface:
 703             case Bytecodes::_invokedynamic:
 704             case Bytecodes::_athrow:
 705               break;
 706             default: {
 707               InterpreterOopMap next_mask;
 708               OopMapCache::compute_one_oop_map(mh, str.bci(), &amp;next_mask);
 709               next_mask_expression_stack_size = next_mask.expression_stack_size();
 710               // Need to subtract off the size of the result type of
 711               // the bytecode because this is not described in the
 712               // debug info but returned to the interpreter in the TOS
 713               // caching register
 714               BasicType bytecode_result_type = Bytecodes::result_type(cur_code);
 715               if (bytecode_result_type != T_ILLEGAL) {
 716                 top_frame_expression_stack_adjustment = type2size[bytecode_result_type];
 717               }
 718               assert(top_frame_expression_stack_adjustment &gt;= 0, "");
 719               try_next_mask = true;
 720               break;
 721             }
 722           }
 723         }
 724       }
 725 
 726       // Verify stack depth and oops in frame
 727       // This assertion may be dependent on the platform we're running on and may need modification (tested on x86 and sparc)
 728       if (!(
 729             /* SPARC */
 730             (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + callee_size_of_parameters) ||
 731             /* x86 */
 732             (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + callee_max_locals) ||
 733             (try_next_mask &amp;&amp;
 734              (iframe-&gt;interpreter_frame_expression_stack_size() == (next_mask_expression_stack_size -
 735                                                                     top_frame_expression_stack_adjustment))) ||
 736             (is_top_frame &amp;&amp; (exec_mode == Unpack_exception) &amp;&amp; iframe-&gt;interpreter_frame_expression_stack_size() == 0) ||
 737             (is_top_frame &amp;&amp; (exec_mode == Unpack_uncommon_trap || exec_mode == Unpack_reexecute || el-&gt;should_reexecute()) &amp;&amp;
 738              (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + cur_invoke_parameter_size))
 739             )) {
 740         ttyLocker ttyl;
 741 
 742         // Print out some information that will help us debug the problem
 743         tty-&gt;print_cr("Wrong number of expression stack elements during deoptimization");
 744         tty-&gt;print_cr("  Error occurred while verifying frame %d (0..%d, 0 is topmost)", i, cur_array-&gt;frames() - 1);
 745         tty-&gt;print_cr("  Fabricated interpreter frame had %d expression stack elements",
 746                       iframe-&gt;interpreter_frame_expression_stack_size());
 747         tty-&gt;print_cr("  Interpreter oop map had %d expression stack elements", mask.expression_stack_size());
 748         tty-&gt;print_cr("  try_next_mask = %d", try_next_mask);
 749         tty-&gt;print_cr("  next_mask_expression_stack_size = %d", next_mask_expression_stack_size);
 750         tty-&gt;print_cr("  callee_size_of_parameters = %d", callee_size_of_parameters);
 751         tty-&gt;print_cr("  callee_max_locals = %d", callee_max_locals);
 752         tty-&gt;print_cr("  top_frame_expression_stack_adjustment = %d", top_frame_expression_stack_adjustment);
 753         tty-&gt;print_cr("  exec_mode = %d", exec_mode);
 754         tty-&gt;print_cr("  cur_invoke_parameter_size = %d", cur_invoke_parameter_size);
 755         tty-&gt;print_cr("  Thread = " INTPTR_FORMAT ", thread ID = %d", p2i(thread), thread-&gt;osthread()-&gt;thread_id());
 756         tty-&gt;print_cr("  Interpreted frames:");
 757         for (int k = 0; k &lt; cur_array-&gt;frames(); k++) {
 758           vframeArrayElement* el = cur_array-&gt;element(k);
 759           tty-&gt;print_cr("    %s (bci %d)", el-&gt;method()-&gt;name_and_sig_as_C_string(), el-&gt;bci());
 760         }
 761         cur_array-&gt;print_on_2(tty);
 762         guarantee(false, "wrong number of expression stack elements during deopt");
 763       }
 764       VerifyOopClosure verify;
 765       iframe-&gt;oops_interpreted_do(&amp;verify, NULL, &amp;rm, false);
 766       callee_size_of_parameters = mh-&gt;size_of_parameters();
 767       callee_max_locals = mh-&gt;max_locals();
 768       is_top_frame = false;
 769     }
 770   }
 771 #endif /* !PRODUCT */
 772 
 773 
 774   return bt;
 775 JRT_END
 776 
 777 void Deoptimization::print_deoptimization_count() {
 778   ttyLocker ttyl;
 779   tty-&gt;print_cr("Total number of deoptimizations: %d &lt;&lt;&lt;", _deoptimization_count);
 780 }
 781 
 782 int Deoptimization::deoptimize_dependents() {
 783   Threads::deoptimized_wrt_marked_nmethods();
 784   return 0;
 785 }
 786 
 787 Deoptimization::DeoptAction Deoptimization::_unloaded_action
 788   = Deoptimization::Action_reinterpret;
 789 
 790 #if defined(COMPILER2) || INCLUDE_JVMCI
 791 bool Deoptimization::realloc_objects(JavaThread* thread, frame* fr, GrowableArray&lt;ScopeValue*&gt;* objects, TRAPS) {
 792   Handle pending_exception(thread-&gt;pending_exception());
 793   const char* exception_file = thread-&gt;exception_file();
 794   int exception_line = thread-&gt;exception_line();
 795   thread-&gt;clear_pending_exception();
 796 
 797   bool failures = false;
 798 
 799   for (int i = 0; i &lt; objects-&gt;length(); i++) {
 800     assert(objects-&gt;at(i)-&gt;is_object(), "invalid debug information");
 801     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
 802 
 803     KlassHandle k(java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()()));
 804     oop obj = NULL;
 805 
 806     if (k-&gt;is_instance_klass()) {
 807       InstanceKlass* ik = InstanceKlass::cast(k());
 808       obj = ik-&gt;allocate_instance(THREAD);
 809     } else if (k-&gt;is_typeArray_klass()) {
 810       TypeArrayKlass* ak = TypeArrayKlass::cast(k());
 811       assert(sv-&gt;field_size() % type2size[ak-&gt;element_type()] == 0, "non-integral array length");
 812       int len = sv-&gt;field_size() / type2size[ak-&gt;element_type()];
 813       obj = ak-&gt;allocate(len, THREAD);
 814     } else if (k-&gt;is_objArray_klass()) {
 815       ObjArrayKlass* ak = ObjArrayKlass::cast(k());
 816       obj = ak-&gt;allocate(sv-&gt;field_size(), THREAD);
 817     }
 818 
 819     if (obj == NULL) {
 820       failures = true;
 821     }
 822 
 823     assert(sv-&gt;value().is_null(), "redundant reallocation");
 824     assert(obj != NULL || HAS_PENDING_EXCEPTION, "allocation should succeed or we should get an exception");
 825     CLEAR_PENDING_EXCEPTION;
 826     sv-&gt;set_value(obj);
 827   }
 828 
 829   if (failures) {
 830     THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), failures);
 831   } else if (pending_exception.not_null()) {
 832     thread-&gt;set_pending_exception(pending_exception(), exception_file, exception_line);
 833   }
 834 
 835   return failures;
 836 }
 837 
 838 // restore elements of an eliminated type array
 839 void Deoptimization::reassign_type_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, typeArrayOop obj, BasicType type) {
 840   int index = 0;
 841   intptr_t val;
 842 
 843   for (int i = 0; i &lt; sv-&gt;field_size(); i++) {
 844     StackValue* value = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(i));
 845     switch(type) {
 846     case T_LONG: case T_DOUBLE: {
 847       assert(value-&gt;type() == T_INT, "Agreement.");
 848       StackValue* low =
 849         StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++i));
 850 #ifdef _LP64
 851       jlong res = (jlong)low-&gt;get_int();
 852 #else
 853 #ifdef SPARC
 854       // For SPARC we have to swap high and low words.
 855       jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
 856 #else
 857       jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
 858 #endif //SPARC
 859 #endif
 860       obj-&gt;long_at_put(index, res);
 861       break;
 862     }
 863 
 864     // Have to cast to INT (32 bits) pointer to avoid little/big-endian problem.
 865     case T_INT: case T_FLOAT: { // 4 bytes.
 866       assert(value-&gt;type() == T_INT, "Agreement.");
 867       bool big_value = false;
 868       if (i + 1 &lt; sv-&gt;field_size() &amp;&amp; type == T_INT) {
 869         if (sv-&gt;field_at(i)-&gt;is_location()) {
 870           Location::Type type = ((LocationValue*) sv-&gt;field_at(i))-&gt;location().type();
 871           if (type == Location::dbl || type == Location::lng) {
 872             big_value = true;
 873           }
 874         } else if (sv-&gt;field_at(i)-&gt;is_constant_int()) {
 875           ScopeValue* next_scope_field = sv-&gt;field_at(i + 1);
 876           if (next_scope_field-&gt;is_constant_long() || next_scope_field-&gt;is_constant_double()) {
 877             big_value = true;
 878           }
 879         }
 880       }
 881 
 882       if (big_value) {
 883         StackValue* low = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++i));
 884   #ifdef _LP64
 885         jlong res = (jlong)low-&gt;get_int();
 886   #else
 887   #ifdef SPARC
 888         // For SPARC we have to swap high and low words.
 889         jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
 890   #else
 891         jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
 892   #endif //SPARC
 893   #endif
 894         obj-&gt;int_at_put(index, (jint)*((jint*)&amp;res));
 895         obj-&gt;int_at_put(++index, (jint)*(((jint*)&amp;res) + 1));
 896       } else {
 897         val = value-&gt;get_int();
 898         obj-&gt;int_at_put(index, (jint)*((jint*)&amp;val));
 899       }
 900       break;
 901     }
 902 
 903     case T_SHORT: case T_CHAR: // 2 bytes
 904       assert(value-&gt;type() == T_INT, "Agreement.");
 905       val = value-&gt;get_int();
 906       obj-&gt;short_at_put(index, (jshort)*((jint*)&amp;val));
 907       break;
 908 
 909     case T_BOOLEAN: case T_BYTE: // 1 byte
 910       assert(value-&gt;type() == T_INT, "Agreement.");
 911       val = value-&gt;get_int();
 912       obj-&gt;bool_at_put(index, (jboolean)*((jint*)&amp;val));
 913       break;
 914 
 915       default:
 916         ShouldNotReachHere();
 917     }
 918     index++;
 919   }
 920 }
 921 
 922 
 923 // restore fields of an eliminated object array
 924 void Deoptimization::reassign_object_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, objArrayOop obj) {
 925   for (int i = 0; i &lt; sv-&gt;field_size(); i++) {
 926     StackValue* value = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(i));
 927     assert(value-&gt;type() == T_OBJECT, "object element expected");
 928     obj-&gt;obj_at_put(i, value-&gt;get_obj()());
 929   }
 930 }
 931 
 932 class ReassignedField {
 933 public:
 934   int _offset;
 935   BasicType _type;
 936 public:
 937   ReassignedField() {
 938     _offset = 0;
 939     _type = T_ILLEGAL;
 940   }
 941 };
 942 
 943 int compare(ReassignedField* left, ReassignedField* right) {
 944   return left-&gt;_offset - right-&gt;_offset;
 945 }
 946 
 947 // Restore fields of an eliminated instance object using the same field order
 948 // returned by HotSpotResolvedObjectTypeImpl.getInstanceFields(true)
 949 static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {
 950   if (klass-&gt;superklass() != NULL) {
 951     svIndex = reassign_fields_by_klass(klass-&gt;superklass(), fr, reg_map, sv, svIndex, obj, skip_internal);
 952   }
 953 
 954   GrowableArray&lt;ReassignedField&gt;* fields = new GrowableArray&lt;ReassignedField&gt;();
 955   for (AllFieldStream fs(klass); !fs.done(); fs.next()) {
 956     if (!fs.access_flags().is_static() &amp;&amp; (!skip_internal || !fs.access_flags().is_internal())) {
 957       ReassignedField field;
 958       field._offset = fs.offset();
 959       field._type = FieldType::basic_type(fs.signature());
 960       fields-&gt;append(field);
 961     }
 962   }
 963   fields-&gt;sort(compare);
 964   for (int i = 0; i &lt; fields-&gt;length(); i++) {
 965     intptr_t val;
 966     ScopeValue* scope_field = sv-&gt;field_at(svIndex);
 967     StackValue* value = StackValue::create_stack_value(fr, reg_map, scope_field);
 968     int offset = fields-&gt;at(i)._offset;
 969     BasicType type = fields-&gt;at(i)._type;
 970     switch (type) {
 971       case T_OBJECT: case T_ARRAY:
 972         assert(value-&gt;type() == T_OBJECT, "Agreement.");
 973         obj-&gt;obj_field_put(offset, value-&gt;get_obj()());
 974         break;
 975 
 976       // Have to cast to INT (32 bits) pointer to avoid little/big-endian problem.
 977       case T_INT: case T_FLOAT: { // 4 bytes.
 978         assert(value-&gt;type() == T_INT, "Agreement.");
 979         bool big_value = false;
 980         if (i+1 &lt; fields-&gt;length() &amp;&amp; fields-&gt;at(i+1)._type == T_INT) {
 981           if (scope_field-&gt;is_location()) {
 982             Location::Type type = ((LocationValue*) scope_field)-&gt;location().type();
 983             if (type == Location::dbl || type == Location::lng) {
 984               big_value = true;
 985             }
 986           }
 987           if (scope_field-&gt;is_constant_int()) {
 988             ScopeValue* next_scope_field = sv-&gt;field_at(svIndex + 1);
 989             if (next_scope_field-&gt;is_constant_long() || next_scope_field-&gt;is_constant_double()) {
 990               big_value = true;
 991             }
 992           }
 993         }
 994 
 995         if (big_value) {
 996           i++;
 997           assert(i &lt; fields-&gt;length(), "second T_INT field needed");
 998           assert(fields-&gt;at(i)._type == T_INT, "T_INT field needed");
 999         } else {
1000           val = value-&gt;get_int();
1001           obj-&gt;int_field_put(offset, (jint)*((jint*)&amp;val));
1002           break;
1003         }
1004       }
1005         /* no break */
1006 
1007       case T_LONG: case T_DOUBLE: {
1008         assert(value-&gt;type() == T_INT, "Agreement.");
1009         StackValue* low = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++svIndex));
1010 #ifdef _LP64
1011         jlong res = (jlong)low-&gt;get_int();
1012 #else
1013 #ifdef SPARC
1014         // For SPARC we have to swap high and low words.
1015         jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
1016 #else
1017         jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
1018 #endif //SPARC
1019 #endif
1020         obj-&gt;long_field_put(offset, res);
1021         break;
1022       }
1023 
1024       case T_SHORT: case T_CHAR: // 2 bytes
1025         assert(value-&gt;type() == T_INT, "Agreement.");
1026         val = value-&gt;get_int();
1027         obj-&gt;short_field_put(offset, (jshort)*((jint*)&amp;val));
1028         break;
1029 
1030       case T_BOOLEAN: case T_BYTE: // 1 byte
1031         assert(value-&gt;type() == T_INT, "Agreement.");
1032         val = value-&gt;get_int();
1033         obj-&gt;bool_field_put(offset, (jboolean)*((jint*)&amp;val));
1034         break;
1035 
1036       default:
1037         ShouldNotReachHere();
1038     }
1039     svIndex++;
1040   }
1041   return svIndex;
1042 }
1043 
1044 // restore fields of all eliminated objects and arrays
1045 void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray&lt;ScopeValue*&gt;* objects, bool realloc_failures, bool skip_internal) {
1046   for (int i = 0; i &lt; objects-&gt;length(); i++) {
1047     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
1048     KlassHandle k(java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()()));
1049     Handle obj = sv-&gt;value();
1050     assert(obj.not_null() || realloc_failures, "reallocation was missed");
1051     if (PrintDeoptimizationDetails) {
1052       tty-&gt;print_cr("reassign fields for object of type %s!", k-&gt;name()-&gt;as_C_string());
1053     }
1054     if (obj.is_null()) {
1055       continue;
1056     }
1057 
1058     if (k-&gt;is_instance_klass()) {
1059       InstanceKlass* ik = InstanceKlass::cast(k());
1060       reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);
1061     } else if (k-&gt;is_typeArray_klass()) {
1062       TypeArrayKlass* ak = TypeArrayKlass::cast(k());
1063       reassign_type_array_elements(fr, reg_map, sv, (typeArrayOop) obj(), ak-&gt;element_type());
1064     } else if (k-&gt;is_objArray_klass()) {
1065       reassign_object_array_elements(fr, reg_map, sv, (objArrayOop) obj());
1066     }
1067   }
1068 }
1069 
1070 
1071 // relock objects for which synchronization was eliminated
1072 void Deoptimization::relock_objects(GrowableArray&lt;MonitorInfo*&gt;* monitors, JavaThread* thread, bool realloc_failures) {
1073   for (int i = 0; i &lt; monitors-&gt;length(); i++) {
1074     MonitorInfo* mon_info = monitors-&gt;at(i);
1075     if (mon_info-&gt;eliminated()) {
1076       assert(!mon_info-&gt;owner_is_scalar_replaced() || realloc_failures, "reallocation was missed");
1077       if (!mon_info-&gt;owner_is_scalar_replaced()) {
1078         Handle obj = Handle(mon_info-&gt;owner());
1079         markOop mark = obj-&gt;mark();
1080         if (UseBiasedLocking &amp;&amp; mark-&gt;has_bias_pattern()) {
1081           // New allocated objects may have the mark set to anonymously biased.
1082           // Also the deoptimized method may called methods with synchronization
1083           // where the thread-local object is bias locked to the current thread.
1084           assert(mark-&gt;is_biased_anonymously() ||
1085                  mark-&gt;biased_locker() == thread, "should be locked to current thread");
1086           // Reset mark word to unbiased prototype.
1087           markOop unbiased_prototype = markOopDesc::prototype()-&gt;set_age(mark-&gt;age());
1088           obj-&gt;set_mark(unbiased_prototype);
1089         }
1090         BasicLock* lock = mon_info-&gt;lock();
1091         ObjectSynchronizer::slow_enter(obj, lock, thread);
1092         assert(mon_info-&gt;owner()-&gt;is_locked(), "object must be locked now");
1093       }
1094     }
1095   }
1096 }
1097 
1098 
1099 #ifndef PRODUCT
1100 // print information about reallocated objects
1101 void Deoptimization::print_objects(GrowableArray&lt;ScopeValue*&gt;* objects, bool realloc_failures) {
1102   fieldDescriptor fd;
1103 
1104   for (int i = 0; i &lt; objects-&gt;length(); i++) {
1105     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
1106     KlassHandle k(java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()()));
1107     Handle obj = sv-&gt;value();
1108 
1109     tty-&gt;print("     object &lt;" INTPTR_FORMAT "&gt; of type ", p2i(sv-&gt;value()()));
1110     k-&gt;print_value();
1111     assert(obj.not_null() || realloc_failures, "reallocation was missed");
1112     if (obj.is_null()) {
1113       tty-&gt;print(" allocation failed");
1114     } else {
1115       tty-&gt;print(" allocated (%d bytes)", obj-&gt;size() * HeapWordSize);
1116     }
1117     tty-&gt;cr();
1118 
1119     if (Verbose &amp;&amp; !obj.is_null()) {
1120       k-&gt;oop_print_on(obj(), tty);
1121     }
1122   }
1123 }
1124 #endif
1125 #endif // COMPILER2 || INCLUDE_JVMCI
1126 
1127 vframeArray* Deoptimization::create_vframeArray(JavaThread* thread, frame fr, RegisterMap *reg_map, GrowableArray&lt;compiledVFrame*&gt;* chunk, bool realloc_failures) {
1128   Events::log(thread, "DEOPT PACKING pc=" INTPTR_FORMAT " sp=" INTPTR_FORMAT, p2i(fr.pc()), p2i(fr.sp()));
1129 
1130 #ifndef PRODUCT
1131   if (PrintDeoptimizationDetails) {
1132     ttyLocker ttyl;
1133     tty-&gt;print("DEOPT PACKING thread " INTPTR_FORMAT " ", p2i(thread));
1134     fr.print_on(tty);
1135     tty-&gt;print_cr("     Virtual frames (innermost first):");
1136     for (int index = 0; index &lt; chunk-&gt;length(); index++) {
1137       compiledVFrame* vf = chunk-&gt;at(index);
1138       tty-&gt;print("       %2d - ", index);
1139       vf-&gt;print_value();
1140       int bci = chunk-&gt;at(index)-&gt;raw_bci();
1141       const char* code_name;
1142       if (bci == SynchronizationEntryBCI) {
1143         code_name = "sync entry";
1144       } else {
1145         Bytecodes::Code code = vf-&gt;method()-&gt;code_at(bci);
1146         code_name = Bytecodes::name(code);
1147       }
1148       tty-&gt;print(" - %s", code_name);
1149       tty-&gt;print_cr(" @ bci %d ", bci);
1150       if (Verbose) {
1151         vf-&gt;print();
1152         tty-&gt;cr();
1153       }
1154     }
1155   }
1156 #endif
1157   if (PrintDeoptimizationCount || PrintDeoptimizationCountVerbose) {
1158     increase_deoptimization_count();
1159     if (PrintDeoptimizationCountVerbose) {
1160       print_deoptimization_count();
1161     }
1162   }
1163 
1164   // Register map for next frame (used for stack crawl).  We capture
1165   // the state of the deopt'ing frame's caller.  Thus if we need to
1166   // stuff a C2I adapter we can properly fill in the callee-save
1167   // register locations.
1168   frame caller = fr.sender(reg_map);
1169   int frame_size = caller.sp() - fr.sp();
1170 
1171   frame sender = caller;
1172 
1173   // Since the Java thread being deoptimized will eventually adjust it's own stack,
1174   // the vframeArray containing the unpacking information is allocated in the C heap.
1175   // For Compiler1, the caller of the deoptimized frame is saved for use by unpack_frames().
1176   vframeArray* array = vframeArray::allocate(thread, frame_size, chunk, reg_map, sender, caller, fr, realloc_failures);
1177 
1178   // Compare the vframeArray to the collected vframes
1179   assert(array-&gt;structural_compare(thread, chunk), "just checking");
1180 
1181 #ifndef PRODUCT
1182   if (PrintDeoptimizationDetails) {
1183     ttyLocker ttyl;
1184     tty-&gt;print_cr("     Created vframeArray " INTPTR_FORMAT, p2i(array));
1185   }
1186 #endif // PRODUCT
1187 
1188   return array;
1189 }
1190 
1191 #if defined(COMPILER2) || INCLUDE_JVMCI
1192 void Deoptimization::pop_frames_failed_reallocs(JavaThread* thread, vframeArray* array) {
1193   // Reallocation of some scalar replaced objects failed. Record
1194   // that we need to pop all the interpreter frames for the
1195   // deoptimized compiled frame.
1196   assert(thread-&gt;frames_to_pop_failed_realloc() == 0, "missed frames to pop?");
1197   thread-&gt;set_frames_to_pop_failed_realloc(array-&gt;frames());
1198   // Unlock all monitors here otherwise the interpreter will see a
1199   // mix of locked and unlocked monitors (because of failed
1200   // reallocations of synchronized objects) and be confused.
1201   for (int i = 0; i &lt; array-&gt;frames(); i++) {
1202     MonitorChunk* monitors = array-&gt;element(i)-&gt;monitors();
1203     if (monitors != NULL) {
1204       for (int j = 0; j &lt; monitors-&gt;number_of_monitors(); j++) {
1205         BasicObjectLock* src = monitors-&gt;at(j);
1206         if (src-&gt;obj() != NULL) {
1207           ObjectSynchronizer::fast_exit(src-&gt;obj(), src-&gt;lock(), thread);
1208         }
1209       }
1210       array-&gt;element(i)-&gt;free_monitors(thread);
1211 #ifdef ASSERT
1212       array-&gt;element(i)-&gt;set_removed_monitors();
1213 #endif
1214     }
1215   }
1216 }
1217 #endif
1218 
1219 static void collect_monitors(compiledVFrame* cvf, GrowableArray&lt;Handle&gt;* objects_to_revoke) {
1220   GrowableArray&lt;MonitorInfo*&gt;* monitors = cvf-&gt;monitors();
1221   for (int i = 0; i &lt; monitors-&gt;length(); i++) {
1222     MonitorInfo* mon_info = monitors-&gt;at(i);
1223     if (!mon_info-&gt;eliminated() &amp;&amp; mon_info-&gt;owner() != NULL) {
1224       objects_to_revoke-&gt;append(Handle(mon_info-&gt;owner()));
1225     }
1226   }
1227 }
1228 
1229 
1230 void Deoptimization::revoke_biases_of_monitors(JavaThread* thread, frame fr, RegisterMap* map) {
1231   if (!UseBiasedLocking) {
1232     return;
1233   }
1234 
1235   GrowableArray&lt;Handle&gt;* objects_to_revoke = new GrowableArray&lt;Handle&gt;();
1236 
1237   // Unfortunately we don't have a RegisterMap available in most of
1238   // the places we want to call this routine so we need to walk the
1239   // stack again to update the register map.
1240   if (map == NULL || !map-&gt;update_map()) {
1241     StackFrameStream sfs(thread, true);
1242     bool found = false;
1243     while (!found &amp;&amp; !sfs.is_done()) {
1244       frame* cur = sfs.current();
1245       sfs.next();
1246       found = cur-&gt;id() == fr.id();
1247     }
1248     assert(found, "frame to be deoptimized not found on target thread's stack");
1249     map = sfs.register_map();
1250   }
1251 
1252   vframe* vf = vframe::new_vframe(&amp;fr, map, thread);
1253   compiledVFrame* cvf = compiledVFrame::cast(vf);
1254   // Revoke monitors' biases in all scopes
1255   while (!cvf-&gt;is_top()) {
1256     collect_monitors(cvf, objects_to_revoke);
1257     cvf = compiledVFrame::cast(cvf-&gt;sender());
1258   }
1259   collect_monitors(cvf, objects_to_revoke);
1260 
1261   if (SafepointSynchronize::is_at_safepoint()) {
1262     BiasedLocking::revoke_at_safepoint(objects_to_revoke);
1263   } else {
1264     BiasedLocking::revoke(objects_to_revoke);
1265   }
1266 }
1267 
1268 
1269 void Deoptimization::revoke_biases_of_monitors(CodeBlob* cb) {
1270   if (!UseBiasedLocking) {
1271     return;
1272   }
1273 
1274   assert(SafepointSynchronize::is_at_safepoint(), "must only be called from safepoint");
1275   GrowableArray&lt;Handle&gt;* objects_to_revoke = new GrowableArray&lt;Handle&gt;();
1276   for (JavaThread* jt = Threads::first(); jt != NULL ; jt = jt-&gt;next()) {
1277     if (jt-&gt;has_last_Java_frame()) {
1278       StackFrameStream sfs(jt, true);
1279       while (!sfs.is_done()) {
1280         frame* cur = sfs.current();
1281         if (cb-&gt;contains(cur-&gt;pc())) {
1282           vframe* vf = vframe::new_vframe(cur, sfs.register_map(), jt);
1283           compiledVFrame* cvf = compiledVFrame::cast(vf);
1284           // Revoke monitors' biases in all scopes
1285           while (!cvf-&gt;is_top()) {
1286             collect_monitors(cvf, objects_to_revoke);
1287             cvf = compiledVFrame::cast(cvf-&gt;sender());
1288           }
1289           collect_monitors(cvf, objects_to_revoke);
1290         }
1291         sfs.next();
1292       }
1293     }
1294   }
1295   BiasedLocking::revoke_at_safepoint(objects_to_revoke);
1296 }
1297 
1298 volatile int Deoptimization::_deoptimization_count = 0;
1299 
1300 void Deoptimization::increase_deoptimization_count() {
1301   _deoptimization_count++;
1302 }
1303 
1304 void Deoptimization::deoptimize_single_frame(JavaThread* thread, frame fr, Deoptimization::DeoptReason reason) {
1305   assert(fr.can_be_deoptimized(), "checking frame type");
1306 
1307   gather_statistics(reason, Action_none, Bytecodes::_illegal);
1308 
1309   if (LogCompilation &amp;&amp; xtty != NULL) {
1310     nmethod* nm = fr.cb()-&gt;as_nmethod_or_null();
1311     assert(nm != NULL, "only compiled methods can deopt");
1312 
1313     ttyLocker ttyl;
1314     xtty-&gt;begin_head("deoptimized thread='" UINTX_FORMAT "'", (uintx)thread-&gt;osthread()-&gt;thread_id());
1315     nm-&gt;log_identity(xtty);
1316     xtty-&gt;end_head();
1317     for (ScopeDesc* sd = nm-&gt;scope_desc_at(fr.pc()); ; sd = sd-&gt;sender()) {
1318       xtty-&gt;begin_elem("jvms bci='%d'", sd-&gt;bci());
1319       xtty-&gt;method(sd-&gt;method());
1320       xtty-&gt;end_elem();
1321       if (sd-&gt;is_top())  break;
1322     }
1323     xtty-&gt;tail("deoptimized");
1324   }
1325 
1326   // Patch the compiled method so that when execution returns to it we will
1327   // deopt the execution state and return to the interpreter.
1328   fr.deoptimize(thread);
1329 }
1330 
1331 void Deoptimization::deoptimize(JavaThread* thread, frame fr, RegisterMap *map) {
1332   deoptimize(thread, fr, map, Reason_constraint);
1333 }
1334 
1335 void Deoptimization::deoptimize(JavaThread* thread, frame fr, RegisterMap *map, DeoptReason reason) {
1336   // Deoptimize only if the frame comes from compile code.
1337   // Do not deoptimize the frame which is already patched
1338   // during the execution of the loops below.
1339   if (!fr.is_compiled_frame() || fr.is_deoptimized_frame()) {
1340     return;
1341   }
1342   ResourceMark rm;
1343   DeoptimizationMarker dm;
1344   if (UseBiasedLocking) {
1345     revoke_biases_of_monitors(thread, fr, map);
1346   }
1347   deoptimize_single_frame(thread, fr, reason);
1348 
1349 }
1350 
1351 
1352 void Deoptimization::deoptimize_frame_internal(JavaThread* thread, intptr_t* id, DeoptReason reason) {
1353   assert(thread == Thread::current() || SafepointSynchronize::is_at_safepoint(),
1354          "can only deoptimize other thread at a safepoint");
1355   // Compute frame and register map based on thread and sp.
1356   RegisterMap reg_map(thread, UseBiasedLocking);
1357   frame fr = thread-&gt;last_frame();
1358   while (fr.id() != id) {
1359     fr = fr.sender(&amp;reg_map);
1360   }
1361   deoptimize(thread, fr, &amp;reg_map, reason);
1362 }
1363 
1364 
1365 void Deoptimization::deoptimize_frame(JavaThread* thread, intptr_t* id, DeoptReason reason) {
1366   if (thread == Thread::current()) {
1367     Deoptimization::deoptimize_frame_internal(thread, id, reason);
1368   } else {
1369     VM_DeoptimizeFrame deopt(thread, id, reason);
1370     VMThread::execute(&amp;deopt);
1371   }
1372 }
1373 
1374 void Deoptimization::deoptimize_frame(JavaThread* thread, intptr_t* id) {
1375   deoptimize_frame(thread, id, Reason_constraint);
1376 }
1377 
1378 // JVMTI PopFrame support
1379 JRT_LEAF(void, Deoptimization::popframe_preserve_args(JavaThread* thread, int bytes_to_save, void* start_address))
1380 {
1381   thread-&gt;popframe_preserve_args(in_ByteSize(bytes_to_save), start_address);
1382 }
1383 JRT_END
1384 
1385 MethodData*
1386 Deoptimization::get_method_data(JavaThread* thread, methodHandle m,
1387                                 bool create_if_missing) {
1388   Thread* THREAD = thread;
1389   MethodData* mdo = m()-&gt;method_data();
1390   if (mdo == NULL &amp;&amp; create_if_missing &amp;&amp; !HAS_PENDING_EXCEPTION) {
1391     // Build an MDO.  Ignore errors like OutOfMemory;
1392     // that simply means we won't have an MDO to update.
1393     Method::build_interpreter_method_data(m, THREAD);
1394     if (HAS_PENDING_EXCEPTION) {
1395       assert((PENDING_EXCEPTION-&gt;is_a(SystemDictionary::OutOfMemoryError_klass())), "we expect only an OOM error here");
1396       CLEAR_PENDING_EXCEPTION;
1397     }
1398     mdo = m()-&gt;method_data();
1399   }
1400   return mdo;
1401 }
1402 
1403 #if defined(COMPILER2) || defined(SHARK) || INCLUDE_JVMCI
1404 void Deoptimization::load_class_by_index(const constantPoolHandle&amp; constant_pool, int index, TRAPS) {
1405   // in case of an unresolved klass entry, load the class.
1406   if (constant_pool-&gt;tag_at(index).is_unresolved_klass()) {
1407     Klass* tk = constant_pool-&gt;klass_at_ignore_error(index, CHECK);
1408     return;
1409   }
1410 
1411   if (!constant_pool-&gt;tag_at(index).is_symbol()) return;
1412 
1413   Handle class_loader (THREAD, constant_pool-&gt;pool_holder()-&gt;class_loader());
1414   Symbol*  symbol  = constant_pool-&gt;symbol_at(index);
1415 
1416   // class name?
1417   if (symbol-&gt;byte_at(0) != '(') {
1418     Handle protection_domain (THREAD, constant_pool-&gt;pool_holder()-&gt;protection_domain());
1419     SystemDictionary::resolve_or_null(symbol, class_loader, protection_domain, CHECK);
1420     return;
1421   }
1422 
1423   // then it must be a signature!
1424   ResourceMark rm(THREAD);
1425   for (SignatureStream ss(symbol); !ss.is_done(); ss.next()) {
1426     if (ss.is_object()) {
1427       Symbol* class_name = ss.as_symbol(CHECK);
1428       Handle protection_domain (THREAD, constant_pool-&gt;pool_holder()-&gt;protection_domain());
1429       SystemDictionary::resolve_or_null(class_name, class_loader, protection_domain, CHECK);
1430     }
1431   }
1432 }
1433 
1434 
1435 void Deoptimization::load_class_by_index(const constantPoolHandle&amp; constant_pool, int index) {
1436   EXCEPTION_MARK;
1437   load_class_by_index(constant_pool, index, THREAD);
1438   if (HAS_PENDING_EXCEPTION) {
1439     // Exception happened during classloading. We ignore the exception here, since it
1440     // is going to be rethrown since the current activation is going to be deoptimized and
1441     // the interpreter will re-execute the bytecode.
1442     CLEAR_PENDING_EXCEPTION;
1443     // Class loading called java code which may have caused a stack
1444     // overflow. If the exception was thrown right before the return
1445     // to the runtime the stack is no longer guarded. Reguard the
1446     // stack otherwise if we return to the uncommon trap blob and the
1447     // stack bang causes a stack overflow we crash.
1448     assert(THREAD-&gt;is_Java_thread(), "only a java thread can be here");
1449     JavaThread* thread = (JavaThread*)THREAD;
1450     bool guard_pages_enabled = thread-&gt;stack_guards_enabled();
1451     if (!guard_pages_enabled) guard_pages_enabled = thread-&gt;reguard_stack();
1452     assert(guard_pages_enabled, "stack banging in uncommon trap blob may cause crash");
1453   }
1454 }
1455 
1456 JRT_ENTRY(void, Deoptimization::uncommon_trap_inner(JavaThread* thread, jint trap_request)) {
1457   HandleMark hm;
1458 
1459   // uncommon_trap() is called at the beginning of the uncommon trap
1460   // handler. Note this fact before we start generating temporary frames
1461   // that can confuse an asynchronous stack walker. This counter is
1462   // decremented at the end of unpack_frames().
1463   thread-&gt;inc_in_deopt_handler();
1464 
1465   // We need to update the map if we have biased locking.
1466 #if INCLUDE_JVMCI
1467   // JVMCI might need to get an exception from the stack, which in turn requires the register map to be valid
1468   RegisterMap reg_map(thread, true);
1469 #else
1470   RegisterMap reg_map(thread, UseBiasedLocking);
1471 #endif
1472   frame stub_frame = thread-&gt;last_frame();
1473   frame fr = stub_frame.sender(&amp;reg_map);
1474   // Make sure the calling nmethod is not getting deoptimized and removed
1475   // before we are done with it.
1476   nmethodLocker nl(fr.pc());
1477 
1478   // Log a message
1479   Events::log(thread, "Uncommon trap: trap_request=" PTR32_FORMAT " fr.pc=" INTPTR_FORMAT " relative=" INTPTR_FORMAT,
1480               trap_request, p2i(fr.pc()), fr.pc() - fr.cb()-&gt;code_begin());
1481 
1482   {
1483     ResourceMark rm;
1484 
1485     // Revoke biases of any monitors in the frame to ensure we can migrate them
1486     revoke_biases_of_monitors(thread, fr, &amp;reg_map);
1487 
1488     DeoptReason reason = trap_request_reason(trap_request);
1489     DeoptAction action = trap_request_action(trap_request);
1490 #if INCLUDE_JVMCI
1491     int debug_id = trap_request_debug_id(trap_request);
1492 #endif
1493     jint unloaded_class_index = trap_request_index(trap_request); // CP idx or -1
1494 
1495     vframe*  vf  = vframe::new_vframe(&amp;fr, &amp;reg_map, thread);
1496     compiledVFrame* cvf = compiledVFrame::cast(vf);
1497 
1498     nmethod* nm = cvf-&gt;code();
1499 
1500     ScopeDesc*      trap_scope  = cvf-&gt;scope();
1501 
1502     if (TraceDeoptimization) {
1503       ttyLocker ttyl;
1504       tty-&gt;print_cr("  bci=%d pc=" INTPTR_FORMAT ", relative_pc=" INTPTR_FORMAT ", method=%s" JVMCI_ONLY(", debug_id=%d"), trap_scope-&gt;bci(), p2i(fr.pc()), fr.pc() - nm-&gt;code_begin(), trap_scope-&gt;method()-&gt;name_and_sig_as_C_string()
1505 #if INCLUDE_JVMCI
1506           , debug_id
1507 #endif
1508           );
1509     }
1510 
1511     methodHandle    trap_method = trap_scope-&gt;method();
1512     int             trap_bci    = trap_scope-&gt;bci();
1513 #if INCLUDE_JVMCI
1514     oop speculation = thread-&gt;pending_failed_speculation();
1515     if (nm-&gt;is_compiled_by_jvmci()) {
1516       if (speculation != NULL) {
1517         oop speculation_log = nm-&gt;speculation_log();
1518         if (speculation_log != NULL) {
1519           if (TraceDeoptimization || TraceUncollectedSpeculations) {
1520             if (HotSpotSpeculationLog::lastFailed(speculation_log) != NULL) {
1521               tty-&gt;print_cr("A speculation that was not collected by the compiler is being overwritten");
1522             }
1523           }
1524           if (TraceDeoptimization) {
1525             tty-&gt;print_cr("Saving speculation to speculation log");
1526           }
1527           HotSpotSpeculationLog::set_lastFailed(speculation_log, speculation);
1528         } else {
1529           if (TraceDeoptimization) {
1530             tty-&gt;print_cr("Speculation present but no speculation log");
1531           }
1532         }
1533         thread-&gt;set_pending_failed_speculation(NULL);
1534       } else {
1535         if (TraceDeoptimization) {
1536           tty-&gt;print_cr("No speculation");
1537         }
1538       }
1539     } else {
1540       assert(speculation == NULL, "There should not be a speculation for method compiled by non-JVMCI compilers");
1541     }
1542 
1543     if (trap_bci == SynchronizationEntryBCI) {
1544       trap_bci = 0;
1545       thread-&gt;set_pending_monitorenter(true);
1546     }
1547 
1548     if (reason == Deoptimization::Reason_transfer_to_interpreter) {
1549       thread-&gt;set_pending_transfer_to_interpreter(true);
1550     }
1551 #endif
1552 
1553     Bytecodes::Code trap_bc     = trap_method-&gt;java_code_at(trap_bci);
1554     // Record this event in the histogram.
1555     gather_statistics(reason, action, trap_bc);
1556 
1557     // Ensure that we can record deopt. history:
1558     // Need MDO to record RTM code generation state.
1559     bool create_if_missing = ProfileTraps || UseCodeAging RTM_OPT_ONLY( || UseRTMLocking );
1560 
1561     methodHandle profiled_method;
1562 #if INCLUDE_JVMCI
1563     if (nm-&gt;is_compiled_by_jvmci()) {
1564       profiled_method = nm-&gt;method();
1565     } else {
1566       profiled_method = trap_method;
1567     }
1568 #else
1569     profiled_method = trap_method;
1570 #endif
1571 
1572     MethodData* trap_mdo =
1573       get_method_data(thread, profiled_method, create_if_missing);
1574 
1575     // Log a message
1576     Events::log_deopt_message(thread, "Uncommon trap: reason=%s action=%s pc=" INTPTR_FORMAT " method=%s @ %d",
1577                               trap_reason_name(reason), trap_action_name(action), p2i(fr.pc()),
1578                               trap_method-&gt;name_and_sig_as_C_string(), trap_bci);
1579 
1580     // Print a bunch of diagnostics, if requested.
1581     if (TraceDeoptimization || LogCompilation) {
1582       ResourceMark rm;
1583       ttyLocker ttyl;
1584       char buf[100];
1585       if (xtty != NULL) {
1586         xtty-&gt;begin_head("uncommon_trap thread='" UINTX_FORMAT "' %s",
1587                          os::current_thread_id(),
1588                          format_trap_request(buf, sizeof(buf), trap_request));
1589         nm-&gt;log_identity(xtty);
1590       }
1591       Symbol* class_name = NULL;
1592       bool unresolved = false;
1593       if (unloaded_class_index &gt;= 0) {
1594         constantPoolHandle constants (THREAD, trap_method-&gt;constants());
1595         if (constants-&gt;tag_at(unloaded_class_index).is_unresolved_klass()) {
1596           class_name = constants-&gt;klass_name_at(unloaded_class_index);
1597           unresolved = true;
1598           if (xtty != NULL)
1599             xtty-&gt;print(" unresolved='1'");
1600         } else if (constants-&gt;tag_at(unloaded_class_index).is_symbol()) {
1601           class_name = constants-&gt;symbol_at(unloaded_class_index);
1602         }
1603         if (xtty != NULL)
1604           xtty-&gt;name(class_name);
1605       }
1606       if (xtty != NULL &amp;&amp; trap_mdo != NULL &amp;&amp; (int)reason &lt; (int)MethodData::_trap_hist_limit) {
1607         // Dump the relevant MDO state.
1608         // This is the deopt count for the current reason, any previous
1609         // reasons or recompiles seen at this point.
1610         int dcnt = trap_mdo-&gt;trap_count(reason);
1611         if (dcnt != 0)
1612           xtty-&gt;print(" count='%d'", dcnt);
1613         ProfileData* pdata = trap_mdo-&gt;bci_to_data(trap_bci);
1614         int dos = (pdata == NULL)? 0: pdata-&gt;trap_state();
1615         if (dos != 0) {
1616           xtty-&gt;print(" state='%s'", format_trap_state(buf, sizeof(buf), dos));
1617           if (trap_state_is_recompiled(dos)) {
1618             int recnt2 = trap_mdo-&gt;overflow_recompile_count();
1619             if (recnt2 != 0)
1620               xtty-&gt;print(" recompiles2='%d'", recnt2);
1621           }
1622         }
1623       }
1624       if (xtty != NULL) {
1625         xtty-&gt;stamp();
1626         xtty-&gt;end_head();
1627       }
1628       if (TraceDeoptimization) {  // make noise on the tty
1629         tty-&gt;print("Uncommon trap occurred in");
1630         nm-&gt;method()-&gt;print_short_name(tty);
1631         tty-&gt;print(" compiler=%s compile_id=%d", nm-&gt;compiler() == NULL ? "" : nm-&gt;compiler()-&gt;name(), nm-&gt;compile_id());
1632 #if INCLUDE_JVMCI
1633         oop installedCode = nm-&gt;jvmci_installed_code();
1634         if (installedCode != NULL) {
1635           oop installedCodeName = NULL;
1636           if (installedCode-&gt;is_a(InstalledCode::klass())) {
1637             installedCodeName = InstalledCode::name(installedCode);
1638           }
1639           if (installedCodeName != NULL) {
1640             tty-&gt;print(" (JVMCI: installedCodeName=%s) ", java_lang_String::as_utf8_string(installedCodeName));
1641           } else {
1642             tty-&gt;print(" (JVMCI: installed code has no name) ");
1643           }
1644         } else if (nm-&gt;is_compiled_by_jvmci()) {
1645           tty-&gt;print(" (JVMCI: no installed code) ");
1646         }
1647 #endif
1648         tty-&gt;print(" (@" INTPTR_FORMAT ") thread=" UINTX_FORMAT " reason=%s action=%s unloaded_class_index=%d" JVMCI_ONLY(" debug_id=%d"),
1649                    p2i(fr.pc()),
1650                    os::current_thread_id(),
1651                    trap_reason_name(reason),
1652                    trap_action_name(action),
1653                    unloaded_class_index
1654 #if INCLUDE_JVMCI
1655                    , debug_id
1656 #endif
1657                    );
1658         if (class_name != NULL) {
1659           tty-&gt;print(unresolved ? " unresolved class: " : " symbol: ");
1660           class_name-&gt;print_symbol_on(tty);
1661         }
1662         tty-&gt;cr();
1663       }
1664       if (xtty != NULL) {
1665         // Log the precise location of the trap.
1666         for (ScopeDesc* sd = trap_scope; ; sd = sd-&gt;sender()) {
1667           xtty-&gt;begin_elem("jvms bci='%d'", sd-&gt;bci());
1668           xtty-&gt;method(sd-&gt;method());
1669           xtty-&gt;end_elem();
1670           if (sd-&gt;is_top())  break;
1671         }
1672         xtty-&gt;tail("uncommon_trap");
1673       }
1674     }
1675     // (End diagnostic printout.)
1676 
1677     // Load class if necessary
1678     if (unloaded_class_index &gt;= 0) {
1679       constantPoolHandle constants(THREAD, trap_method-&gt;constants());
1680       load_class_by_index(constants, unloaded_class_index);
1681     }
1682 
1683     // Flush the nmethod if necessary and desirable.
1684     //
1685     // We need to avoid situations where we are re-flushing the nmethod
1686     // because of a hot deoptimization site.  Repeated flushes at the same
1687     // point need to be detected by the compiler and avoided.  If the compiler
1688     // cannot avoid them (or has a bug and "refuses" to avoid them), this
1689     // module must take measures to avoid an infinite cycle of recompilation
1690     // and deoptimization.  There are several such measures:
1691     //
1692     //   1. If a recompilation is ordered a second time at some site X
1693     //   and for the same reason R, the action is adjusted to 'reinterpret',
1694     //   to give the interpreter time to exercise the method more thoroughly.
1695     //   If this happens, the method's overflow_recompile_count is incremented.
1696     //
1697     //   2. If the compiler fails to reduce the deoptimization rate, then
1698     //   the method's overflow_recompile_count will begin to exceed the set
1699     //   limit PerBytecodeRecompilationCutoff.  If this happens, the action
1700     //   is adjusted to 'make_not_compilable', and the method is abandoned
1701     //   to the interpreter.  This is a performance hit for hot methods,
1702     //   but is better than a disastrous infinite cycle of recompilations.
1703     //   (Actually, only the method containing the site X is abandoned.)
1704     //
1705     //   3. In parallel with the previous measures, if the total number of
1706     //   recompilations of a method exceeds the much larger set limit
1707     //   PerMethodRecompilationCutoff, the method is abandoned.
1708     //   This should only happen if the method is very large and has
1709     //   many "lukewarm" deoptimizations.  The code which enforces this
1710     //   limit is elsewhere (class nmethod, class Method).
1711     //
1712     // Note that the per-BCI 'is_recompiled' bit gives the compiler one chance
1713     // to recompile at each bytecode independently of the per-BCI cutoff.
1714     //
1715     // The decision to update code is up to the compiler, and is encoded
1716     // in the Action_xxx code.  If the compiler requests Action_none
1717     // no trap state is changed, no compiled code is changed, and the
1718     // computation suffers along in the interpreter.
1719     //
1720     // The other action codes specify various tactics for decompilation
1721     // and recompilation.  Action_maybe_recompile is the loosest, and
1722     // allows the compiled code to stay around until enough traps are seen,
1723     // and until the compiler gets around to recompiling the trapping method.
1724     //
1725     // The other actions cause immediate removal of the present code.
1726 
1727     // Traps caused by injected profile shouldn't pollute trap counts.
1728     bool injected_profile_trap = trap_method-&gt;has_injected_profile() &amp;&amp;
1729                                  (reason == Reason_intrinsic || reason == Reason_unreached);
1730 
1731     bool update_trap_state = (reason != Reason_tenured) &amp;&amp; !injected_profile_trap;
1732     bool make_not_entrant = false;
1733     bool make_not_compilable = false;
1734     bool reprofile = false;
1735     switch (action) {
1736     case Action_none:
1737       // Keep the old code.
1738       update_trap_state = false;
1739       break;
1740     case Action_maybe_recompile:
1741       // Do not need to invalidate the present code, but we can
1742       // initiate another
1743       // Start compiler without (necessarily) invalidating the nmethod.
1744       // The system will tolerate the old code, but new code should be
1745       // generated when possible.
1746       break;
1747     case Action_reinterpret:
1748       // Go back into the interpreter for a while, and then consider
1749       // recompiling form scratch.
1750       make_not_entrant = true;
1751       // Reset invocation counter for outer most method.
1752       // This will allow the interpreter to exercise the bytecodes
1753       // for a while before recompiling.
1754       // By contrast, Action_make_not_entrant is immediate.
1755       //
1756       // Note that the compiler will track null_check, null_assert,
1757       // range_check, and class_check events and log them as if they
1758       // had been traps taken from compiled code.  This will update
1759       // the MDO trap history so that the next compilation will
1760       // properly detect hot trap sites.
1761       reprofile = true;
1762       break;
1763     case Action_make_not_entrant:
1764       // Request immediate recompilation, and get rid of the old code.
1765       // Make them not entrant, so next time they are called they get
1766       // recompiled.  Unloaded classes are loaded now so recompile before next
1767       // time they are called.  Same for uninitialized.  The interpreter will
1768       // link the missing class, if any.
1769       make_not_entrant = true;
1770       break;
1771     case Action_make_not_compilable:
1772       // Give up on compiling this method at all.
1773       make_not_entrant = true;
1774       make_not_compilable = true;
1775       break;
1776     default:
1777       ShouldNotReachHere();
1778     }
1779 
1780     // Setting +ProfileTraps fixes the following, on all platforms:
1781     // 4852688: ProfileInterpreter is off by default for ia64.  The result is
1782     // infinite heroic-opt-uncommon-trap/deopt/recompile cycles, since the
1783     // recompile relies on a MethodData* to record heroic opt failures.
1784 
1785     // Whether the interpreter is producing MDO data or not, we also need
1786     // to use the MDO to detect hot deoptimization points and control
1787     // aggressive optimization.
1788     bool inc_recompile_count = false;
1789     ProfileData* pdata = NULL;
1790     if (ProfileTraps &amp;&amp; update_trap_state &amp;&amp; trap_mdo != NULL) {
1791       assert(trap_mdo == get_method_data(thread, profiled_method, false), "sanity");
1792       uint this_trap_count = 0;
1793       bool maybe_prior_trap = false;
1794       bool maybe_prior_recompile = false;
1795       pdata = query_update_method_data(trap_mdo, trap_bci, reason, true,
1796 #if INCLUDE_JVMCI
1797                                    nm-&gt;is_compiled_by_jvmci() &amp;&amp; nm-&gt;is_osr_method(),
1798 #endif
1799                                    nm-&gt;method(),
1800                                    //outputs:
1801                                    this_trap_count,
1802                                    maybe_prior_trap,
1803                                    maybe_prior_recompile);
1804       // Because the interpreter also counts null, div0, range, and class
1805       // checks, these traps from compiled code are double-counted.
1806       // This is harmless; it just means that the PerXTrapLimit values
1807       // are in effect a little smaller than they look.
1808 
1809       DeoptReason per_bc_reason = reason_recorded_per_bytecode_if_any(reason);
1810       if (per_bc_reason != Reason_none) {
1811         // Now take action based on the partially known per-BCI history.
1812         if (maybe_prior_trap
1813             &amp;&amp; this_trap_count &gt;= (uint)PerBytecodeTrapLimit) {
1814           // If there are too many traps at this BCI, force a recompile.
1815           // This will allow the compiler to see the limit overflow, and
1816           // take corrective action, if possible.  The compiler generally
1817           // does not use the exact PerBytecodeTrapLimit value, but instead
1818           // changes its tactics if it sees any traps at all.  This provides
1819           // a little hysteresis, delaying a recompile until a trap happens
1820           // several times.
1821           //
1822           // Actually, since there is only one bit of counter per BCI,
1823           // the possible per-BCI counts are {0,1,(per-method count)}.
1824           // This produces accurate results if in fact there is only
1825           // one hot trap site, but begins to get fuzzy if there are
1826           // many sites.  For example, if there are ten sites each
1827           // trapping two or more times, they each get the blame for
1828           // all of their traps.
1829           make_not_entrant = true;
1830         }
1831 
1832         // Detect repeated recompilation at the same BCI, and enforce a limit.
1833         if (make_not_entrant &amp;&amp; maybe_prior_recompile) {
1834           // More than one recompile at this point.
1835           inc_recompile_count = maybe_prior_trap;
1836         }
1837       } else {
1838         // For reasons which are not recorded per-bytecode, we simply
1839         // force recompiles unconditionally.
1840         // (Note that PerMethodRecompilationCutoff is enforced elsewhere.)
1841         make_not_entrant = true;
1842       }
1843 
1844       // Go back to the compiler if there are too many traps in this method.
1845       if (this_trap_count &gt;= per_method_trap_limit(reason)) {
1846         // If there are too many traps in this method, force a recompile.
1847         // This will allow the compiler to see the limit overflow, and
1848         // take corrective action, if possible.
1849         // (This condition is an unlikely backstop only, because the
1850         // PerBytecodeTrapLimit is more likely to take effect first,
1851         // if it is applicable.)
1852         make_not_entrant = true;
1853       }
1854 
1855       // Here's more hysteresis:  If there has been a recompile at
1856       // this trap point already, run the method in the interpreter
1857       // for a while to exercise it more thoroughly.
1858       if (make_not_entrant &amp;&amp; maybe_prior_recompile &amp;&amp; maybe_prior_trap) {
1859         reprofile = true;
1860       }
1861     }
1862 
1863     // Take requested actions on the method:
1864 
1865     // Recompile
1866     if (make_not_entrant) {
1867       if (!nm-&gt;make_not_entrant()) {
1868         return; // the call did not change nmethod's state
1869       }
1870 
1871       if (pdata != NULL) {
1872         // Record the recompilation event, if any.
1873         int tstate0 = pdata-&gt;trap_state();
1874         int tstate1 = trap_state_set_recompiled(tstate0, true);
1875         if (tstate1 != tstate0)
1876           pdata-&gt;set_trap_state(tstate1);
1877       }
1878 
1879 #if INCLUDE_RTM_OPT
1880       // Restart collecting RTM locking abort statistic if the method
1881       // is recompiled for a reason other than RTM state change.
1882       // Assume that in new recompiled code the statistic could be different,
1883       // for example, due to different inlining.
1884       if ((reason != Reason_rtm_state_change) &amp;&amp; (trap_mdo != NULL) &amp;&amp;
1885           UseRTMDeopt &amp;&amp; (nm-&gt;rtm_state() != ProfileRTM)) {
1886         trap_mdo-&gt;atomic_set_rtm_state(ProfileRTM);
1887       }
1888 #endif
1889       // For code aging we count traps separately here, using make_not_entrant()
1890       // as a guard against simultaneous deopts in multiple threads.
1891       if (reason == Reason_tenured &amp;&amp; trap_mdo != NULL) {
1892         trap_mdo-&gt;inc_tenure_traps();
1893       }
1894     }
1895 
1896     if (inc_recompile_count) {
1897       trap_mdo-&gt;inc_overflow_recompile_count();
1898       if ((uint)trap_mdo-&gt;overflow_recompile_count() &gt;
1899           (uint)PerBytecodeRecompilationCutoff) {
1900         // Give up on the method containing the bad BCI.
1901         if (trap_method() == nm-&gt;method()) {
1902           make_not_compilable = true;
1903         } else {
1904           trap_method-&gt;set_not_compilable(CompLevel_full_optimization, true, "overflow_recompile_count &gt; PerBytecodeRecompilationCutoff");
1905           // But give grace to the enclosing nm-&gt;method().
1906         }
1907       }
1908     }
1909 
1910     // Reprofile
1911     if (reprofile) {
1912       CompilationPolicy::policy()-&gt;reprofile(trap_scope, nm-&gt;is_osr_method());
1913     }
1914 
1915     // Give up compiling
1916     if (make_not_compilable &amp;&amp; !nm-&gt;method()-&gt;is_not_compilable(CompLevel_full_optimization)) {
1917       assert(make_not_entrant, "consistent");
1918       nm-&gt;method()-&gt;set_not_compilable(CompLevel_full_optimization);
1919     }
1920 
1921   } // Free marked resources
1922 
1923 }
1924 JRT_END
1925 
1926 ProfileData*
1927 Deoptimization::query_update_method_data(MethodData* trap_mdo,
1928                                          int trap_bci,
1929                                          Deoptimization::DeoptReason reason,
1930                                          bool update_total_trap_count,
1931 #if INCLUDE_JVMCI
1932                                          bool is_osr,
1933 #endif
1934                                          Method* compiled_method,
1935                                          //outputs:
1936                                          uint&amp; ret_this_trap_count,
1937                                          bool&amp; ret_maybe_prior_trap,
1938                                          bool&amp; ret_maybe_prior_recompile) {
1939   bool maybe_prior_trap = false;
1940   bool maybe_prior_recompile = false;
1941   uint this_trap_count = 0;
1942   if (update_total_trap_count) {
1943     uint idx = reason;
1944 #if INCLUDE_JVMCI
1945     if (is_osr) {
1946       idx += Reason_LIMIT;
1947     }
1948 #endif
1949     uint prior_trap_count = trap_mdo-&gt;trap_count(idx);
1950     this_trap_count  = trap_mdo-&gt;inc_trap_count(idx);
1951 
1952     // If the runtime cannot find a place to store trap history,
1953     // it is estimated based on the general condition of the method.
1954     // If the method has ever been recompiled, or has ever incurred
1955     // a trap with the present reason , then this BCI is assumed
1956     // (pessimistically) to be the culprit.
1957     maybe_prior_trap      = (prior_trap_count != 0);
1958     maybe_prior_recompile = (trap_mdo-&gt;decompile_count() != 0);
1959   }
1960   ProfileData* pdata = NULL;
1961 
1962 
1963   // For reasons which are recorded per bytecode, we check per-BCI data.
1964   DeoptReason per_bc_reason = reason_recorded_per_bytecode_if_any(reason);
1965   assert(per_bc_reason != Reason_none || update_total_trap_count, "must be");
1966   if (per_bc_reason != Reason_none) {
1967     // Find the profile data for this BCI.  If there isn't one,
1968     // try to allocate one from the MDO's set of spares.
1969     // This will let us detect a repeated trap at this point.
1970     pdata = trap_mdo-&gt;allocate_bci_to_data(trap_bci, reason_is_speculate(reason) ? compiled_method : NULL);
1971 
1972     if (pdata != NULL) {
1973       if (reason_is_speculate(reason) &amp;&amp; !pdata-&gt;is_SpeculativeTrapData()) {
1974         if (LogCompilation &amp;&amp; xtty != NULL) {
1975           ttyLocker ttyl;
1976           // no more room for speculative traps in this MDO
1977           xtty-&gt;elem("speculative_traps_oom");
1978         }
1979       }
1980       // Query the trap state of this profile datum.
1981       int tstate0 = pdata-&gt;trap_state();
1982       if (!trap_state_has_reason(tstate0, per_bc_reason))
1983         maybe_prior_trap = false;
1984       if (!trap_state_is_recompiled(tstate0))
1985         maybe_prior_recompile = false;
1986 
1987       // Update the trap state of this profile datum.
1988       int tstate1 = tstate0;
1989       // Record the reason.
1990       tstate1 = trap_state_add_reason(tstate1, per_bc_reason);
1991       // Store the updated state on the MDO, for next time.
1992       if (tstate1 != tstate0)
1993         pdata-&gt;set_trap_state(tstate1);
1994     } else {
1995       if (LogCompilation &amp;&amp; xtty != NULL) {
1996         ttyLocker ttyl;
1997         // Missing MDP?  Leave a small complaint in the log.
1998         xtty-&gt;elem("missing_mdp bci='%d'", trap_bci);
1999       }
2000     }
2001   }
2002 
2003   // Return results:
2004   ret_this_trap_count = this_trap_count;
2005   ret_maybe_prior_trap = maybe_prior_trap;
2006   ret_maybe_prior_recompile = maybe_prior_recompile;
2007   return pdata;
2008 }
2009 
2010 void
2011 Deoptimization::update_method_data_from_interpreter(MethodData* trap_mdo, int trap_bci, int reason) {
2012   ResourceMark rm;
2013   // Ignored outputs:
2014   uint ignore_this_trap_count;
2015   bool ignore_maybe_prior_trap;
2016   bool ignore_maybe_prior_recompile;
2017   assert(!reason_is_speculate(reason), "reason speculate only used by compiler");
2018   // JVMCI uses the total counts to determine if deoptimizations are happening too frequently -&gt; do not adjust total counts
2019   bool update_total_counts = JVMCI_ONLY(false) NOT_JVMCI(true);
2020   query_update_method_data(trap_mdo, trap_bci,
2021                            (DeoptReason)reason,
2022                            update_total_counts,
2023 #if INCLUDE_JVMCI
2024                            false,
2025 #endif
2026                            NULL,
2027                            ignore_this_trap_count,
2028                            ignore_maybe_prior_trap,
2029                            ignore_maybe_prior_recompile);
2030 }
2031 
2032 Deoptimization::UnrollBlock* Deoptimization::uncommon_trap(JavaThread* thread, jint trap_request, jint exec_mode) {
2033   if (TraceDeoptimization) {
2034     tty-&gt;print("Uncommon trap ");
2035   }
2036   // Still in Java no safepoints
2037   {
2038     // This enters VM and may safepoint
2039     uncommon_trap_inner(thread, trap_request);
2040   }
2041   return fetch_unroll_info_helper(thread, exec_mode);
2042 }
2043 
2044 // Local derived constants.
2045 // Further breakdown of DataLayout::trap_state, as promised by DataLayout.
2046 const int DS_REASON_MASK   = DataLayout::trap_mask &gt;&gt; 1;
2047 const int DS_RECOMPILE_BIT = DataLayout::trap_mask - DS_REASON_MASK;
2048 
2049 //---------------------------trap_state_reason---------------------------------
2050 Deoptimization::DeoptReason
2051 Deoptimization::trap_state_reason(int trap_state) {
2052   // This assert provides the link between the width of DataLayout::trap_bits
2053   // and the encoding of "recorded" reasons.  It ensures there are enough
2054   // bits to store all needed reasons in the per-BCI MDO profile.
2055   assert(DS_REASON_MASK &gt;= Reason_RECORDED_LIMIT, "enough bits");
2056   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2057   trap_state -= recompile_bit;
2058   if (trap_state == DS_REASON_MASK) {
2059     return Reason_many;
2060   } else {
2061     assert((int)Reason_none == 0, "state=0 =&gt; Reason_none");
2062     return (DeoptReason)trap_state;
2063   }
2064 }
2065 //-------------------------trap_state_has_reason-------------------------------
2066 int Deoptimization::trap_state_has_reason(int trap_state, int reason) {
2067   assert(reason_is_recorded_per_bytecode((DeoptReason)reason), "valid reason");
2068   assert(DS_REASON_MASK &gt;= Reason_RECORDED_LIMIT, "enough bits");
2069   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2070   trap_state -= recompile_bit;
2071   if (trap_state == DS_REASON_MASK) {
2072     return -1;  // true, unspecifically (bottom of state lattice)
2073   } else if (trap_state == reason) {
2074     return 1;   // true, definitely
2075   } else if (trap_state == 0) {
2076     return 0;   // false, definitely (top of state lattice)
2077   } else {
2078     return 0;   // false, definitely
2079   }
2080 }
2081 //-------------------------trap_state_add_reason-------------------------------
2082 int Deoptimization::trap_state_add_reason(int trap_state, int reason) {
2083   assert(reason_is_recorded_per_bytecode((DeoptReason)reason) || reason == Reason_many, "valid reason");
2084   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2085   trap_state -= recompile_bit;
2086   if (trap_state == DS_REASON_MASK) {
2087     return trap_state + recompile_bit;     // already at state lattice bottom
2088   } else if (trap_state == reason) {
2089     return trap_state + recompile_bit;     // the condition is already true
2090   } else if (trap_state == 0) {
2091     return reason + recompile_bit;          // no condition has yet been true
2092   } else {
2093     return DS_REASON_MASK + recompile_bit;  // fall to state lattice bottom
2094   }
2095 }
2096 //-----------------------trap_state_is_recompiled------------------------------
2097 bool Deoptimization::trap_state_is_recompiled(int trap_state) {
2098   return (trap_state &amp; DS_RECOMPILE_BIT) != 0;
2099 }
2100 //-----------------------trap_state_set_recompiled-----------------------------
2101 int Deoptimization::trap_state_set_recompiled(int trap_state, bool z) {
2102   if (z)  return trap_state |  DS_RECOMPILE_BIT;
2103   else    return trap_state &amp; ~DS_RECOMPILE_BIT;
2104 }
2105 //---------------------------format_trap_state---------------------------------
2106 // This is used for debugging and diagnostics, including LogFile output.
2107 const char* Deoptimization::format_trap_state(char* buf, size_t buflen,
2108                                               int trap_state) {
2109   assert(buflen &gt; 0, "sanity");
2110   DeoptReason reason      = trap_state_reason(trap_state);
2111   bool        recomp_flag = trap_state_is_recompiled(trap_state);
2112   // Re-encode the state from its decoded components.
2113   int decoded_state = 0;
2114   if (reason_is_recorded_per_bytecode(reason) || reason == Reason_many)
2115     decoded_state = trap_state_add_reason(decoded_state, reason);
2116   if (recomp_flag)
2117     decoded_state = trap_state_set_recompiled(decoded_state, recomp_flag);
2118   // If the state re-encodes properly, format it symbolically.
2119   // Because this routine is used for debugging and diagnostics,
2120   // be robust even if the state is a strange value.
2121   size_t len;
2122   if (decoded_state != trap_state) {
2123     // Random buggy state that doesn't decode??
2124     len = jio_snprintf(buf, buflen, "#%d", trap_state);
2125   } else {
2126     len = jio_snprintf(buf, buflen, "%s%s",
2127                        trap_reason_name(reason),
2128                        recomp_flag ? " recompiled" : "");
2129   }
2130   return buf;
2131 }
2132 
2133 
2134 //--------------------------------statics--------------------------------------
2135 const char* Deoptimization::_trap_reason_name[] = {
2136   // Note:  Keep this in sync. with enum DeoptReason.
2137   "none",
2138   "null_check",
2139   "null_assert" JVMCI_ONLY("_or_unreached0"),
2140   "range_check",
2141   "class_check",
2142   "array_check",
2143   "intrinsic" JVMCI_ONLY("_or_type_checked_inlining"),
2144   "bimorphic" JVMCI_ONLY("_or_optimized_type_check"),
2145   "unloaded",
2146   "uninitialized",
2147   "unreached",
2148   "unhandled",
2149   "constraint",
2150   "div0_check",
2151   "age",
2152   "predicate",
2153   "loop_limit_check",
2154   "speculate_class_check",
2155   "speculate_null_check",
2156   "rtm_state_change",
2157   "unstable_if",
2158   "unstable_fused_if",
2159 #if INCLUDE_JVMCI
2160   "aliasing",
2161   "transfer_to_interpreter",
2162   "not_compiled_exception_handler",
2163   "unresolved",
2164   "jsr_mismatch",
2165 #endif
2166   "tenured"
2167 };
2168 const char* Deoptimization::_trap_action_name[] = {
2169   // Note:  Keep this in sync. with enum DeoptAction.
2170   "none",
2171   "maybe_recompile",
2172   "reinterpret",
2173   "make_not_entrant",
2174   "make_not_compilable"
2175 };
2176 
2177 const char* Deoptimization::trap_reason_name(int reason) {
2178   // Check that every reason has a name
2179   STATIC_ASSERT(sizeof(_trap_reason_name)/sizeof(const char*) == Reason_LIMIT);
2180 
2181   if (reason == Reason_many)  return "many";
2182   if ((uint)reason &lt; Reason_LIMIT)
2183     return _trap_reason_name[reason];
2184   static char buf[20];
2185   sprintf(buf, "reason%d", reason);
2186   return buf;
2187 }
2188 const char* Deoptimization::trap_action_name(int action) {
2189   // Check that every action has a name
2190   STATIC_ASSERT(sizeof(_trap_action_name)/sizeof(const char*) == Action_LIMIT);
2191 
2192   if ((uint)action &lt; Action_LIMIT)
2193     return _trap_action_name[action];
2194   static char buf[20];
2195   sprintf(buf, "action%d", action);
2196   return buf;
2197 }
2198 
2199 // This is used for debugging and diagnostics, including LogFile output.
2200 const char* Deoptimization::format_trap_request(char* buf, size_t buflen,
2201                                                 int trap_request) {
2202   jint unloaded_class_index = trap_request_index(trap_request);
2203   const char* reason = trap_reason_name(trap_request_reason(trap_request));
2204   const char* action = trap_action_name(trap_request_action(trap_request));
2205 #if INCLUDE_JVMCI
2206   int debug_id = trap_request_debug_id(trap_request);
2207 #endif
2208   size_t len;
2209   if (unloaded_class_index &lt; 0) {
2210     len = jio_snprintf(buf, buflen, "reason='%s' action='%s'" JVMCI_ONLY(" debug_id='%d'"),
2211                        reason, action
2212 #if INCLUDE_JVMCI
2213                        ,debug_id
2214 #endif
2215                        );
2216   } else {
2217     len = jio_snprintf(buf, buflen, "reason='%s' action='%s' index='%d'" JVMCI_ONLY(" debug_id='%d'"),
2218                        reason, action, unloaded_class_index
2219 #if INCLUDE_JVMCI
2220                        ,debug_id
2221 #endif
2222                        );
2223   }
2224   return buf;
2225 }
2226 
2227 juint Deoptimization::_deoptimization_hist
2228         [Deoptimization::Reason_LIMIT]
2229     [1 + Deoptimization::Action_LIMIT]
2230         [Deoptimization::BC_CASE_LIMIT]
2231   = {0};
2232 
2233 enum {
2234   LSB_BITS = 8,
2235   LSB_MASK = right_n_bits(LSB_BITS)
2236 };
2237 
2238 void Deoptimization::gather_statistics(DeoptReason reason, DeoptAction action,
2239                                        Bytecodes::Code bc) {
2240   assert(reason &gt;= 0 &amp;&amp; reason &lt; Reason_LIMIT, "oob");
2241   assert(action &gt;= 0 &amp;&amp; action &lt; Action_LIMIT, "oob");
2242   _deoptimization_hist[Reason_none][0][0] += 1;  // total
2243   _deoptimization_hist[reason][0][0]      += 1;  // per-reason total
2244   juint* cases = _deoptimization_hist[reason][1+action];
2245   juint* bc_counter_addr = NULL;
2246   juint  bc_counter      = 0;
2247   // Look for an unused counter, or an exact match to this BC.
2248   if (bc != Bytecodes::_illegal) {
2249     for (int bc_case = 0; bc_case &lt; BC_CASE_LIMIT; bc_case++) {
2250       juint* counter_addr = &amp;cases[bc_case];
2251       juint  counter = *counter_addr;
2252       if ((counter == 0 &amp;&amp; bc_counter_addr == NULL)
2253           || (Bytecodes::Code)(counter &amp; LSB_MASK) == bc) {
2254         // this counter is either free or is already devoted to this BC
2255         bc_counter_addr = counter_addr;
2256         bc_counter = counter | bc;
2257       }
2258     }
2259   }
2260   if (bc_counter_addr == NULL) {
2261     // Overflow, or no given bytecode.
2262     bc_counter_addr = &amp;cases[BC_CASE_LIMIT-1];
2263     bc_counter = (*bc_counter_addr &amp; ~LSB_MASK);  // clear LSB
2264   }
2265   *bc_counter_addr = bc_counter + (1 &lt;&lt; LSB_BITS);
2266 }
2267 
2268 jint Deoptimization::total_deoptimization_count() {
2269   return _deoptimization_hist[Reason_none][0][0];
2270 }
2271 
2272 jint Deoptimization::deoptimization_count(DeoptReason reason) {
2273   assert(reason &gt;= 0 &amp;&amp; reason &lt; Reason_LIMIT, "oob");
2274   return _deoptimization_hist[reason][0][0];
2275 }
2276 
2277 void Deoptimization::print_statistics() {
2278   juint total = total_deoptimization_count();
2279   juint account = total;
2280   if (total != 0) {
2281     ttyLocker ttyl;
2282     if (xtty != NULL)  xtty-&gt;head("statistics type='deoptimization'");
2283     tty-&gt;print_cr("Deoptimization traps recorded:");
2284     #define PRINT_STAT_LINE(name, r) \
2285       tty-&gt;print_cr("  %4d (%4.1f%%) %s", (int)(r), ((r) * 100.0) / total, name);
2286     PRINT_STAT_LINE("total", total);
2287     // For each non-zero entry in the histogram, print the reason,
2288     // the action, and (if specifically known) the type of bytecode.
2289     for (int reason = 0; reason &lt; Reason_LIMIT; reason++) {
2290       for (int action = 0; action &lt; Action_LIMIT; action++) {
2291         juint* cases = _deoptimization_hist[reason][1+action];
2292         for (int bc_case = 0; bc_case &lt; BC_CASE_LIMIT; bc_case++) {
2293           juint counter = cases[bc_case];
2294           if (counter != 0) {
2295             char name[1*K];
2296             Bytecodes::Code bc = (Bytecodes::Code)(counter &amp; LSB_MASK);
2297             if (bc_case == BC_CASE_LIMIT &amp;&amp; (int)bc == 0)
2298               bc = Bytecodes::_illegal;
2299             sprintf(name, "%s/%s/%s",
2300                     trap_reason_name(reason),
2301                     trap_action_name(action),
2302                     Bytecodes::is_defined(bc)? Bytecodes::name(bc): "other");
2303             juint r = counter &gt;&gt; LSB_BITS;
2304             tty-&gt;print_cr("  %40s: " UINT32_FORMAT " (%.1f%%)", name, r, (r * 100.0) / total);
2305             account -= r;
2306           }
2307         }
2308       }
2309     }
2310     if (account != 0) {
2311       PRINT_STAT_LINE("unaccounted", account);
2312     }
2313     #undef PRINT_STAT_LINE
2314     if (xtty != NULL)  xtty-&gt;tail("statistics");
2315   }
2316 }
2317 #else // COMPILER2 || SHARK || INCLUDE_JVMCI
2318 
2319 
2320 // Stubs for C1 only system.
2321 bool Deoptimization::trap_state_is_recompiled(int trap_state) {
2322   return false;
2323 }
2324 
2325 const char* Deoptimization::trap_reason_name(int reason) {
2326   return "unknown";
2327 }
2328 
2329 void Deoptimization::print_statistics() {
2330   // no output
2331 }
2332 
2333 void
2334 Deoptimization::update_method_data_from_interpreter(MethodData* trap_mdo, int trap_bci, int reason) {
2335   // no udpate
2336 }
2337 
2338 int Deoptimization::trap_state_has_reason(int trap_state, int reason) {
2339   return 0;
2340 }
2341 
2342 void Deoptimization::gather_statistics(DeoptReason reason, DeoptAction action,
2343                                        Bytecodes::Code bc) {
2344   // no update
2345 }
2346 
2347 const char* Deoptimization::format_trap_state(char* buf, size_t buflen,
2348                                               int trap_state) {
2349   jio_snprintf(buf, buflen, "#%d", trap_state);
2350   return buf;
2351 }
2352 
2353 #endif // COMPILER2 || SHARK || INCLUDE_JVMCI
</pre></body></html>
