<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/runtime/deoptimization.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2016, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "classfile/systemDictionary.hpp"
  27 #include "code/codeCache.hpp"
  28 #include "code/debugInfoRec.hpp"
  29 #include "code/nmethod.hpp"
  30 #include "code/pcDesc.hpp"
  31 #include "code/scopeDesc.hpp"
  32 #include "interpreter/bytecode.hpp"
  33 #include "interpreter/interpreter.hpp"
  34 #include "interpreter/oopMapCache.hpp"
  35 #include "memory/allocation.inline.hpp"
  36 #include "memory/oopFactory.hpp"
  37 #include "memory/resourceArea.hpp"
  38 #include "oops/method.hpp"
  39 #include "oops/objArrayOop.inline.hpp"
  40 #include "oops/oop.inline.hpp"
  41 #include "oops/fieldStreams.hpp"
  42 #include "oops/verifyOopClosure.hpp"
  43 #include "prims/jvmtiThreadState.hpp"
  44 #include "runtime/biasedLocking.hpp"
  45 #include "runtime/compilationPolicy.hpp"
  46 #include "runtime/deoptimization.hpp"
  47 #include "runtime/interfaceSupport.hpp"
  48 #include "runtime/sharedRuntime.hpp"
  49 #include "runtime/signature.hpp"
  50 #include "runtime/stubRoutines.hpp"
  51 #include "runtime/thread.hpp"
  52 #include "runtime/vframe.hpp"
  53 #include "runtime/vframeArray.hpp"
  54 #include "runtime/vframe_hp.hpp"
  55 #include "utilities/events.hpp"
  56 #include "utilities/xmlstream.hpp"
  57 
  58 #if INCLUDE_JVMCI
  59 #include "jvmci/jvmciRuntime.hpp"
  60 #include "jvmci/jvmciJavaClasses.hpp"
  61 #endif
  62 
  63 
  64 bool DeoptimizationMarker::_is_active = false;
  65 
  66 Deoptimization::UnrollBlock::UnrollBlock(int  size_of_deoptimized_frame,
  67                                          int  caller_adjustment,
  68                                          int  caller_actual_parameters,
  69                                          int  number_of_frames,
  70                                          intptr_t* frame_sizes,
  71                                          address* frame_pcs,
  72                                          BasicType return_type,
  73                                          int exec_mode) {
  74   _size_of_deoptimized_frame = size_of_deoptimized_frame;
  75   _caller_adjustment         = caller_adjustment;
  76   _caller_actual_parameters  = caller_actual_parameters;
  77   _number_of_frames          = number_of_frames;
  78   _frame_sizes               = frame_sizes;
  79   _frame_pcs                 = frame_pcs;
  80   _register_block            = NEW_C_HEAP_ARRAY(intptr_t, RegisterMap::reg_count * 2, mtCompiler);
  81   _return_type               = return_type;
  82   _initial_info              = 0;
  83   // PD (x86 only)
  84   _counter_temp              = 0;
  85   _unpack_kind               = exec_mode;
  86   _sender_sp_temp            = 0;
  87 
  88   _total_frame_sizes         = size_of_frames();
  89   assert(exec_mode &gt;= 0 &amp;&amp; exec_mode &lt; Unpack_LIMIT, "Unexpected exec_mode");
  90 }
  91 
  92 
  93 Deoptimization::UnrollBlock::~UnrollBlock() {
  94   FREE_C_HEAP_ARRAY(intptr_t, _frame_sizes);
  95   FREE_C_HEAP_ARRAY(intptr_t, _frame_pcs);
  96   FREE_C_HEAP_ARRAY(intptr_t, _register_block);
  97 }
  98 
  99 
 100 intptr_t* Deoptimization::UnrollBlock::value_addr_at(int register_number) const {
 101   assert(register_number &lt; RegisterMap::reg_count, "checking register number");
 102   return &amp;_register_block[register_number * 2];
 103 }
 104 
 105 
 106 
 107 int Deoptimization::UnrollBlock::size_of_frames() const {
 108   // Acount first for the adjustment of the initial frame
 109   int result = _caller_adjustment;
 110   for (int index = 0; index &lt; number_of_frames(); index++) {
 111     result += frame_sizes()[index];
 112   }
 113   return result;
 114 }
 115 
 116 
 117 void Deoptimization::UnrollBlock::print() {
 118   ttyLocker ttyl;
 119   tty-&gt;print_cr("UnrollBlock");
 120   tty-&gt;print_cr("  size_of_deoptimized_frame = %d", _size_of_deoptimized_frame);
 121   tty-&gt;print(   "  frame_sizes: ");
 122   for (int index = 0; index &lt; number_of_frames(); index++) {
 123     tty-&gt;print(INTX_FORMAT " ", frame_sizes()[index]);
 124   }
 125   tty-&gt;cr();
 126 }
 127 
 128 
 129 // In order to make fetch_unroll_info work properly with escape
 130 // analysis, The method was changed from JRT_LEAF to JRT_BLOCK_ENTRY and
 131 // ResetNoHandleMark and HandleMark were removed from it. The actual reallocation
 132 // of previously eliminated objects occurs in realloc_objects, which is
 133 // called from the method fetch_unroll_info_helper below.
 134 JRT_BLOCK_ENTRY(Deoptimization::UnrollBlock*, Deoptimization::fetch_unroll_info(JavaThread* thread, int exec_mode))
 135   // It is actually ok to allocate handles in a leaf method. It causes no safepoints,
 136   // but makes the entry a little slower. There is however a little dance we have to
 137   // do in debug mode to get around the NoHandleMark code in the JRT_LEAF macro
 138 
 139   // fetch_unroll_info() is called at the beginning of the deoptimization
 140   // handler. Note this fact before we start generating temporary frames
 141   // that can confuse an asynchronous stack walker. This counter is
 142   // decremented at the end of unpack_frames().
 143   if (TraceDeoptimization) {
 144     tty-&gt;print_cr("Deoptimizing thread " INTPTR_FORMAT, p2i(thread));
 145   }
 146   thread-&gt;inc_in_deopt_handler();
 147 
 148   return fetch_unroll_info_helper(thread, exec_mode);
 149 JRT_END
 150 
 151 
 152 // This is factored, since it is both called from a JRT_LEAF (deoptimization) and a JRT_ENTRY (uncommon_trap)
 153 Deoptimization::UnrollBlock* Deoptimization::fetch_unroll_info_helper(JavaThread* thread, int exec_mode) {
 154 
 155   // Note: there is a safepoint safety issue here. No matter whether we enter
 156   // via vanilla deopt or uncommon trap we MUST NOT stop at a safepoint once
 157   // the vframeArray is created.
 158   //
 159 
 160   // Allocate our special deoptimization ResourceMark
 161   DeoptResourceMark* dmark = new DeoptResourceMark(thread);
 162   assert(thread-&gt;deopt_mark() == NULL, "Pending deopt!");
 163   thread-&gt;set_deopt_mark(dmark);
 164 
 165   frame stub_frame = thread-&gt;last_frame(); // Makes stack walkable as side effect
 166   RegisterMap map(thread, true);
 167   RegisterMap dummy_map(thread, false);
 168   // Now get the deoptee with a valid map
 169   frame deoptee = stub_frame.sender(&amp;map);
 170   // Set the deoptee nmethod
 171   assert(thread-&gt;deopt_nmethod() == NULL, "Pending deopt!");
 172   thread-&gt;set_deopt_nmethod(deoptee.cb()-&gt;as_nmethod_or_null());
 173   bool skip_internal = thread-&gt;deopt_nmethod() != NULL &amp;&amp; !thread-&gt;deopt_nmethod()-&gt;compiler()-&gt;is_jvmci();
 174 
 175   if (VerifyStack) {
 176     thread-&gt;validate_frame_layout();
 177   }
 178 
 179   // Create a growable array of VFrames where each VFrame represents an inlined
 180   // Java frame.  This storage is allocated with the usual system arena.
 181   assert(deoptee.is_compiled_frame(), "Wrong frame type");
 182   GrowableArray&lt;compiledVFrame*&gt;* chunk = new GrowableArray&lt;compiledVFrame*&gt;(10);
 183   vframe* vf = vframe::new_vframe(&amp;deoptee, &amp;map, thread);
 184   while (!vf-&gt;is_top()) {
 185     assert(vf-&gt;is_compiled_frame(), "Wrong frame type");
 186     chunk-&gt;push(compiledVFrame::cast(vf));
 187     vf = vf-&gt;sender();
 188   }
 189   assert(vf-&gt;is_compiled_frame(), "Wrong frame type");
 190   chunk-&gt;push(compiledVFrame::cast(vf));
 191 
 192   ScopeDesc* trap_scope = chunk-&gt;at(0)-&gt;scope();
 193   Handle exceptionObject;
 194   if (trap_scope-&gt;rethrow_exception()) {
 195     if (PrintDeoptimizationDetails) {
 196       tty-&gt;print_cr("Exception to be rethrown in the interpreter for method %s::%s at bci %d", trap_scope-&gt;method()-&gt;method_holder()-&gt;name()-&gt;as_C_string(), trap_scope-&gt;method()-&gt;name()-&gt;as_C_string(), trap_scope-&gt;bci());
 197     }
 198     GrowableArray&lt;ScopeValue*&gt;* expressions = trap_scope-&gt;expressions();
 199     guarantee(expressions != NULL &amp;&amp; expressions-&gt;length() &gt; 0, "must have exception to throw");
 200     ScopeValue* topOfStack = expressions-&gt;top();
 201     exceptionObject = StackValue::create_stack_value(&amp;deoptee, &amp;map, topOfStack)-&gt;get_obj();
 202     assert(exceptionObject() != NULL, "exception oop can not be null");
 203   }
 204 
 205   bool realloc_failures = false;
 206 
 207 #if defined(COMPILER2) || INCLUDE_JVMCI
 208   // Reallocate the non-escaping objects and restore their fields. Then
 209   // relock objects if synchronization on them was eliminated.
 210 #ifndef INCLUDE_JVMCI
 211   if (DoEscapeAnalysis || EliminateNestedLocks) {
 212     if (EliminateAllocations) {
 213 #endif // INCLUDE_JVMCI
 214       assert (chunk-&gt;at(0)-&gt;scope() != NULL,"expect only compiled java frames");
 215       GrowableArray&lt;ScopeValue*&gt;* objects = chunk-&gt;at(0)-&gt;scope()-&gt;objects();
 216 
 217       // The flag return_oop() indicates call sites which return oop
 218       // in compiled code. Such sites include java method calls,
 219       // runtime calls (for example, used to allocate new objects/arrays
 220       // on slow code path) and any other calls generated in compiled code.
 221       // It is not guaranteed that we can get such information here only
 222       // by analyzing bytecode in deoptimized frames. This is why this flag
 223       // is set during method compilation (see Compile::Process_OopMap_Node()).
 224       // If the previous frame was popped, we don't have a result.
 225       bool save_oop_result = chunk-&gt;at(0)-&gt;scope()-&gt;return_oop() &amp;&amp; !thread-&gt;popframe_forcing_deopt_reexecution();
 226       Handle return_value;
 227       if (save_oop_result) {
 228         // Reallocation may trigger GC. If deoptimization happened on return from
 229         // call which returns oop we need to save it since it is not in oopmap.
 230         oop result = deoptee.saved_oop_result(&amp;map);
 231         assert(result == NULL || result-&gt;is_oop(), "must be oop");
 232         return_value = Handle(thread, result);
 233         assert(Universe::heap()-&gt;is_in_or_null(result), "must be heap pointer");
 234         if (TraceDeoptimization) {
 235           ttyLocker ttyl;
 236           tty-&gt;print_cr("SAVED OOP RESULT " INTPTR_FORMAT " in thread " INTPTR_FORMAT, p2i(result), p2i(thread));
 237         }
 238       }
 239       if (objects != NULL) {
 240         JRT_BLOCK
 241           realloc_failures = realloc_objects(thread, &amp;deoptee, objects, THREAD);
 242         JRT_END
 243         reassign_fields(&amp;deoptee, &amp;map, objects, realloc_failures, skip_internal);
 244 #ifndef PRODUCT
 245         if (TraceDeoptimization) {
 246           ttyLocker ttyl;
 247           tty-&gt;print_cr("REALLOC OBJECTS in thread " INTPTR_FORMAT, p2i(thread));
 248           print_objects(objects, realloc_failures);
 249         }
 250 #endif
 251       }
 252       if (save_oop_result) {
 253         // Restore result.
 254         deoptee.set_saved_oop_result(&amp;map, return_value());
 255       }
 256 #ifndef INCLUDE_JVMCI
 257     }
 258     if (EliminateLocks) {
 259 #endif // INCLUDE_JVMCI
 260 #ifndef PRODUCT
 261       bool first = true;
 262 #endif
 263       for (int i = 0; i &lt; chunk-&gt;length(); i++) {
 264         compiledVFrame* cvf = chunk-&gt;at(i);
 265         assert (cvf-&gt;scope() != NULL,"expect only compiled java frames");
 266         GrowableArray&lt;MonitorInfo*&gt;* monitors = cvf-&gt;monitors();
 267         if (monitors-&gt;is_nonempty()) {
 268           relock_objects(monitors, thread, realloc_failures);
 269 #ifndef PRODUCT
 270           if (PrintDeoptimizationDetails) {
 271             ttyLocker ttyl;
 272             for (int j = 0; j &lt; monitors-&gt;length(); j++) {
 273               MonitorInfo* mi = monitors-&gt;at(j);
 274               if (mi-&gt;eliminated()) {
 275                 if (first) {
 276                   first = false;
 277                   tty-&gt;print_cr("RELOCK OBJECTS in thread " INTPTR_FORMAT, p2i(thread));
 278                 }
 279                 if (mi-&gt;owner_is_scalar_replaced()) {
 280                   Klass* k = java_lang_Class::as_Klass(mi-&gt;owner_klass());
 281                   tty-&gt;print_cr("     failed reallocation for klass %s", k-&gt;external_name());
 282                 } else {
 283                   tty-&gt;print_cr("     object &lt;" INTPTR_FORMAT "&gt; locked", p2i(mi-&gt;owner()));
 284                 }
 285               }
 286             }
 287           }
 288 #endif // !PRODUCT
 289         }
 290       }
 291 #ifndef INCLUDE_JVMCI
 292     }
 293   }
 294 #endif // INCLUDE_JVMCI
 295 #endif // COMPILER2 || INCLUDE_JVMCI
 296 
 297   // Ensure that no safepoint is taken after pointers have been stored
 298   // in fields of rematerialized objects.  If a safepoint occurs from here on
 299   // out the java state residing in the vframeArray will be missed.
 300   NoSafepointVerifier no_safepoint;
 301 
 302   vframeArray* array = create_vframeArray(thread, deoptee, &amp;map, chunk, realloc_failures);
 303 #if defined(COMPILER2) || INCLUDE_JVMCI
 304   if (realloc_failures) {
 305     pop_frames_failed_reallocs(thread, array);
 306   }
 307 #endif
 308 
 309   assert(thread-&gt;vframe_array_head() == NULL, "Pending deopt!");
 310   thread-&gt;set_vframe_array_head(array);
 311 
 312   // Now that the vframeArray has been created if we have any deferred local writes
 313   // added by jvmti then we can free up that structure as the data is now in the
 314   // vframeArray
 315 
 316   if (thread-&gt;deferred_locals() != NULL) {
 317     GrowableArray&lt;jvmtiDeferredLocalVariableSet*&gt;* list = thread-&gt;deferred_locals();
 318     int i = 0;
 319     do {
 320       // Because of inlining we could have multiple vframes for a single frame
 321       // and several of the vframes could have deferred writes. Find them all.
 322       if (list-&gt;at(i)-&gt;id() == array-&gt;original().id()) {
 323         jvmtiDeferredLocalVariableSet* dlv = list-&gt;at(i);
 324         list-&gt;remove_at(i);
 325         // individual jvmtiDeferredLocalVariableSet are CHeapObj's
 326         delete dlv;
 327       } else {
 328         i++;
 329       }
 330     } while ( i &lt; list-&gt;length() );
 331     if (list-&gt;length() == 0) {
 332       thread-&gt;set_deferred_locals(NULL);
 333       // free the list and elements back to C heap.
 334       delete list;
 335     }
 336 
 337   }
 338 
 339 #ifndef SHARK
 340   // Compute the caller frame based on the sender sp of stub_frame and stored frame sizes info.
 341   CodeBlob* cb = stub_frame.cb();
 342   // Verify we have the right vframeArray
 343   assert(cb-&gt;frame_size() &gt;= 0, "Unexpected frame size");
 344   intptr_t* unpack_sp = stub_frame.sp() + cb-&gt;frame_size();
 345 
 346   // If the deopt call site is a MethodHandle invoke call site we have
 347   // to adjust the unpack_sp.
 348   nmethod* deoptee_nm = deoptee.cb()-&gt;as_nmethod_or_null();
 349   if (deoptee_nm != NULL &amp;&amp; deoptee_nm-&gt;is_method_handle_return(deoptee.pc()))
 350     unpack_sp = deoptee.unextended_sp();
 351 
 352 #ifdef ASSERT
 353   assert(cb-&gt;is_deoptimization_stub() ||
 354          cb-&gt;is_uncommon_trap_stub() ||
 355          strcmp("Stub&lt;DeoptimizationStub.deoptimizationHandler&gt;", cb-&gt;name()) == 0 ||
 356          strcmp("Stub&lt;UncommonTrapStub.uncommonTrapHandler&gt;", cb-&gt;name()) == 0,
 357          "unexpected code blob: %s", cb-&gt;name());
 358 #endif
 359 #else
 360   intptr_t* unpack_sp = stub_frame.sender(&amp;dummy_map).unextended_sp();
 361 #endif // !SHARK
 362 
 363   // This is a guarantee instead of an assert because if vframe doesn't match
 364   // we will unpack the wrong deoptimized frame and wind up in strange places
 365   // where it will be very difficult to figure out what went wrong. Better
 366   // to die an early death here than some very obscure death later when the
 367   // trail is cold.
 368   // Note: on ia64 this guarantee can be fooled by frames with no memory stack
 369   // in that it will fail to detect a problem when there is one. This needs
 370   // more work in tiger timeframe.
 371   guarantee(array-&gt;unextended_sp() == unpack_sp, "vframe_array_head must contain the vframeArray to unpack");
 372 
 373   int number_of_frames = array-&gt;frames();
 374 
 375   // Compute the vframes' sizes.  Note that frame_sizes[] entries are ordered from outermost to innermost
 376   // virtual activation, which is the reverse of the elements in the vframes array.
 377   intptr_t* frame_sizes = NEW_C_HEAP_ARRAY(intptr_t, number_of_frames, mtCompiler);
 378   // +1 because we always have an interpreter return address for the final slot.
 379   address* frame_pcs = NEW_C_HEAP_ARRAY(address, number_of_frames + 1, mtCompiler);
 380   int popframe_extra_args = 0;
 381   // Create an interpreter return address for the stub to use as its return
 382   // address so the skeletal frames are perfectly walkable
 383   frame_pcs[number_of_frames] = Interpreter::deopt_entry(vtos, 0);
 384 
 385   // PopFrame requires that the preserved incoming arguments from the recently-popped topmost
 386   // activation be put back on the expression stack of the caller for reexecution
 387   if (JvmtiExport::can_pop_frame() &amp;&amp; thread-&gt;popframe_forcing_deopt_reexecution()) {
 388     popframe_extra_args = in_words(thread-&gt;popframe_preserved_args_size_in_words());
 389   }
 390 
 391   // Find the current pc for sender of the deoptee. Since the sender may have been deoptimized
 392   // itself since the deoptee vframeArray was created we must get a fresh value of the pc rather
 393   // than simply use array-&gt;sender.pc(). This requires us to walk the current set of frames
 394   //
 395   frame deopt_sender = stub_frame.sender(&amp;dummy_map); // First is the deoptee frame
 396   deopt_sender = deopt_sender.sender(&amp;dummy_map);     // Now deoptee caller
 397 
 398   // It's possible that the number of parameters at the call site is
 399   // different than number of arguments in the callee when method
 400   // handles are used.  If the caller is interpreted get the real
 401   // value so that the proper amount of space can be added to it's
 402   // frame.
 403   bool caller_was_method_handle = false;
 404   if (deopt_sender.is_interpreted_frame()) {
 405     methodHandle method = deopt_sender.interpreter_frame_method();
 406     Bytecode_invoke cur = Bytecode_invoke_check(method, deopt_sender.interpreter_frame_bci());
 407     if (cur.is_invokedynamic() || cur.is_invokehandle()) {
 408       // Method handle invokes may involve fairly arbitrary chains of
 409       // calls so it's impossible to know how much actual space the
 410       // caller has for locals.
 411       caller_was_method_handle = true;
 412     }
 413   }
 414 
 415   //
 416   // frame_sizes/frame_pcs[0] oldest frame (int or c2i)
 417   // frame_sizes/frame_pcs[1] next oldest frame (int)
 418   // frame_sizes/frame_pcs[n] youngest frame (int)
 419   //
 420   // Now a pc in frame_pcs is actually the return address to the frame's caller (a frame
 421   // owns the space for the return address to it's caller).  Confusing ain't it.
 422   //
 423   // The vframe array can address vframes with indices running from
 424   // 0.._frames-1. Index  0 is the youngest frame and _frame - 1 is the oldest (root) frame.
 425   // When we create the skeletal frames we need the oldest frame to be in the zero slot
 426   // in the frame_sizes/frame_pcs so the assembly code can do a trivial walk.
 427   // so things look a little strange in this loop.
 428   //
 429   int callee_parameters = 0;
 430   int callee_locals = 0;
 431   for (int index = 0; index &lt; array-&gt;frames(); index++ ) {
 432     // frame[number_of_frames - 1 ] = on_stack_size(youngest)
 433     // frame[number_of_frames - 2 ] = on_stack_size(sender(youngest))
 434     // frame[number_of_frames - 3 ] = on_stack_size(sender(sender(youngest)))
 435     frame_sizes[number_of_frames - 1 - index] = BytesPerWord * array-&gt;element(index)-&gt;on_stack_size(callee_parameters,
 436                                                                                                     callee_locals,
 437                                                                                                     index == 0,
 438                                                                                                     popframe_extra_args);
 439     // This pc doesn't have to be perfect just good enough to identify the frame
 440     // as interpreted so the skeleton frame will be walkable
 441     // The correct pc will be set when the skeleton frame is completely filled out
 442     // The final pc we store in the loop is wrong and will be overwritten below
 443     frame_pcs[number_of_frames - 1 - index ] = Interpreter::deopt_entry(vtos, 0) - frame::pc_return_offset;
 444 
 445     callee_parameters = array-&gt;element(index)-&gt;method()-&gt;size_of_parameters();
 446     callee_locals = array-&gt;element(index)-&gt;method()-&gt;max_locals();
 447     popframe_extra_args = 0;
 448   }
 449 
 450   // Compute whether the root vframe returns a float or double value.
 451   BasicType return_type;
 452   {
 453     HandleMark hm;
 454     methodHandle method(thread, array-&gt;element(0)-&gt;method());
 455     Bytecode_invoke invoke = Bytecode_invoke_check(method, array-&gt;element(0)-&gt;bci());
 456     return_type = invoke.is_valid() ? invoke.result_type() : T_ILLEGAL;
 457   }
 458 
 459   // Compute information for handling adapters and adjusting the frame size of the caller.
 460   int caller_adjustment = 0;
 461 
 462   // Compute the amount the oldest interpreter frame will have to adjust
 463   // its caller's stack by. If the caller is a compiled frame then
 464   // we pretend that the callee has no parameters so that the
 465   // extension counts for the full amount of locals and not just
 466   // locals-parms. This is because without a c2i adapter the parm
 467   // area as created by the compiled frame will not be usable by
 468   // the interpreter. (Depending on the calling convention there
 469   // may not even be enough space).
 470 
 471   // QQQ I'd rather see this pushed down into last_frame_adjust
 472   // and have it take the sender (aka caller).
 473 
 474   if (deopt_sender.is_compiled_frame() || caller_was_method_handle) {
 475     caller_adjustment = last_frame_adjust(0, callee_locals);
 476   } else if (callee_locals &gt; callee_parameters) {
 477     // The caller frame may need extending to accommodate
 478     // non-parameter locals of the first unpacked interpreted frame.
 479     // Compute that adjustment.
 480     caller_adjustment = last_frame_adjust(callee_parameters, callee_locals);
 481   }
 482 
 483   // If the sender is deoptimized the we must retrieve the address of the handler
 484   // since the frame will "magically" show the original pc before the deopt
 485   // and we'd undo the deopt.
 486 
 487   frame_pcs[0] = deopt_sender.raw_pc();
 488 
 489 #ifndef SHARK
 490   assert(CodeCache::find_blob_unsafe(frame_pcs[0]) != NULL, "bad pc");
 491 #endif // SHARK
 492 
 493 #ifdef INCLUDE_JVMCI
 494   if (exceptionObject() != NULL) {
 495     thread-&gt;set_exception_oop(exceptionObject());
 496     exec_mode = Unpack_exception;
 497   }
 498 #endif
 499 
 500   UnrollBlock* info = new UnrollBlock(array-&gt;frame_size() * BytesPerWord,
 501                                       caller_adjustment * BytesPerWord,
 502                                       caller_was_method_handle ? 0 : callee_parameters,
 503                                       number_of_frames,
 504                                       frame_sizes,
 505                                       frame_pcs,
 506                                       return_type,
 507                                       exec_mode);
 508   // On some platforms, we need a way to pass some platform dependent
 509   // information to the unpacking code so the skeletal frames come out
 510   // correct (initial fp value, unextended sp, ...)
 511   info-&gt;set_initial_info((intptr_t) array-&gt;sender().initial_deoptimization_info());
 512 
 513   if (array-&gt;frames() &gt; 1) {
 514     if (VerifyStack &amp;&amp; TraceDeoptimization) {
 515       ttyLocker ttyl;
 516       tty-&gt;print_cr("Deoptimizing method containing inlining");
 517     }
 518   }
 519 
 520   array-&gt;set_unroll_block(info);
 521   return info;
 522 }
 523 
 524 // Called to cleanup deoptimization data structures in normal case
 525 // after unpacking to stack and when stack overflow error occurs
 526 void Deoptimization::cleanup_deopt_info(JavaThread *thread,
 527                                         vframeArray *array) {
 528 
 529   // Get array if coming from exception
 530   if (array == NULL) {
 531     array = thread-&gt;vframe_array_head();
 532   }
 533   thread-&gt;set_vframe_array_head(NULL);
 534 
 535   // Free the previous UnrollBlock
 536   vframeArray* old_array = thread-&gt;vframe_array_last();
 537   thread-&gt;set_vframe_array_last(array);
 538 
 539   if (old_array != NULL) {
 540     UnrollBlock* old_info = old_array-&gt;unroll_block();
 541     old_array-&gt;set_unroll_block(NULL);
 542     delete old_info;
 543     delete old_array;
 544   }
 545 
 546   // Deallocate any resource creating in this routine and any ResourceObjs allocated
 547   // inside the vframeArray (StackValueCollections)
 548 
 549   delete thread-&gt;deopt_mark();
 550   thread-&gt;set_deopt_mark(NULL);
 551   thread-&gt;set_deopt_nmethod(NULL);
 552 
 553 
 554   if (JvmtiExport::can_pop_frame()) {
 555 #ifndef CC_INTERP
 556     // Regardless of whether we entered this routine with the pending
 557     // popframe condition bit set, we should always clear it now
 558     thread-&gt;clear_popframe_condition();
 559 #else
 560     // C++ interpreter will clear has_pending_popframe when it enters
 561     // with method_resume. For deopt_resume2 we clear it now.
 562     if (thread-&gt;popframe_forcing_deopt_reexecution())
 563         thread-&gt;clear_popframe_condition();
 564 #endif /* CC_INTERP */
 565   }
 566 
 567   // unpack_frames() is called at the end of the deoptimization handler
 568   // and (in C2) at the end of the uncommon trap handler. Note this fact
 569   // so that an asynchronous stack walker can work again. This counter is
 570   // incremented at the beginning of fetch_unroll_info() and (in C2) at
 571   // the beginning of uncommon_trap().
 572   thread-&gt;dec_in_deopt_handler();
 573 }
 574 
 575 // Moved from cpu directories because none of the cpus has callee save values.
 576 // If a cpu implements callee save values, move this to deoptimization_&lt;cpu&gt;.cpp.
 577 void Deoptimization::unwind_callee_save_values(frame* f, vframeArray* vframe_array) {
 578 
 579   // This code is sort of the equivalent of C2IAdapter::setup_stack_frame back in
 580   // the days we had adapter frames. When we deoptimize a situation where a
 581   // compiled caller calls a compiled caller will have registers it expects
 582   // to survive the call to the callee. If we deoptimize the callee the only
 583   // way we can restore these registers is to have the oldest interpreter
 584   // frame that we create restore these values. That is what this routine
 585   // will accomplish.
 586 
 587   // At the moment we have modified c2 to not have any callee save registers
 588   // so this problem does not exist and this routine is just a place holder.
 589 
 590   assert(f-&gt;is_interpreted_frame(), "must be interpreted");
 591 }
 592 
 593 // Return BasicType of value being returned
 594 JRT_LEAF(BasicType, Deoptimization::unpack_frames(JavaThread* thread, int exec_mode))
 595 
 596   // We are already active int he special DeoptResourceMark any ResourceObj's we
 597   // allocate will be freed at the end of the routine.
 598 
 599   // It is actually ok to allocate handles in a leaf method. It causes no safepoints,
 600   // but makes the entry a little slower. There is however a little dance we have to
 601   // do in debug mode to get around the NoHandleMark code in the JRT_LEAF macro
 602   ResetNoHandleMark rnhm; // No-op in release/product versions
 603   HandleMark hm;
 604 
 605   frame stub_frame = thread-&gt;last_frame();
 606 
 607   // Since the frame to unpack is the top frame of this thread, the vframe_array_head
 608   // must point to the vframeArray for the unpack frame.
 609   vframeArray* array = thread-&gt;vframe_array_head();
 610 
 611 #ifndef PRODUCT
 612   if (TraceDeoptimization) {
 613     ttyLocker ttyl;
 614     tty-&gt;print_cr("DEOPT UNPACKING thread " INTPTR_FORMAT " vframeArray " INTPTR_FORMAT " mode %d",
 615                   p2i(thread), p2i(array), exec_mode);
 616   }
 617 #endif
 618   Events::log(thread, "DEOPT UNPACKING pc=" INTPTR_FORMAT " sp=" INTPTR_FORMAT " mode %d",
 619               p2i(stub_frame.pc()), p2i(stub_frame.sp()), exec_mode);
 620 
 621   UnrollBlock* info = array-&gt;unroll_block();
 622 
 623   // Unpack the interpreter frames and any adapter frame (c2 only) we might create.
 624   array-&gt;unpack_to_stack(stub_frame, exec_mode, info-&gt;caller_actual_parameters());
 625 
 626   BasicType bt = info-&gt;return_type();
 627 
 628   // If we have an exception pending, claim that the return type is an oop
 629   // so the deopt_blob does not overwrite the exception_oop.
 630 
 631   if (exec_mode == Unpack_exception)
 632     bt = T_OBJECT;
 633 
 634   // Cleanup thread deopt data
 635   cleanup_deopt_info(thread, array);
 636 
 637 #ifndef PRODUCT
 638   if (VerifyStack) {
 639     ResourceMark res_mark;
 640 
 641     thread-&gt;validate_frame_layout();
 642 
 643     // Verify that the just-unpacked frames match the interpreter's
 644     // notions of expression stack and locals
 645     vframeArray* cur_array = thread-&gt;vframe_array_last();
 646     RegisterMap rm(thread, false);
 647     rm.set_include_argument_oops(false);
 648     bool is_top_frame = true;
 649     int callee_size_of_parameters = 0;
 650     int callee_max_locals = 0;
 651     for (int i = 0; i &lt; cur_array-&gt;frames(); i++) {
 652       vframeArrayElement* el = cur_array-&gt;element(i);
 653       frame* iframe = el-&gt;iframe();
 654       guarantee(iframe-&gt;is_interpreted_frame(), "Wrong frame type");
 655 
 656       // Get the oop map for this bci
 657       InterpreterOopMap mask;
 658       int cur_invoke_parameter_size = 0;
 659       bool try_next_mask = false;
 660       int next_mask_expression_stack_size = -1;
 661       int top_frame_expression_stack_adjustment = 0;
 662       methodHandle mh(thread, iframe-&gt;interpreter_frame_method());
 663       OopMapCache::compute_one_oop_map(mh, iframe-&gt;interpreter_frame_bci(), &amp;mask);
 664       BytecodeStream str(mh);
 665       str.set_start(iframe-&gt;interpreter_frame_bci());
 666       int max_bci = mh-&gt;code_size();
 667       // Get to the next bytecode if possible
 668       assert(str.bci() &lt; max_bci, "bci in interpreter frame out of bounds");
 669       // Check to see if we can grab the number of outgoing arguments
 670       // at an uncommon trap for an invoke (where the compiler
 671       // generates debug info before the invoke has executed)
 672       Bytecodes::Code cur_code = str.next();
 673       if (cur_code == Bytecodes::_invokevirtual   ||
 674           cur_code == Bytecodes::_invokespecial   ||
 675           cur_code == Bytecodes::_invokestatic    ||
 676           cur_code == Bytecodes::_invokeinterface ||
 677           cur_code == Bytecodes::_invokedynamic) {
 678         Bytecode_invoke invoke(mh, iframe-&gt;interpreter_frame_bci());
 679         Symbol* signature = invoke.signature();
 680         ArgumentSizeComputer asc(signature);
 681         cur_invoke_parameter_size = asc.size();
 682         if (invoke.has_receiver()) {
 683           // Add in receiver
 684           ++cur_invoke_parameter_size;
 685         }
 686         if (i != 0 &amp;&amp; !invoke.is_invokedynamic() &amp;&amp; MethodHandles::has_member_arg(invoke.klass(), invoke.name())) {
 687           callee_size_of_parameters++;
 688         }
 689       }
 690       if (str.bci() &lt; max_bci) {
 691         Bytecodes::Code bc = str.next();
 692         if (bc &gt;= 0) {
 693           // The interpreter oop map generator reports results before
 694           // the current bytecode has executed except in the case of
 695           // calls. It seems to be hard to tell whether the compiler
 696           // has emitted debug information matching the "state before"
 697           // a given bytecode or the state after, so we try both
 698           switch (cur_code) {
 699             case Bytecodes::_invokevirtual:
 700             case Bytecodes::_invokespecial:
 701             case Bytecodes::_invokestatic:
 702             case Bytecodes::_invokeinterface:
 703             case Bytecodes::_invokedynamic:
 704             case Bytecodes::_athrow:
 705               break;
 706             default: {
 707               InterpreterOopMap next_mask;
 708               OopMapCache::compute_one_oop_map(mh, str.bci(), &amp;next_mask);
 709               next_mask_expression_stack_size = next_mask.expression_stack_size();
 710               // Need to subtract off the size of the result type of
 711               // the bytecode because this is not described in the
 712               // debug info but returned to the interpreter in the TOS
 713               // caching register
 714               BasicType bytecode_result_type = Bytecodes::result_type(cur_code);
 715               if (bytecode_result_type != T_ILLEGAL) {
 716                 top_frame_expression_stack_adjustment = type2size[bytecode_result_type];
 717               }
 718               assert(top_frame_expression_stack_adjustment &gt;= 0, "");
 719               try_next_mask = true;
 720               break;
 721             }
 722           }
 723         }
 724       }
 725 
 726       // Verify stack depth and oops in frame
 727       // This assertion may be dependent on the platform we're running on and may need modification (tested on x86 and sparc)
 728       if (!(
 729             /* SPARC */
 730             (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + callee_size_of_parameters) ||
 731             /* x86 */
 732             (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + callee_max_locals) ||
 733             (try_next_mask &amp;&amp;
 734              (iframe-&gt;interpreter_frame_expression_stack_size() == (next_mask_expression_stack_size -
 735                                                                     top_frame_expression_stack_adjustment))) ||
 736             (is_top_frame &amp;&amp; (exec_mode == Unpack_exception) &amp;&amp; iframe-&gt;interpreter_frame_expression_stack_size() == 0) ||
 737             (is_top_frame &amp;&amp; (exec_mode == Unpack_uncommon_trap || exec_mode == Unpack_reexecute || el-&gt;should_reexecute()) &amp;&amp;
 738              (iframe-&gt;interpreter_frame_expression_stack_size() == mask.expression_stack_size() + cur_invoke_parameter_size))
 739             )) {
 740         ttyLocker ttyl;
 741 
 742         // Print out some information that will help us debug the problem
 743         tty-&gt;print_cr("Wrong number of expression stack elements during deoptimization");
 744         tty-&gt;print_cr("  Error occurred while verifying frame %d (0..%d, 0 is topmost)", i, cur_array-&gt;frames() - 1);
 745         tty-&gt;print_cr("  Fabricated interpreter frame had %d expression stack elements",
 746                       iframe-&gt;interpreter_frame_expression_stack_size());
 747         tty-&gt;print_cr("  Interpreter oop map had %d expression stack elements", mask.expression_stack_size());
 748         tty-&gt;print_cr("  try_next_mask = %d", try_next_mask);
 749         tty-&gt;print_cr("  next_mask_expression_stack_size = %d", next_mask_expression_stack_size);
 750         tty-&gt;print_cr("  callee_size_of_parameters = %d", callee_size_of_parameters);
 751         tty-&gt;print_cr("  callee_max_locals = %d", callee_max_locals);
 752         tty-&gt;print_cr("  top_frame_expression_stack_adjustment = %d", top_frame_expression_stack_adjustment);
 753         tty-&gt;print_cr("  exec_mode = %d", exec_mode);
 754         tty-&gt;print_cr("  cur_invoke_parameter_size = %d", cur_invoke_parameter_size);
 755         tty-&gt;print_cr("  Thread = " INTPTR_FORMAT ", thread ID = %d", p2i(thread), thread-&gt;osthread()-&gt;thread_id());
 756         tty-&gt;print_cr("  Interpreted frames:");
 757         for (int k = 0; k &lt; cur_array-&gt;frames(); k++) {
 758           vframeArrayElement* el = cur_array-&gt;element(k);
 759           tty-&gt;print_cr("    %s (bci %d)", el-&gt;method()-&gt;name_and_sig_as_C_string(), el-&gt;bci());
 760         }
 761         cur_array-&gt;print_on_2(tty);
 762         guarantee(false, "wrong number of expression stack elements during deopt");
 763       }
 764       VerifyOopClosure verify;
 765       iframe-&gt;oops_interpreted_do(&amp;verify, NULL, &amp;rm, false);
 766       callee_size_of_parameters = mh-&gt;size_of_parameters();
 767       callee_max_locals = mh-&gt;max_locals();
 768       is_top_frame = false;
 769     }
 770   }
 771 #endif /* !PRODUCT */
 772 
 773 
 774   return bt;
 775 JRT_END
 776 
 777 
 778 int Deoptimization::deoptimize_dependents() {
 779   Threads::deoptimized_wrt_marked_nmethods();
 780   return 0;
 781 }
 782 
 783 Deoptimization::DeoptAction Deoptimization::_unloaded_action
 784   = Deoptimization::Action_reinterpret;
 785 
 786 #if defined(COMPILER2) || INCLUDE_JVMCI
 787 bool Deoptimization::realloc_objects(JavaThread* thread, frame* fr, GrowableArray&lt;ScopeValue*&gt;* objects, TRAPS) {
 788   Handle pending_exception(thread-&gt;pending_exception());
 789   const char* exception_file = thread-&gt;exception_file();
 790   int exception_line = thread-&gt;exception_line();
 791   thread-&gt;clear_pending_exception();
 792 
 793   bool failures = false;
 794 
 795   for (int i = 0; i &lt; objects-&gt;length(); i++) {
 796     assert(objects-&gt;at(i)-&gt;is_object(), "invalid debug information");
 797     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
 798 
 799     KlassHandle k(java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()()));
 800     oop obj = NULL;
 801 
 802     if (k-&gt;is_instance_klass()) {
 803       InstanceKlass* ik = InstanceKlass::cast(k());
 804       obj = ik-&gt;allocate_instance(THREAD);
 805     } else if (k-&gt;is_typeArray_klass()) {
 806       TypeArrayKlass* ak = TypeArrayKlass::cast(k());
 807       assert(sv-&gt;field_size() % type2size[ak-&gt;element_type()] == 0, "non-integral array length");
 808       int len = sv-&gt;field_size() / type2size[ak-&gt;element_type()];
 809       obj = ak-&gt;allocate(len, THREAD);
 810     } else if (k-&gt;is_objArray_klass()) {
 811       ObjArrayKlass* ak = ObjArrayKlass::cast(k());
 812       obj = ak-&gt;allocate(sv-&gt;field_size(), THREAD);
 813     }
 814 
 815     if (obj == NULL) {
 816       failures = true;
 817     }
 818 
 819     assert(sv-&gt;value().is_null(), "redundant reallocation");
 820     assert(obj != NULL || HAS_PENDING_EXCEPTION, "allocation should succeed or we should get an exception");
 821     CLEAR_PENDING_EXCEPTION;
 822     sv-&gt;set_value(obj);
 823   }
 824 
 825   if (failures) {
 826     THROW_OOP_(Universe::out_of_memory_error_realloc_objects(), failures);
 827   } else if (pending_exception.not_null()) {
 828     thread-&gt;set_pending_exception(pending_exception(), exception_file, exception_line);
 829   }
 830 
 831   return failures;
 832 }
 833 
 834 // restore elements of an eliminated type array
 835 void Deoptimization::reassign_type_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, typeArrayOop obj, BasicType type) {
 836   int index = 0;
 837   intptr_t val;
 838 
 839   for (int i = 0; i &lt; sv-&gt;field_size(); i++) {
 840     StackValue* value = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(i));
 841     switch(type) {
 842     case T_LONG: case T_DOUBLE: {
 843       assert(value-&gt;type() == T_INT, "Agreement.");
 844       StackValue* low =
 845         StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++i));
 846 #ifdef _LP64
 847       jlong res = (jlong)low-&gt;get_int();
 848 #else
 849 #ifdef SPARC
 850       // For SPARC we have to swap high and low words.
 851       jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
 852 #else
 853       jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
 854 #endif //SPARC
 855 #endif
 856       obj-&gt;long_at_put(index, res);
 857       break;
 858     }
 859 
 860     // Have to cast to INT (32 bits) pointer to avoid little/big-endian problem.
 861     case T_INT: case T_FLOAT: { // 4 bytes.
 862       assert(value-&gt;type() == T_INT, "Agreement.");
 863       bool big_value = false;
 864       if (i + 1 &lt; sv-&gt;field_size() &amp;&amp; type == T_INT) {
 865         if (sv-&gt;field_at(i)-&gt;is_location()) {
 866           Location::Type type = ((LocationValue*) sv-&gt;field_at(i))-&gt;location().type();
 867           if (type == Location::dbl || type == Location::lng) {
 868             big_value = true;
 869           }
 870         } else if (sv-&gt;field_at(i)-&gt;is_constant_int()) {
 871           ScopeValue* next_scope_field = sv-&gt;field_at(i + 1);
 872           if (next_scope_field-&gt;is_constant_long() || next_scope_field-&gt;is_constant_double()) {
 873             big_value = true;
 874           }
 875         }
 876       }
 877 
 878       if (big_value) {
 879         StackValue* low = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++i));
 880   #ifdef _LP64
 881         jlong res = (jlong)low-&gt;get_int();
 882   #else
 883   #ifdef SPARC
 884         // For SPARC we have to swap high and low words.
 885         jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
 886   #else
 887         jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
 888   #endif //SPARC
 889   #endif
 890         obj-&gt;int_at_put(index, (jint)*((jint*)&amp;res));
 891         obj-&gt;int_at_put(++index, (jint)*(((jint*)&amp;res) + 1));
 892       } else {
 893         val = value-&gt;get_int();
 894         obj-&gt;int_at_put(index, (jint)*((jint*)&amp;val));
 895       }
 896       break;
 897     }
 898 
 899     case T_SHORT: case T_CHAR: // 2 bytes
 900       assert(value-&gt;type() == T_INT, "Agreement.");
 901       val = value-&gt;get_int();
 902       obj-&gt;short_at_put(index, (jshort)*((jint*)&amp;val));
 903       break;
 904 
 905     case T_BOOLEAN: case T_BYTE: // 1 byte
 906       assert(value-&gt;type() == T_INT, "Agreement.");
 907       val = value-&gt;get_int();
 908       obj-&gt;bool_at_put(index, (jboolean)*((jint*)&amp;val));
 909       break;
 910 
 911       default:
 912         ShouldNotReachHere();
 913     }
 914     index++;
 915   }
 916 }
 917 
 918 
 919 // restore fields of an eliminated object array
 920 void Deoptimization::reassign_object_array_elements(frame* fr, RegisterMap* reg_map, ObjectValue* sv, objArrayOop obj) {
 921   for (int i = 0; i &lt; sv-&gt;field_size(); i++) {
 922     StackValue* value = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(i));
 923     assert(value-&gt;type() == T_OBJECT, "object element expected");
 924     obj-&gt;obj_at_put(i, value-&gt;get_obj()());
 925   }
 926 }
 927 
 928 class ReassignedField {
 929 public:
 930   int _offset;
 931   BasicType _type;
 932 public:
 933   ReassignedField() {
 934     _offset = 0;
 935     _type = T_ILLEGAL;
 936   }
 937 };
 938 
 939 int compare(ReassignedField* left, ReassignedField* right) {
 940   return left-&gt;_offset - right-&gt;_offset;
 941 }
 942 
 943 // Restore fields of an eliminated instance object using the same field order
 944 // returned by HotSpotResolvedObjectTypeImpl.getInstanceFields(true)
 945 static int reassign_fields_by_klass(InstanceKlass* klass, frame* fr, RegisterMap* reg_map, ObjectValue* sv, int svIndex, oop obj, bool skip_internal) {
 946   if (klass-&gt;superklass() != NULL) {
 947     svIndex = reassign_fields_by_klass(klass-&gt;superklass(), fr, reg_map, sv, svIndex, obj, skip_internal);
 948   }
 949 
 950   GrowableArray&lt;ReassignedField&gt;* fields = new GrowableArray&lt;ReassignedField&gt;();
 951   for (AllFieldStream fs(klass); !fs.done(); fs.next()) {
 952     if (!fs.access_flags().is_static() &amp;&amp; (!skip_internal || !fs.access_flags().is_internal())) {
 953       ReassignedField field;
 954       field._offset = fs.offset();
 955       field._type = FieldType::basic_type(fs.signature());
 956       fields-&gt;append(field);
 957     }
 958   }
 959   fields-&gt;sort(compare);
 960   for (int i = 0; i &lt; fields-&gt;length(); i++) {
 961     intptr_t val;
 962     ScopeValue* scope_field = sv-&gt;field_at(svIndex);
 963     StackValue* value = StackValue::create_stack_value(fr, reg_map, scope_field);
 964     int offset = fields-&gt;at(i)._offset;
 965     BasicType type = fields-&gt;at(i)._type;
 966     switch (type) {
 967       case T_OBJECT: case T_ARRAY:
 968         assert(value-&gt;type() == T_OBJECT, "Agreement.");
 969         obj-&gt;obj_field_put(offset, value-&gt;get_obj()());
 970         break;
 971 
 972       // Have to cast to INT (32 bits) pointer to avoid little/big-endian problem.
 973       case T_INT: case T_FLOAT: { // 4 bytes.
 974         assert(value-&gt;type() == T_INT, "Agreement.");
 975         bool big_value = false;
 976         if (i+1 &lt; fields-&gt;length() &amp;&amp; fields-&gt;at(i+1)._type == T_INT) {
 977           if (scope_field-&gt;is_location()) {
 978             Location::Type type = ((LocationValue*) scope_field)-&gt;location().type();
 979             if (type == Location::dbl || type == Location::lng) {
 980               big_value = true;
 981             }
 982           }
 983           if (scope_field-&gt;is_constant_int()) {
 984             ScopeValue* next_scope_field = sv-&gt;field_at(svIndex + 1);
 985             if (next_scope_field-&gt;is_constant_long() || next_scope_field-&gt;is_constant_double()) {
 986               big_value = true;
 987             }
 988           }
 989         }
 990 
 991         if (big_value) {
 992           i++;
 993           assert(i &lt; fields-&gt;length(), "second T_INT field needed");
 994           assert(fields-&gt;at(i)._type == T_INT, "T_INT field needed");
 995         } else {
 996           val = value-&gt;get_int();
 997           obj-&gt;int_field_put(offset, (jint)*((jint*)&amp;val));
 998           break;
 999         }
1000       }
1001         /* no break */
1002 
1003       case T_LONG: case T_DOUBLE: {
1004         assert(value-&gt;type() == T_INT, "Agreement.");
1005         StackValue* low = StackValue::create_stack_value(fr, reg_map, sv-&gt;field_at(++svIndex));
1006 #ifdef _LP64
1007         jlong res = (jlong)low-&gt;get_int();
1008 #else
1009 #ifdef SPARC
1010         // For SPARC we have to swap high and low words.
1011         jlong res = jlong_from((jint)low-&gt;get_int(), (jint)value-&gt;get_int());
1012 #else
1013         jlong res = jlong_from((jint)value-&gt;get_int(), (jint)low-&gt;get_int());
1014 #endif //SPARC
1015 #endif
1016         obj-&gt;long_field_put(offset, res);
1017         break;
1018       }
1019 
1020       case T_SHORT: case T_CHAR: // 2 bytes
1021         assert(value-&gt;type() == T_INT, "Agreement.");
1022         val = value-&gt;get_int();
1023         obj-&gt;short_field_put(offset, (jshort)*((jint*)&amp;val));
1024         break;
1025 
1026       case T_BOOLEAN: case T_BYTE: // 1 byte
1027         assert(value-&gt;type() == T_INT, "Agreement.");
1028         val = value-&gt;get_int();
1029         obj-&gt;bool_field_put(offset, (jboolean)*((jint*)&amp;val));
1030         break;
1031 
1032       default:
1033         ShouldNotReachHere();
1034     }
1035     svIndex++;
1036   }
1037   return svIndex;
1038 }
1039 
1040 // restore fields of all eliminated objects and arrays
1041 void Deoptimization::reassign_fields(frame* fr, RegisterMap* reg_map, GrowableArray&lt;ScopeValue*&gt;* objects, bool realloc_failures, bool skip_internal) {
1042   for (int i = 0; i &lt; objects-&gt;length(); i++) {
1043     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
1044     KlassHandle k(java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()()));
1045     Handle obj = sv-&gt;value();
1046     assert(obj.not_null() || realloc_failures, "reallocation was missed");
1047     if (PrintDeoptimizationDetails) {
1048       tty-&gt;print_cr("reassign fields for object of type %s!", k-&gt;name()-&gt;as_C_string());
1049     }
1050     if (obj.is_null()) {
1051       continue;
1052     }
1053 
1054     if (k-&gt;is_instance_klass()) {
1055       InstanceKlass* ik = InstanceKlass::cast(k());
1056       reassign_fields_by_klass(ik, fr, reg_map, sv, 0, obj(), skip_internal);
1057     } else if (k-&gt;is_typeArray_klass()) {
1058       TypeArrayKlass* ak = TypeArrayKlass::cast(k());
1059       reassign_type_array_elements(fr, reg_map, sv, (typeArrayOop) obj(), ak-&gt;element_type());
1060     } else if (k-&gt;is_objArray_klass()) {
1061       reassign_object_array_elements(fr, reg_map, sv, (objArrayOop) obj());
1062     }
1063   }
1064 }
1065 
1066 
1067 // relock objects for which synchronization was eliminated
1068 void Deoptimization::relock_objects(GrowableArray&lt;MonitorInfo*&gt;* monitors, JavaThread* thread, bool realloc_failures) {
1069   for (int i = 0; i &lt; monitors-&gt;length(); i++) {
1070     MonitorInfo* mon_info = monitors-&gt;at(i);
1071     if (mon_info-&gt;eliminated()) {
1072       assert(!mon_info-&gt;owner_is_scalar_replaced() || realloc_failures, "reallocation was missed");
1073       if (!mon_info-&gt;owner_is_scalar_replaced()) {
1074         Handle obj = Handle(mon_info-&gt;owner());
1075         markOop mark = obj-&gt;mark();
1076         if (UseBiasedLocking &amp;&amp; mark-&gt;has_bias_pattern()) {
1077           // New allocated objects may have the mark set to anonymously biased.
1078           // Also the deoptimized method may called methods with synchronization
1079           // where the thread-local object is bias locked to the current thread.
1080           assert(mark-&gt;is_biased_anonymously() ||
1081                  mark-&gt;biased_locker() == thread, "should be locked to current thread");
1082           // Reset mark word to unbiased prototype.
1083           markOop unbiased_prototype = markOopDesc::prototype()-&gt;set_age(mark-&gt;age());
1084           obj-&gt;set_mark(unbiased_prototype);
1085         }
1086         BasicLock* lock = mon_info-&gt;lock();
1087         ObjectSynchronizer::slow_enter(obj, lock, thread);
1088         assert(mon_info-&gt;owner()-&gt;is_locked(), "object must be locked now");
1089       }
1090     }
1091   }
1092 }
1093 
1094 
1095 #ifndef PRODUCT
1096 // print information about reallocated objects
1097 void Deoptimization::print_objects(GrowableArray&lt;ScopeValue*&gt;* objects, bool realloc_failures) {
1098   fieldDescriptor fd;
1099 
1100   for (int i = 0; i &lt; objects-&gt;length(); i++) {
1101     ObjectValue* sv = (ObjectValue*) objects-&gt;at(i);
1102     KlassHandle k(java_lang_Class::as_Klass(sv-&gt;klass()-&gt;as_ConstantOopReadValue()-&gt;value()()));
1103     Handle obj = sv-&gt;value();
1104 
1105     tty-&gt;print("     object &lt;" INTPTR_FORMAT "&gt; of type ", p2i(sv-&gt;value()()));
1106     k-&gt;print_value();
1107     assert(obj.not_null() || realloc_failures, "reallocation was missed");
1108     if (obj.is_null()) {
1109       tty-&gt;print(" allocation failed");
1110     } else {
1111       tty-&gt;print(" allocated (%d bytes)", obj-&gt;size() * HeapWordSize);
1112     }
1113     tty-&gt;cr();
1114 
1115     if (Verbose &amp;&amp; !obj.is_null()) {
1116       k-&gt;oop_print_on(obj(), tty);
1117     }
1118   }
1119 }
1120 #endif
1121 #endif // COMPILER2 || INCLUDE_JVMCI
1122 
1123 vframeArray* Deoptimization::create_vframeArray(JavaThread* thread, frame fr, RegisterMap *reg_map, GrowableArray&lt;compiledVFrame*&gt;* chunk, bool realloc_failures) {
1124   Events::log(thread, "DEOPT PACKING pc=" INTPTR_FORMAT " sp=" INTPTR_FORMAT, p2i(fr.pc()), p2i(fr.sp()));
1125 
1126 #ifndef PRODUCT
1127   if (PrintDeoptimizationDetails) {
1128     ttyLocker ttyl;
1129     tty-&gt;print("DEOPT PACKING thread " INTPTR_FORMAT " ", p2i(thread));
1130     fr.print_on(tty);
1131     tty-&gt;print_cr("     Virtual frames (innermost first):");
1132     for (int index = 0; index &lt; chunk-&gt;length(); index++) {
1133       compiledVFrame* vf = chunk-&gt;at(index);
1134       tty-&gt;print("       %2d - ", index);
1135       vf-&gt;print_value();
1136       int bci = chunk-&gt;at(index)-&gt;raw_bci();
1137       const char* code_name;
1138       if (bci == SynchronizationEntryBCI) {
1139         code_name = "sync entry";
1140       } else {
1141         Bytecodes::Code code = vf-&gt;method()-&gt;code_at(bci);
1142         code_name = Bytecodes::name(code);
1143       }
1144       tty-&gt;print(" - %s", code_name);
1145       tty-&gt;print_cr(" @ bci %d ", bci);
1146       if (Verbose) {
1147         vf-&gt;print();
1148         tty-&gt;cr();
1149       }
1150     }
1151   }
1152 #endif
1153 
1154   // Register map for next frame (used for stack crawl).  We capture
1155   // the state of the deopt'ing frame's caller.  Thus if we need to
1156   // stuff a C2I adapter we can properly fill in the callee-save
1157   // register locations.
1158   frame caller = fr.sender(reg_map);
1159   int frame_size = caller.sp() - fr.sp();
1160 
1161   frame sender = caller;
1162 
1163   // Since the Java thread being deoptimized will eventually adjust it's own stack,
1164   // the vframeArray containing the unpacking information is allocated in the C heap.
1165   // For Compiler1, the caller of the deoptimized frame is saved for use by unpack_frames().
1166   vframeArray* array = vframeArray::allocate(thread, frame_size, chunk, reg_map, sender, caller, fr, realloc_failures);
1167 
1168   // Compare the vframeArray to the collected vframes
1169   assert(array-&gt;structural_compare(thread, chunk), "just checking");
1170 
1171 #ifndef PRODUCT
1172   if (PrintDeoptimizationDetails) {
1173     ttyLocker ttyl;
1174     tty-&gt;print_cr("     Created vframeArray " INTPTR_FORMAT, p2i(array));
1175   }
1176 #endif // PRODUCT
1177 
1178   return array;
1179 }
1180 
1181 #if defined(COMPILER2) || INCLUDE_JVMCI
1182 void Deoptimization::pop_frames_failed_reallocs(JavaThread* thread, vframeArray* array) {
1183   // Reallocation of some scalar replaced objects failed. Record
1184   // that we need to pop all the interpreter frames for the
1185   // deoptimized compiled frame.
1186   assert(thread-&gt;frames_to_pop_failed_realloc() == 0, "missed frames to pop?");
1187   thread-&gt;set_frames_to_pop_failed_realloc(array-&gt;frames());
1188   // Unlock all monitors here otherwise the interpreter will see a
1189   // mix of locked and unlocked monitors (because of failed
1190   // reallocations of synchronized objects) and be confused.
1191   for (int i = 0; i &lt; array-&gt;frames(); i++) {
1192     MonitorChunk* monitors = array-&gt;element(i)-&gt;monitors();
1193     if (monitors != NULL) {
1194       for (int j = 0; j &lt; monitors-&gt;number_of_monitors(); j++) {
1195         BasicObjectLock* src = monitors-&gt;at(j);
1196         if (src-&gt;obj() != NULL) {
1197           ObjectSynchronizer::fast_exit(src-&gt;obj(), src-&gt;lock(), thread);
1198         }
1199       }
1200       array-&gt;element(i)-&gt;free_monitors(thread);
1201 #ifdef ASSERT
1202       array-&gt;element(i)-&gt;set_removed_monitors();
1203 #endif
1204     }
1205   }
1206 }
1207 #endif
1208 
1209 static void collect_monitors(compiledVFrame* cvf, GrowableArray&lt;Handle&gt;* objects_to_revoke) {
1210   GrowableArray&lt;MonitorInfo*&gt;* monitors = cvf-&gt;monitors();
1211   for (int i = 0; i &lt; monitors-&gt;length(); i++) {
1212     MonitorInfo* mon_info = monitors-&gt;at(i);
1213     if (!mon_info-&gt;eliminated() &amp;&amp; mon_info-&gt;owner() != NULL) {
1214       objects_to_revoke-&gt;append(Handle(mon_info-&gt;owner()));
1215     }
1216   }
1217 }
1218 
1219 
1220 void Deoptimization::revoke_biases_of_monitors(JavaThread* thread, frame fr, RegisterMap* map) {
1221   if (!UseBiasedLocking) {
1222     return;
1223   }
1224 
1225   GrowableArray&lt;Handle&gt;* objects_to_revoke = new GrowableArray&lt;Handle&gt;();
1226 
1227   // Unfortunately we don't have a RegisterMap available in most of
1228   // the places we want to call this routine so we need to walk the
1229   // stack again to update the register map.
1230   if (map == NULL || !map-&gt;update_map()) {
1231     StackFrameStream sfs(thread, true);
1232     bool found = false;
1233     while (!found &amp;&amp; !sfs.is_done()) {
1234       frame* cur = sfs.current();
1235       sfs.next();
1236       found = cur-&gt;id() == fr.id();
1237     }
1238     assert(found, "frame to be deoptimized not found on target thread's stack");
1239     map = sfs.register_map();
1240   }
1241 
1242   vframe* vf = vframe::new_vframe(&amp;fr, map, thread);
1243   compiledVFrame* cvf = compiledVFrame::cast(vf);
1244   // Revoke monitors' biases in all scopes
1245   while (!cvf-&gt;is_top()) {
1246     collect_monitors(cvf, objects_to_revoke);
1247     cvf = compiledVFrame::cast(cvf-&gt;sender());
1248   }
1249   collect_monitors(cvf, objects_to_revoke);
1250 
1251   if (SafepointSynchronize::is_at_safepoint()) {
1252     BiasedLocking::revoke_at_safepoint(objects_to_revoke);
1253   } else {
1254     BiasedLocking::revoke(objects_to_revoke);
1255   }
1256 }
1257 
1258 
1259 void Deoptimization::revoke_biases_of_monitors(CodeBlob* cb) {
1260   if (!UseBiasedLocking) {
1261     return;
1262   }
1263 
1264   assert(SafepointSynchronize::is_at_safepoint(), "must only be called from safepoint");
1265   GrowableArray&lt;Handle&gt;* objects_to_revoke = new GrowableArray&lt;Handle&gt;();
1266   for (JavaThread* jt = Threads::first(); jt != NULL ; jt = jt-&gt;next()) {
1267     if (jt-&gt;has_last_Java_frame()) {
1268       StackFrameStream sfs(jt, true);
1269       while (!sfs.is_done()) {
1270         frame* cur = sfs.current();
1271         if (cb-&gt;contains(cur-&gt;pc())) {
1272           vframe* vf = vframe::new_vframe(cur, sfs.register_map(), jt);
1273           compiledVFrame* cvf = compiledVFrame::cast(vf);
1274           // Revoke monitors' biases in all scopes
1275           while (!cvf-&gt;is_top()) {
1276             collect_monitors(cvf, objects_to_revoke);
1277             cvf = compiledVFrame::cast(cvf-&gt;sender());
1278           }
1279           collect_monitors(cvf, objects_to_revoke);
1280         }
1281         sfs.next();
1282       }
1283     }
1284   }
1285   BiasedLocking::revoke_at_safepoint(objects_to_revoke);
1286 }
1287 
1288 
1289 void Deoptimization::deoptimize_single_frame(JavaThread* thread, frame fr, Deoptimization::DeoptReason reason) {
1290   assert(fr.can_be_deoptimized(), "checking frame type");
1291 
1292   gather_statistics(reason, Action_none, Bytecodes::_illegal);
1293 
1294   if (LogCompilation &amp;&amp; xtty != NULL) {
1295     nmethod* nm = fr.cb()-&gt;as_nmethod_or_null();
1296     assert(nm != NULL, "only compiled methods can deopt");
1297 
1298     ttyLocker ttyl;
1299     xtty-&gt;begin_head("deoptimized thread='" UINTX_FORMAT "'", (uintx)thread-&gt;osthread()-&gt;thread_id());
1300     nm-&gt;log_identity(xtty);
1301     xtty-&gt;end_head();
1302     for (ScopeDesc* sd = nm-&gt;scope_desc_at(fr.pc()); ; sd = sd-&gt;sender()) {
1303       xtty-&gt;begin_elem("jvms bci='%d'", sd-&gt;bci());
1304       xtty-&gt;method(sd-&gt;method());
1305       xtty-&gt;end_elem();
1306       if (sd-&gt;is_top())  break;
1307     }
1308     xtty-&gt;tail("deoptimized");
1309   }
1310 
1311   // Patch the compiled method so that when execution returns to it we will
1312   // deopt the execution state and return to the interpreter.
1313   fr.deoptimize(thread);
1314 }
1315 
1316 void Deoptimization::deoptimize(JavaThread* thread, frame fr, RegisterMap *map) {
1317   deoptimize(thread, fr, map, Reason_constraint);
1318 }
1319 
1320 void Deoptimization::deoptimize(JavaThread* thread, frame fr, RegisterMap *map, DeoptReason reason) {
1321   // Deoptimize only if the frame comes from compile code.
1322   // Do not deoptimize the frame which is already patched
1323   // during the execution of the loops below.
1324   if (!fr.is_compiled_frame() || fr.is_deoptimized_frame()) {
1325     return;
1326   }
1327   ResourceMark rm;
1328   DeoptimizationMarker dm;
1329   if (UseBiasedLocking) {
1330     revoke_biases_of_monitors(thread, fr, map);
1331   }
1332   deoptimize_single_frame(thread, fr, reason);
1333 
1334 }
1335 
1336 
1337 void Deoptimization::deoptimize_frame_internal(JavaThread* thread, intptr_t* id, DeoptReason reason) {
1338   assert(thread == Thread::current() || SafepointSynchronize::is_at_safepoint(),
1339          "can only deoptimize other thread at a safepoint");
1340   // Compute frame and register map based on thread and sp.
1341   RegisterMap reg_map(thread, UseBiasedLocking);
1342   frame fr = thread-&gt;last_frame();
1343   while (fr.id() != id) {
1344     fr = fr.sender(&amp;reg_map);
1345   }
1346   deoptimize(thread, fr, &amp;reg_map, reason);
1347 }
1348 
1349 
1350 void Deoptimization::deoptimize_frame(JavaThread* thread, intptr_t* id, DeoptReason reason) {
1351   if (thread == Thread::current()) {
1352     Deoptimization::deoptimize_frame_internal(thread, id, reason);
1353   } else {
1354     VM_DeoptimizeFrame deopt(thread, id, reason);
1355     VMThread::execute(&amp;deopt);
1356   }
1357 }
1358 
1359 void Deoptimization::deoptimize_frame(JavaThread* thread, intptr_t* id) {
1360   deoptimize_frame(thread, id, Reason_constraint);
1361 }
1362 
1363 // JVMTI PopFrame support
1364 JRT_LEAF(void, Deoptimization::popframe_preserve_args(JavaThread* thread, int bytes_to_save, void* start_address))
1365 {
1366   thread-&gt;popframe_preserve_args(in_ByteSize(bytes_to_save), start_address);
1367 }
1368 JRT_END
1369 
1370 MethodData*
1371 Deoptimization::get_method_data(JavaThread* thread, methodHandle m,
1372                                 bool create_if_missing) {
1373   Thread* THREAD = thread;
1374   MethodData* mdo = m()-&gt;method_data();
1375   if (mdo == NULL &amp;&amp; create_if_missing &amp;&amp; !HAS_PENDING_EXCEPTION) {
1376     // Build an MDO.  Ignore errors like OutOfMemory;
1377     // that simply means we won't have an MDO to update.
1378     Method::build_interpreter_method_data(m, THREAD);
1379     if (HAS_PENDING_EXCEPTION) {
1380       assert((PENDING_EXCEPTION-&gt;is_a(SystemDictionary::OutOfMemoryError_klass())), "we expect only an OOM error here");
1381       CLEAR_PENDING_EXCEPTION;
1382     }
1383     mdo = m()-&gt;method_data();
1384   }
1385   return mdo;
1386 }
1387 
1388 #if defined(COMPILER2) || defined(SHARK) || INCLUDE_JVMCI
1389 void Deoptimization::load_class_by_index(const constantPoolHandle&amp; constant_pool, int index, TRAPS) {
1390   // in case of an unresolved klass entry, load the class.
1391   if (constant_pool-&gt;tag_at(index).is_unresolved_klass()) {
1392     Klass* tk = constant_pool-&gt;klass_at_ignore_error(index, CHECK);
1393     return;
1394   }
1395 
1396   if (!constant_pool-&gt;tag_at(index).is_symbol()) return;
1397 
1398   Handle class_loader (THREAD, constant_pool-&gt;pool_holder()-&gt;class_loader());
1399   Symbol*  symbol  = constant_pool-&gt;symbol_at(index);
1400 
1401   // class name?
1402   if (symbol-&gt;byte_at(0) != '(') {
1403     Handle protection_domain (THREAD, constant_pool-&gt;pool_holder()-&gt;protection_domain());
1404     SystemDictionary::resolve_or_null(symbol, class_loader, protection_domain, CHECK);
1405     return;
1406   }
1407 
1408   // then it must be a signature!
1409   ResourceMark rm(THREAD);
1410   for (SignatureStream ss(symbol); !ss.is_done(); ss.next()) {
1411     if (ss.is_object()) {
1412       Symbol* class_name = ss.as_symbol(CHECK);
1413       Handle protection_domain (THREAD, constant_pool-&gt;pool_holder()-&gt;protection_domain());
1414       SystemDictionary::resolve_or_null(class_name, class_loader, protection_domain, CHECK);
1415     }
1416   }
1417 }
1418 
1419 
1420 void Deoptimization::load_class_by_index(const constantPoolHandle&amp; constant_pool, int index) {
1421   EXCEPTION_MARK;
1422   load_class_by_index(constant_pool, index, THREAD);
1423   if (HAS_PENDING_EXCEPTION) {
1424     // Exception happened during classloading. We ignore the exception here, since it
1425     // is going to be rethrown since the current activation is going to be deoptimized and
1426     // the interpreter will re-execute the bytecode.
1427     CLEAR_PENDING_EXCEPTION;
1428     // Class loading called java code which may have caused a stack
1429     // overflow. If the exception was thrown right before the return
1430     // to the runtime the stack is no longer guarded. Reguard the
1431     // stack otherwise if we return to the uncommon trap blob and the
1432     // stack bang causes a stack overflow we crash.
1433     assert(THREAD-&gt;is_Java_thread(), "only a java thread can be here");
1434     JavaThread* thread = (JavaThread*)THREAD;
1435     bool guard_pages_enabled = thread-&gt;stack_guards_enabled();
1436     if (!guard_pages_enabled) guard_pages_enabled = thread-&gt;reguard_stack();
1437     assert(guard_pages_enabled, "stack banging in uncommon trap blob may cause crash");
1438   }
1439 }
1440 
1441 JRT_ENTRY(void, Deoptimization::uncommon_trap_inner(JavaThread* thread, jint trap_request)) {
1442   HandleMark hm;
1443 
1444   // uncommon_trap() is called at the beginning of the uncommon trap
1445   // handler. Note this fact before we start generating temporary frames
1446   // that can confuse an asynchronous stack walker. This counter is
1447   // decremented at the end of unpack_frames().
1448   thread-&gt;inc_in_deopt_handler();
1449 
1450   // We need to update the map if we have biased locking.
1451 #if INCLUDE_JVMCI
1452   // JVMCI might need to get an exception from the stack, which in turn requires the register map to be valid
1453   RegisterMap reg_map(thread, true);
1454 #else
1455   RegisterMap reg_map(thread, UseBiasedLocking);
1456 #endif
1457   frame stub_frame = thread-&gt;last_frame();
1458   frame fr = stub_frame.sender(&amp;reg_map);
1459   // Make sure the calling nmethod is not getting deoptimized and removed
1460   // before we are done with it.
1461   nmethodLocker nl(fr.pc());
1462 
1463   // Log a message
1464   Events::log(thread, "Uncommon trap: trap_request=" PTR32_FORMAT " fr.pc=" INTPTR_FORMAT " relative=" INTPTR_FORMAT,
1465               trap_request, p2i(fr.pc()), fr.pc() - fr.cb()-&gt;code_begin());
1466 
1467   {
1468     ResourceMark rm;
1469 
1470     // Revoke biases of any monitors in the frame to ensure we can migrate them
1471     revoke_biases_of_monitors(thread, fr, &amp;reg_map);
1472 
1473     DeoptReason reason = trap_request_reason(trap_request);
1474     DeoptAction action = trap_request_action(trap_request);
1475 #if INCLUDE_JVMCI
1476     int debug_id = trap_request_debug_id(trap_request);
1477 #endif
1478     jint unloaded_class_index = trap_request_index(trap_request); // CP idx or -1
1479 
1480     vframe*  vf  = vframe::new_vframe(&amp;fr, &amp;reg_map, thread);
1481     compiledVFrame* cvf = compiledVFrame::cast(vf);
1482 
1483     nmethod* nm = cvf-&gt;code();
1484 
1485     ScopeDesc*      trap_scope  = cvf-&gt;scope();
1486 
1487     if (TraceDeoptimization) {
1488       ttyLocker ttyl;
1489       tty-&gt;print_cr("  bci=%d pc=" INTPTR_FORMAT ", relative_pc=" INTPTR_FORMAT ", method=%s" JVMCI_ONLY(", debug_id=%d"), trap_scope-&gt;bci(), p2i(fr.pc()), fr.pc() - nm-&gt;code_begin(), trap_scope-&gt;method()-&gt;name_and_sig_as_C_string()
1490 #if INCLUDE_JVMCI
1491           , debug_id
1492 #endif
1493           );
1494     }
1495 
1496     methodHandle    trap_method = trap_scope-&gt;method();
1497     int             trap_bci    = trap_scope-&gt;bci();
1498 #if INCLUDE_JVMCI
1499     oop speculation = thread-&gt;pending_failed_speculation();
1500     if (nm-&gt;is_compiled_by_jvmci()) {
1501       if (speculation != NULL) {
1502         oop speculation_log = nm-&gt;speculation_log();
1503         if (speculation_log != NULL) {
1504           if (TraceDeoptimization || TraceUncollectedSpeculations) {
1505             if (HotSpotSpeculationLog::lastFailed(speculation_log) != NULL) {
1506               tty-&gt;print_cr("A speculation that was not collected by the compiler is being overwritten");
1507             }
1508           }
1509           if (TraceDeoptimization) {
1510             tty-&gt;print_cr("Saving speculation to speculation log");
1511           }
1512           HotSpotSpeculationLog::set_lastFailed(speculation_log, speculation);
1513         } else {
1514           if (TraceDeoptimization) {
1515             tty-&gt;print_cr("Speculation present but no speculation log");
1516           }
1517         }
1518         thread-&gt;set_pending_failed_speculation(NULL);
1519       } else {
1520         if (TraceDeoptimization) {
1521           tty-&gt;print_cr("No speculation");
1522         }
1523       }
1524     } else {
1525       assert(speculation == NULL, "There should not be a speculation for method compiled by non-JVMCI compilers");
1526     }
1527 
1528     if (trap_bci == SynchronizationEntryBCI) {
1529       trap_bci = 0;
1530       thread-&gt;set_pending_monitorenter(true);
1531     }
1532 
1533     if (reason == Deoptimization::Reason_transfer_to_interpreter) {
1534       thread-&gt;set_pending_transfer_to_interpreter(true);
1535     }
1536 #endif
1537 
1538     Bytecodes::Code trap_bc     = trap_method-&gt;java_code_at(trap_bci);
1539     // Record this event in the histogram.
1540     gather_statistics(reason, action, trap_bc);
1541 
1542     // Ensure that we can record deopt. history:
1543     // Need MDO to record RTM code generation state.
1544     bool create_if_missing = ProfileTraps || UseCodeAging RTM_OPT_ONLY( || UseRTMLocking );
1545 
1546     methodHandle profiled_method;
1547 #if INCLUDE_JVMCI
1548     if (nm-&gt;is_compiled_by_jvmci()) {
1549       profiled_method = nm-&gt;method();
1550     } else {
1551       profiled_method = trap_method;
1552     }
1553 #else
1554     profiled_method = trap_method;
1555 #endif
1556 
1557     MethodData* trap_mdo =
1558       get_method_data(thread, profiled_method, create_if_missing);
1559 
1560     // Log a message
1561     Events::log_deopt_message(thread, "Uncommon trap: reason=%s action=%s pc=" INTPTR_FORMAT " method=%s @ %d",
1562                               trap_reason_name(reason), trap_action_name(action), p2i(fr.pc()),
1563                               trap_method-&gt;name_and_sig_as_C_string(), trap_bci);
1564 
1565     // Print a bunch of diagnostics, if requested.
1566     if (TraceDeoptimization || LogCompilation) {
1567       ResourceMark rm;
1568       ttyLocker ttyl;
1569       char buf[100];
1570       if (xtty != NULL) {
1571         xtty-&gt;begin_head("uncommon_trap thread='" UINTX_FORMAT "' %s",
1572                          os::current_thread_id(),
1573                          format_trap_request(buf, sizeof(buf), trap_request));
1574         nm-&gt;log_identity(xtty);
1575       }
1576       Symbol* class_name = NULL;
1577       bool unresolved = false;
1578       if (unloaded_class_index &gt;= 0) {
1579         constantPoolHandle constants (THREAD, trap_method-&gt;constants());
1580         if (constants-&gt;tag_at(unloaded_class_index).is_unresolved_klass()) {
1581           class_name = constants-&gt;klass_name_at(unloaded_class_index);
1582           unresolved = true;
1583           if (xtty != NULL)
1584             xtty-&gt;print(" unresolved='1'");
1585         } else if (constants-&gt;tag_at(unloaded_class_index).is_symbol()) {
1586           class_name = constants-&gt;symbol_at(unloaded_class_index);
1587         }
1588         if (xtty != NULL)
1589           xtty-&gt;name(class_name);
1590       }
1591       if (xtty != NULL &amp;&amp; trap_mdo != NULL &amp;&amp; (int)reason &lt; (int)MethodData::_trap_hist_limit) {
1592         // Dump the relevant MDO state.
1593         // This is the deopt count for the current reason, any previous
1594         // reasons or recompiles seen at this point.
1595         int dcnt = trap_mdo-&gt;trap_count(reason);
1596         if (dcnt != 0)
1597           xtty-&gt;print(" count='%d'", dcnt);
1598         ProfileData* pdata = trap_mdo-&gt;bci_to_data(trap_bci);
1599         int dos = (pdata == NULL)? 0: pdata-&gt;trap_state();
1600         if (dos != 0) {
1601           xtty-&gt;print(" state='%s'", format_trap_state(buf, sizeof(buf), dos));
1602           if (trap_state_is_recompiled(dos)) {
1603             int recnt2 = trap_mdo-&gt;overflow_recompile_count();
1604             if (recnt2 != 0)
1605               xtty-&gt;print(" recompiles2='%d'", recnt2);
1606           }
1607         }
1608       }
1609       if (xtty != NULL) {
1610         xtty-&gt;stamp();
1611         xtty-&gt;end_head();
1612       }
1613       if (TraceDeoptimization) {  // make noise on the tty
1614         tty-&gt;print("Uncommon trap occurred in");
1615         nm-&gt;method()-&gt;print_short_name(tty);
1616         tty-&gt;print(" compiler=%s compile_id=%d", nm-&gt;compiler() == NULL ? "" : nm-&gt;compiler()-&gt;name(), nm-&gt;compile_id());
1617 #if INCLUDE_JVMCI
1618         oop installedCode = nm-&gt;jvmci_installed_code();
1619         if (installedCode != NULL) {
1620           oop installedCodeName = NULL;
1621           if (installedCode-&gt;is_a(InstalledCode::klass())) {
1622             installedCodeName = InstalledCode::name(installedCode);
1623           }
1624           if (installedCodeName != NULL) {
1625             tty-&gt;print(" (JVMCI: installedCodeName=%s) ", java_lang_String::as_utf8_string(installedCodeName));
1626           } else {
1627             tty-&gt;print(" (JVMCI: installed code has no name) ");
1628           }
1629         } else if (nm-&gt;is_compiled_by_jvmci()) {
1630           tty-&gt;print(" (JVMCI: no installed code) ");
1631         }
1632 #endif
1633         tty-&gt;print(" (@" INTPTR_FORMAT ") thread=" UINTX_FORMAT " reason=%s action=%s unloaded_class_index=%d" JVMCI_ONLY(" debug_id=%d"),
1634                    p2i(fr.pc()),
1635                    os::current_thread_id(),
1636                    trap_reason_name(reason),
1637                    trap_action_name(action),
1638                    unloaded_class_index
1639 #if INCLUDE_JVMCI
1640                    , debug_id
1641 #endif
1642                    );
1643         if (class_name != NULL) {
1644           tty-&gt;print(unresolved ? " unresolved class: " : " symbol: ");
1645           class_name-&gt;print_symbol_on(tty);
1646         }
1647         tty-&gt;cr();
1648       }
1649       if (xtty != NULL) {
1650         // Log the precise location of the trap.
1651         for (ScopeDesc* sd = trap_scope; ; sd = sd-&gt;sender()) {
1652           xtty-&gt;begin_elem("jvms bci='%d'", sd-&gt;bci());
1653           xtty-&gt;method(sd-&gt;method());
1654           xtty-&gt;end_elem();
1655           if (sd-&gt;is_top())  break;
1656         }
1657         xtty-&gt;tail("uncommon_trap");
1658       }
1659     }
1660     // (End diagnostic printout.)
1661 
1662     // Load class if necessary
1663     if (unloaded_class_index &gt;= 0) {
1664       constantPoolHandle constants(THREAD, trap_method-&gt;constants());
1665       load_class_by_index(constants, unloaded_class_index);
1666     }
1667 
1668     // Flush the nmethod if necessary and desirable.
1669     //
1670     // We need to avoid situations where we are re-flushing the nmethod
1671     // because of a hot deoptimization site.  Repeated flushes at the same
1672     // point need to be detected by the compiler and avoided.  If the compiler
1673     // cannot avoid them (or has a bug and "refuses" to avoid them), this
1674     // module must take measures to avoid an infinite cycle of recompilation
1675     // and deoptimization.  There are several such measures:
1676     //
1677     //   1. If a recompilation is ordered a second time at some site X
1678     //   and for the same reason R, the action is adjusted to 'reinterpret',
1679     //   to give the interpreter time to exercise the method more thoroughly.
1680     //   If this happens, the method's overflow_recompile_count is incremented.
1681     //
1682     //   2. If the compiler fails to reduce the deoptimization rate, then
1683     //   the method's overflow_recompile_count will begin to exceed the set
1684     //   limit PerBytecodeRecompilationCutoff.  If this happens, the action
1685     //   is adjusted to 'make_not_compilable', and the method is abandoned
1686     //   to the interpreter.  This is a performance hit for hot methods,
1687     //   but is better than a disastrous infinite cycle of recompilations.
1688     //   (Actually, only the method containing the site X is abandoned.)
1689     //
1690     //   3. In parallel with the previous measures, if the total number of
1691     //   recompilations of a method exceeds the much larger set limit
1692     //   PerMethodRecompilationCutoff, the method is abandoned.
1693     //   This should only happen if the method is very large and has
1694     //   many "lukewarm" deoptimizations.  The code which enforces this
1695     //   limit is elsewhere (class nmethod, class Method).
1696     //
1697     // Note that the per-BCI 'is_recompiled' bit gives the compiler one chance
1698     // to recompile at each bytecode independently of the per-BCI cutoff.
1699     //
1700     // The decision to update code is up to the compiler, and is encoded
1701     // in the Action_xxx code.  If the compiler requests Action_none
1702     // no trap state is changed, no compiled code is changed, and the
1703     // computation suffers along in the interpreter.
1704     //
1705     // The other action codes specify various tactics for decompilation
1706     // and recompilation.  Action_maybe_recompile is the loosest, and
1707     // allows the compiled code to stay around until enough traps are seen,
1708     // and until the compiler gets around to recompiling the trapping method.
1709     //
1710     // The other actions cause immediate removal of the present code.
1711 
1712     // Traps caused by injected profile shouldn't pollute trap counts.
1713     bool injected_profile_trap = trap_method-&gt;has_injected_profile() &amp;&amp;
1714                                  (reason == Reason_intrinsic || reason == Reason_unreached);
1715 
1716     bool update_trap_state = (reason != Reason_tenured) &amp;&amp; !injected_profile_trap;
1717     bool make_not_entrant = false;
1718     bool make_not_compilable = false;
1719     bool reprofile = false;
1720     switch (action) {
1721     case Action_none:
1722       // Keep the old code.
1723       update_trap_state = false;
1724       break;
1725     case Action_maybe_recompile:
1726       // Do not need to invalidate the present code, but we can
1727       // initiate another
1728       // Start compiler without (necessarily) invalidating the nmethod.
1729       // The system will tolerate the old code, but new code should be
1730       // generated when possible.
1731       break;
1732     case Action_reinterpret:
1733       // Go back into the interpreter for a while, and then consider
1734       // recompiling form scratch.
1735       make_not_entrant = true;
1736       // Reset invocation counter for outer most method.
1737       // This will allow the interpreter to exercise the bytecodes
1738       // for a while before recompiling.
1739       // By contrast, Action_make_not_entrant is immediate.
1740       //
1741       // Note that the compiler will track null_check, null_assert,
1742       // range_check, and class_check events and log them as if they
1743       // had been traps taken from compiled code.  This will update
1744       // the MDO trap history so that the next compilation will
1745       // properly detect hot trap sites.
1746       reprofile = true;
1747       break;
1748     case Action_make_not_entrant:
1749       // Request immediate recompilation, and get rid of the old code.
1750       // Make them not entrant, so next time they are called they get
1751       // recompiled.  Unloaded classes are loaded now so recompile before next
1752       // time they are called.  Same for uninitialized.  The interpreter will
1753       // link the missing class, if any.
1754       make_not_entrant = true;
1755       break;
1756     case Action_make_not_compilable:
1757       // Give up on compiling this method at all.
1758       make_not_entrant = true;
1759       make_not_compilable = true;
1760       break;
1761     default:
1762       ShouldNotReachHere();
1763     }
1764 
1765     // Setting +ProfileTraps fixes the following, on all platforms:
1766     // 4852688: ProfileInterpreter is off by default for ia64.  The result is
1767     // infinite heroic-opt-uncommon-trap/deopt/recompile cycles, since the
1768     // recompile relies on a MethodData* to record heroic opt failures.
1769 
1770     // Whether the interpreter is producing MDO data or not, we also need
1771     // to use the MDO to detect hot deoptimization points and control
1772     // aggressive optimization.
1773     bool inc_recompile_count = false;
1774     ProfileData* pdata = NULL;
1775     if (ProfileTraps &amp;&amp; update_trap_state &amp;&amp; trap_mdo != NULL) {
1776       assert(trap_mdo == get_method_data(thread, profiled_method, false), "sanity");
1777       uint this_trap_count = 0;
1778       bool maybe_prior_trap = false;
1779       bool maybe_prior_recompile = false;
1780       pdata = query_update_method_data(trap_mdo, trap_bci, reason, true,
1781 #if INCLUDE_JVMCI
1782                                    nm-&gt;is_compiled_by_jvmci() &amp;&amp; nm-&gt;is_osr_method(),
1783 #endif
1784                                    nm-&gt;method(),
1785                                    //outputs:
1786                                    this_trap_count,
1787                                    maybe_prior_trap,
1788                                    maybe_prior_recompile);
1789       // Because the interpreter also counts null, div0, range, and class
1790       // checks, these traps from compiled code are double-counted.
1791       // This is harmless; it just means that the PerXTrapLimit values
1792       // are in effect a little smaller than they look.
1793 
1794       DeoptReason per_bc_reason = reason_recorded_per_bytecode_if_any(reason);
1795       if (per_bc_reason != Reason_none) {
1796         // Now take action based on the partially known per-BCI history.
1797         if (maybe_prior_trap
1798             &amp;&amp; this_trap_count &gt;= (uint)PerBytecodeTrapLimit) {
1799           // If there are too many traps at this BCI, force a recompile.
1800           // This will allow the compiler to see the limit overflow, and
1801           // take corrective action, if possible.  The compiler generally
1802           // does not use the exact PerBytecodeTrapLimit value, but instead
1803           // changes its tactics if it sees any traps at all.  This provides
1804           // a little hysteresis, delaying a recompile until a trap happens
1805           // several times.
1806           //
1807           // Actually, since there is only one bit of counter per BCI,
1808           // the possible per-BCI counts are {0,1,(per-method count)}.
1809           // This produces accurate results if in fact there is only
1810           // one hot trap site, but begins to get fuzzy if there are
1811           // many sites.  For example, if there are ten sites each
1812           // trapping two or more times, they each get the blame for
1813           // all of their traps.
1814           make_not_entrant = true;
1815         }
1816 
1817         // Detect repeated recompilation at the same BCI, and enforce a limit.
1818         if (make_not_entrant &amp;&amp; maybe_prior_recompile) {
1819           // More than one recompile at this point.
1820           inc_recompile_count = maybe_prior_trap;
1821         }
1822       } else {
1823         // For reasons which are not recorded per-bytecode, we simply
1824         // force recompiles unconditionally.
1825         // (Note that PerMethodRecompilationCutoff is enforced elsewhere.)
1826         make_not_entrant = true;
1827       }
1828 
1829       // Go back to the compiler if there are too many traps in this method.
1830       if (this_trap_count &gt;= per_method_trap_limit(reason)) {
1831         // If there are too many traps in this method, force a recompile.
1832         // This will allow the compiler to see the limit overflow, and
1833         // take corrective action, if possible.
1834         // (This condition is an unlikely backstop only, because the
1835         // PerBytecodeTrapLimit is more likely to take effect first,
1836         // if it is applicable.)
1837         make_not_entrant = true;
1838       }
1839 
1840       // Here's more hysteresis:  If there has been a recompile at
1841       // this trap point already, run the method in the interpreter
1842       // for a while to exercise it more thoroughly.
1843       if (make_not_entrant &amp;&amp; maybe_prior_recompile &amp;&amp; maybe_prior_trap) {
1844         reprofile = true;
1845       }
1846     }
1847 
1848     // Take requested actions on the method:
1849 
1850     // Recompile
1851     if (make_not_entrant) {
1852       if (!nm-&gt;make_not_entrant()) {
1853         return; // the call did not change nmethod's state
1854       }
1855 
1856       if (pdata != NULL) {
1857         // Record the recompilation event, if any.
1858         int tstate0 = pdata-&gt;trap_state();
1859         int tstate1 = trap_state_set_recompiled(tstate0, true);
1860         if (tstate1 != tstate0)
1861           pdata-&gt;set_trap_state(tstate1);
1862       }
1863 
1864 #if INCLUDE_RTM_OPT
1865       // Restart collecting RTM locking abort statistic if the method
1866       // is recompiled for a reason other than RTM state change.
1867       // Assume that in new recompiled code the statistic could be different,
1868       // for example, due to different inlining.
1869       if ((reason != Reason_rtm_state_change) &amp;&amp; (trap_mdo != NULL) &amp;&amp;
1870           UseRTMDeopt &amp;&amp; (nm-&gt;rtm_state() != ProfileRTM)) {
1871         trap_mdo-&gt;atomic_set_rtm_state(ProfileRTM);
1872       }
1873 #endif
1874       // For code aging we count traps separately here, using make_not_entrant()
1875       // as a guard against simultaneous deopts in multiple threads.
1876       if (reason == Reason_tenured &amp;&amp; trap_mdo != NULL) {
1877         trap_mdo-&gt;inc_tenure_traps();
1878       }
1879     }
1880 
1881     if (inc_recompile_count) {
1882       trap_mdo-&gt;inc_overflow_recompile_count();
1883       if ((uint)trap_mdo-&gt;overflow_recompile_count() &gt;
1884           (uint)PerBytecodeRecompilationCutoff) {
1885         // Give up on the method containing the bad BCI.
1886         if (trap_method() == nm-&gt;method()) {
1887           make_not_compilable = true;
1888         } else {
1889           trap_method-&gt;set_not_compilable(CompLevel_full_optimization, true, "overflow_recompile_count &gt; PerBytecodeRecompilationCutoff");
1890           // But give grace to the enclosing nm-&gt;method().
1891         }
1892       }
1893     }
1894 
1895     // Reprofile
1896     if (reprofile) {
1897       CompilationPolicy::policy()-&gt;reprofile(trap_scope, nm-&gt;is_osr_method());
1898     }
1899 
1900     // Give up compiling
1901     if (make_not_compilable &amp;&amp; !nm-&gt;method()-&gt;is_not_compilable(CompLevel_full_optimization)) {
1902       assert(make_not_entrant, "consistent");
1903       nm-&gt;method()-&gt;set_not_compilable(CompLevel_full_optimization);
1904     }
1905 
1906   } // Free marked resources
1907 
1908 }
1909 JRT_END
1910 
1911 ProfileData*
1912 Deoptimization::query_update_method_data(MethodData* trap_mdo,
1913                                          int trap_bci,
1914                                          Deoptimization::DeoptReason reason,
1915                                          bool update_total_trap_count,
1916 #if INCLUDE_JVMCI
1917                                          bool is_osr,
1918 #endif
1919                                          Method* compiled_method,
1920                                          //outputs:
1921                                          uint&amp; ret_this_trap_count,
1922                                          bool&amp; ret_maybe_prior_trap,
1923                                          bool&amp; ret_maybe_prior_recompile) {
1924   bool maybe_prior_trap = false;
1925   bool maybe_prior_recompile = false;
1926   uint this_trap_count = 0;
1927   if (update_total_trap_count) {
1928     uint idx = reason;
1929 #if INCLUDE_JVMCI
1930     if (is_osr) {
1931       idx += Reason_LIMIT;
1932     }
1933 #endif
1934     uint prior_trap_count = trap_mdo-&gt;trap_count(idx);
1935     this_trap_count  = trap_mdo-&gt;inc_trap_count(idx);
1936 
1937     // If the runtime cannot find a place to store trap history,
1938     // it is estimated based on the general condition of the method.
1939     // If the method has ever been recompiled, or has ever incurred
1940     // a trap with the present reason , then this BCI is assumed
1941     // (pessimistically) to be the culprit.
1942     maybe_prior_trap      = (prior_trap_count != 0);
1943     maybe_prior_recompile = (trap_mdo-&gt;decompile_count() != 0);
1944   }
1945   ProfileData* pdata = NULL;
1946 
1947 
1948   // For reasons which are recorded per bytecode, we check per-BCI data.
1949   DeoptReason per_bc_reason = reason_recorded_per_bytecode_if_any(reason);
1950   assert(per_bc_reason != Reason_none || update_total_trap_count, "must be");
1951   if (per_bc_reason != Reason_none) {
1952     // Find the profile data for this BCI.  If there isn't one,
1953     // try to allocate one from the MDO's set of spares.
1954     // This will let us detect a repeated trap at this point.
1955     pdata = trap_mdo-&gt;allocate_bci_to_data(trap_bci, reason_is_speculate(reason) ? compiled_method : NULL);
1956 
1957     if (pdata != NULL) {
1958       if (reason_is_speculate(reason) &amp;&amp; !pdata-&gt;is_SpeculativeTrapData()) {
1959         if (LogCompilation &amp;&amp; xtty != NULL) {
1960           ttyLocker ttyl;
1961           // no more room for speculative traps in this MDO
1962           xtty-&gt;elem("speculative_traps_oom");
1963         }
1964       }
1965       // Query the trap state of this profile datum.
1966       int tstate0 = pdata-&gt;trap_state();
1967       if (!trap_state_has_reason(tstate0, per_bc_reason))
1968         maybe_prior_trap = false;
1969       if (!trap_state_is_recompiled(tstate0))
1970         maybe_prior_recompile = false;
1971 
1972       // Update the trap state of this profile datum.
1973       int tstate1 = tstate0;
1974       // Record the reason.
1975       tstate1 = trap_state_add_reason(tstate1, per_bc_reason);
1976       // Store the updated state on the MDO, for next time.
1977       if (tstate1 != tstate0)
1978         pdata-&gt;set_trap_state(tstate1);
1979     } else {
1980       if (LogCompilation &amp;&amp; xtty != NULL) {
1981         ttyLocker ttyl;
1982         // Missing MDP?  Leave a small complaint in the log.
1983         xtty-&gt;elem("missing_mdp bci='%d'", trap_bci);
1984       }
1985     }
1986   }
1987 
1988   // Return results:
1989   ret_this_trap_count = this_trap_count;
1990   ret_maybe_prior_trap = maybe_prior_trap;
1991   ret_maybe_prior_recompile = maybe_prior_recompile;
1992   return pdata;
1993 }
1994 
1995 void
1996 Deoptimization::update_method_data_from_interpreter(MethodData* trap_mdo, int trap_bci, int reason) {
1997   ResourceMark rm;
1998   // Ignored outputs:
1999   uint ignore_this_trap_count;
2000   bool ignore_maybe_prior_trap;
2001   bool ignore_maybe_prior_recompile;
2002   assert(!reason_is_speculate(reason), "reason speculate only used by compiler");
2003   // JVMCI uses the total counts to determine if deoptimizations are happening too frequently -&gt; do not adjust total counts
2004   bool update_total_counts = JVMCI_ONLY(false) NOT_JVMCI(true);
2005   query_update_method_data(trap_mdo, trap_bci,
2006                            (DeoptReason)reason,
2007                            update_total_counts,
2008 #if INCLUDE_JVMCI
2009                            false,
2010 #endif
2011                            NULL,
2012                            ignore_this_trap_count,
2013                            ignore_maybe_prior_trap,
2014                            ignore_maybe_prior_recompile);
2015 }
2016 
2017 Deoptimization::UnrollBlock* Deoptimization::uncommon_trap(JavaThread* thread, jint trap_request, jint exec_mode) {
2018   if (TraceDeoptimization) {
2019     tty-&gt;print("Uncommon trap ");
2020   }
2021   // Still in Java no safepoints
2022   {
2023     // This enters VM and may safepoint
2024     uncommon_trap_inner(thread, trap_request);
2025   }
2026   return fetch_unroll_info_helper(thread, exec_mode);
2027 }
2028 
2029 // Local derived constants.
2030 // Further breakdown of DataLayout::trap_state, as promised by DataLayout.
2031 const int DS_REASON_MASK   = DataLayout::trap_mask &gt;&gt; 1;
2032 const int DS_RECOMPILE_BIT = DataLayout::trap_mask - DS_REASON_MASK;
2033 
2034 //---------------------------trap_state_reason---------------------------------
2035 Deoptimization::DeoptReason
2036 Deoptimization::trap_state_reason(int trap_state) {
2037   // This assert provides the link between the width of DataLayout::trap_bits
2038   // and the encoding of "recorded" reasons.  It ensures there are enough
2039   // bits to store all needed reasons in the per-BCI MDO profile.
2040   assert(DS_REASON_MASK &gt;= Reason_RECORDED_LIMIT, "enough bits");
2041   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2042   trap_state -= recompile_bit;
2043   if (trap_state == DS_REASON_MASK) {
2044     return Reason_many;
2045   } else {
2046     assert((int)Reason_none == 0, "state=0 =&gt; Reason_none");
2047     return (DeoptReason)trap_state;
2048   }
2049 }
2050 //-------------------------trap_state_has_reason-------------------------------
2051 int Deoptimization::trap_state_has_reason(int trap_state, int reason) {
2052   assert(reason_is_recorded_per_bytecode((DeoptReason)reason), "valid reason");
2053   assert(DS_REASON_MASK &gt;= Reason_RECORDED_LIMIT, "enough bits");
2054   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2055   trap_state -= recompile_bit;
2056   if (trap_state == DS_REASON_MASK) {
2057     return -1;  // true, unspecifically (bottom of state lattice)
2058   } else if (trap_state == reason) {
2059     return 1;   // true, definitely
2060   } else if (trap_state == 0) {
2061     return 0;   // false, definitely (top of state lattice)
2062   } else {
2063     return 0;   // false, definitely
2064   }
2065 }
2066 //-------------------------trap_state_add_reason-------------------------------
2067 int Deoptimization::trap_state_add_reason(int trap_state, int reason) {
2068   assert(reason_is_recorded_per_bytecode((DeoptReason)reason) || reason == Reason_many, "valid reason");
2069   int recompile_bit = (trap_state &amp; DS_RECOMPILE_BIT);
2070   trap_state -= recompile_bit;
2071   if (trap_state == DS_REASON_MASK) {
2072     return trap_state + recompile_bit;     // already at state lattice bottom
2073   } else if (trap_state == reason) {
2074     return trap_state + recompile_bit;     // the condition is already true
2075   } else if (trap_state == 0) {
2076     return reason + recompile_bit;          // no condition has yet been true
2077   } else {
2078     return DS_REASON_MASK + recompile_bit;  // fall to state lattice bottom
2079   }
2080 }
2081 //-----------------------trap_state_is_recompiled------------------------------
2082 bool Deoptimization::trap_state_is_recompiled(int trap_state) {
2083   return (trap_state &amp; DS_RECOMPILE_BIT) != 0;
2084 }
2085 //-----------------------trap_state_set_recompiled-----------------------------
2086 int Deoptimization::trap_state_set_recompiled(int trap_state, bool z) {
2087   if (z)  return trap_state |  DS_RECOMPILE_BIT;
2088   else    return trap_state &amp; ~DS_RECOMPILE_BIT;
2089 }
2090 //---------------------------format_trap_state---------------------------------
2091 // This is used for debugging and diagnostics, including LogFile output.
2092 const char* Deoptimization::format_trap_state(char* buf, size_t buflen,
2093                                               int trap_state) {
2094   assert(buflen &gt; 0, "sanity");
2095   DeoptReason reason      = trap_state_reason(trap_state);
2096   bool        recomp_flag = trap_state_is_recompiled(trap_state);
2097   // Re-encode the state from its decoded components.
2098   int decoded_state = 0;
2099   if (reason_is_recorded_per_bytecode(reason) || reason == Reason_many)
2100     decoded_state = trap_state_add_reason(decoded_state, reason);
2101   if (recomp_flag)
2102     decoded_state = trap_state_set_recompiled(decoded_state, recomp_flag);
2103   // If the state re-encodes properly, format it symbolically.
2104   // Because this routine is used for debugging and diagnostics,
2105   // be robust even if the state is a strange value.
2106   size_t len;
2107   if (decoded_state != trap_state) {
2108     // Random buggy state that doesn't decode??
2109     len = jio_snprintf(buf, buflen, "#%d", trap_state);
2110   } else {
2111     len = jio_snprintf(buf, buflen, "%s%s",
2112                        trap_reason_name(reason),
2113                        recomp_flag ? " recompiled" : "");
2114   }
2115   return buf;
2116 }
2117 
2118 
2119 //--------------------------------statics--------------------------------------
2120 const char* Deoptimization::_trap_reason_name[] = {
2121   // Note:  Keep this in sync. with enum DeoptReason.
2122   "none",
2123   "null_check",
2124   "null_assert" JVMCI_ONLY("_or_unreached0"),
2125   "range_check",
2126   "class_check",
2127   "array_check",
2128   "intrinsic" JVMCI_ONLY("_or_type_checked_inlining"),
2129   "bimorphic" JVMCI_ONLY("_or_optimized_type_check"),
2130   "unloaded",
2131   "uninitialized",
2132   "unreached",
2133   "unhandled",
2134   "constraint",
2135   "div0_check",
2136   "age",
2137   "predicate",
2138   "loop_limit_check",
2139   "speculate_class_check",
2140   "speculate_null_check",
2141   "rtm_state_change",
2142   "unstable_if",
2143   "unstable_fused_if",
2144 #if INCLUDE_JVMCI
2145   "aliasing",
2146   "transfer_to_interpreter",
2147   "not_compiled_exception_handler",
2148   "unresolved",
2149   "jsr_mismatch",
2150 #endif
2151   "tenured"
2152 };
2153 const char* Deoptimization::_trap_action_name[] = {
2154   // Note:  Keep this in sync. with enum DeoptAction.
2155   "none",
2156   "maybe_recompile",
2157   "reinterpret",
2158   "make_not_entrant",
2159   "make_not_compilable"
2160 };
2161 
2162 const char* Deoptimization::trap_reason_name(int reason) {
2163   // Check that every reason has a name
2164   STATIC_ASSERT(sizeof(_trap_reason_name)/sizeof(const char*) == Reason_LIMIT);
2165 
2166   if (reason == Reason_many)  return "many";
2167   if ((uint)reason &lt; Reason_LIMIT)
2168     return _trap_reason_name[reason];
2169   static char buf[20];
2170   sprintf(buf, "reason%d", reason);
2171   return buf;
2172 }
2173 const char* Deoptimization::trap_action_name(int action) {
2174   // Check that every action has a name
2175   STATIC_ASSERT(sizeof(_trap_action_name)/sizeof(const char*) == Action_LIMIT);
2176 
2177   if ((uint)action &lt; Action_LIMIT)
2178     return _trap_action_name[action];
2179   static char buf[20];
2180   sprintf(buf, "action%d", action);
2181   return buf;
2182 }
2183 
2184 // This is used for debugging and diagnostics, including LogFile output.
2185 const char* Deoptimization::format_trap_request(char* buf, size_t buflen,
2186                                                 int trap_request) {
2187   jint unloaded_class_index = trap_request_index(trap_request);
2188   const char* reason = trap_reason_name(trap_request_reason(trap_request));
2189   const char* action = trap_action_name(trap_request_action(trap_request));
2190 #if INCLUDE_JVMCI
2191   int debug_id = trap_request_debug_id(trap_request);
2192 #endif
2193   size_t len;
2194   if (unloaded_class_index &lt; 0) {
2195     len = jio_snprintf(buf, buflen, "reason='%s' action='%s'" JVMCI_ONLY(" debug_id='%d'"),
2196                        reason, action
2197 #if INCLUDE_JVMCI
2198                        ,debug_id
2199 #endif
2200                        );
2201   } else {
2202     len = jio_snprintf(buf, buflen, "reason='%s' action='%s' index='%d'" JVMCI_ONLY(" debug_id='%d'"),
2203                        reason, action, unloaded_class_index
2204 #if INCLUDE_JVMCI
2205                        ,debug_id
2206 #endif
2207                        );
2208   }
2209   return buf;
2210 }
2211 
2212 juint Deoptimization::_deoptimization_hist
2213         [Deoptimization::Reason_LIMIT]
2214     [1 + Deoptimization::Action_LIMIT]
2215         [Deoptimization::BC_CASE_LIMIT]
2216   = {0};
2217 
2218 enum {
2219   LSB_BITS = 8,
2220   LSB_MASK = right_n_bits(LSB_BITS)
2221 };
2222 
2223 void Deoptimization::gather_statistics(DeoptReason reason, DeoptAction action,
2224                                        Bytecodes::Code bc) {
2225   assert(reason &gt;= 0 &amp;&amp; reason &lt; Reason_LIMIT, "oob");
2226   assert(action &gt;= 0 &amp;&amp; action &lt; Action_LIMIT, "oob");
2227   _deoptimization_hist[Reason_none][0][0] += 1;  // total
2228   _deoptimization_hist[reason][0][0]      += 1;  // per-reason total
2229   juint* cases = _deoptimization_hist[reason][1+action];
2230   juint* bc_counter_addr = NULL;
2231   juint  bc_counter      = 0;
2232   // Look for an unused counter, or an exact match to this BC.
2233   if (bc != Bytecodes::_illegal) {
2234     for (int bc_case = 0; bc_case &lt; BC_CASE_LIMIT; bc_case++) {
2235       juint* counter_addr = &amp;cases[bc_case];
2236       juint  counter = *counter_addr;
2237       if ((counter == 0 &amp;&amp; bc_counter_addr == NULL)
2238           || (Bytecodes::Code)(counter &amp; LSB_MASK) == bc) {
2239         // this counter is either free or is already devoted to this BC
2240         bc_counter_addr = counter_addr;
2241         bc_counter = counter | bc;
2242       }
2243     }
2244   }
2245   if (bc_counter_addr == NULL) {
2246     // Overflow, or no given bytecode.
2247     bc_counter_addr = &amp;cases[BC_CASE_LIMIT-1];
2248     bc_counter = (*bc_counter_addr &amp; ~LSB_MASK);  // clear LSB
2249   }
2250   *bc_counter_addr = bc_counter + (1 &lt;&lt; LSB_BITS);
2251 }
2252 
2253 jint Deoptimization::total_deoptimization_count() {
2254   return _deoptimization_hist[Reason_none][0][0];
2255 }
2256 
2257 jint Deoptimization::deoptimization_count(DeoptReason reason) {
2258   assert(reason &gt;= 0 &amp;&amp; reason &lt; Reason_LIMIT, "oob");
2259   return _deoptimization_hist[reason][0][0];
2260 }
2261 
2262 void Deoptimization::print_statistics() {
2263   juint total = total_deoptimization_count();
2264   juint account = total;
2265   if (total != 0) {
2266     ttyLocker ttyl;
2267     if (xtty != NULL)  xtty-&gt;head("statistics type='deoptimization'");
2268     tty-&gt;print_cr("Deoptimization traps recorded:");
2269     #define PRINT_STAT_LINE(name, r) \
2270       tty-&gt;print_cr("  %4d (%4.1f%%) %s", (int)(r), ((r) * 100.0) / total, name);
2271     PRINT_STAT_LINE("total", total);
2272     // For each non-zero entry in the histogram, print the reason,
2273     // the action, and (if specifically known) the type of bytecode.
2274     for (int reason = 0; reason &lt; Reason_LIMIT; reason++) {
2275       for (int action = 0; action &lt; Action_LIMIT; action++) {
2276         juint* cases = _deoptimization_hist[reason][1+action];
2277         for (int bc_case = 0; bc_case &lt; BC_CASE_LIMIT; bc_case++) {
2278           juint counter = cases[bc_case];
2279           if (counter != 0) {
2280             char name[1*K];
2281             Bytecodes::Code bc = (Bytecodes::Code)(counter &amp; LSB_MASK);
2282             if (bc_case == BC_CASE_LIMIT &amp;&amp; (int)bc == 0)
2283               bc = Bytecodes::_illegal;
2284             sprintf(name, "%s/%s/%s",
2285                     trap_reason_name(reason),
2286                     trap_action_name(action),
2287                     Bytecodes::is_defined(bc)? Bytecodes::name(bc): "other");
2288             juint r = counter &gt;&gt; LSB_BITS;
2289             tty-&gt;print_cr("  %40s: " UINT32_FORMAT " (%.1f%%)", name, r, (r * 100.0) / total);
2290             account -= r;
2291           }
2292         }
2293       }
2294     }
2295     if (account != 0) {
2296       PRINT_STAT_LINE("unaccounted", account);
2297     }
2298     #undef PRINT_STAT_LINE
2299     if (xtty != NULL)  xtty-&gt;tail("statistics");
2300   }
2301 }
2302 #else // COMPILER2 || SHARK || INCLUDE_JVMCI
2303 
2304 
2305 // Stubs for C1 only system.
2306 bool Deoptimization::trap_state_is_recompiled(int trap_state) {
2307   return false;
2308 }
2309 
2310 const char* Deoptimization::trap_reason_name(int reason) {
2311   return "unknown";
2312 }
2313 
2314 void Deoptimization::print_statistics() {
2315   // no output
2316 }
2317 
2318 void
2319 Deoptimization::update_method_data_from_interpreter(MethodData* trap_mdo, int trap_bci, int reason) {
2320   // no udpate
2321 }
2322 
2323 int Deoptimization::trap_state_has_reason(int trap_state, int reason) {
2324   return 0;
2325 }
2326 
2327 void Deoptimization::gather_statistics(DeoptReason reason, DeoptAction action,
2328                                        Bytecodes::Code bc) {
2329   // no update
2330 }
2331 
2332 const char* Deoptimization::format_trap_state(char* buf, size_t buflen,
2333                                               int trap_state) {
2334   jio_snprintf(buf, buflen, "#%d", trap_state);
2335   return buf;
2336 }
2337 
2338 #endif // COMPILER2 || SHARK || INCLUDE_JVMCI
</pre></body></html>
