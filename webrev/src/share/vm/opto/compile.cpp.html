<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/opto/compile.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/macroAssembler.hpp"
  27 #include "asm/macroAssembler.inline.hpp"
  28 #include "ci/ciReplay.hpp"
  29 #include "ci/ciCacheReplay.hpp"
  30 #include "ci/ciCacheProfiles.hpp"
  31 #include "classfile/systemDictionary.hpp"
  32 #include "code/exceptionHandlerTable.hpp"
  33 #include "code/nmethod.hpp"
  34 #include "compiler/compileBroker.hpp"
  35 #include "compiler/compileLog.hpp"
  36 #include "compiler/disassembler.hpp"
  37 #include "compiler/oopMap.hpp"
  38 #include "opto/addnode.hpp"
  39 #include "opto/block.hpp"
  40 #include "opto/c2compiler.hpp"
  41 #include "opto/callGenerator.hpp"
  42 #include "opto/callnode.hpp"
  43 #include "opto/castnode.hpp"
  44 #include "opto/cfgnode.hpp"
  45 #include "opto/chaitin.hpp"
  46 #include "opto/compile.hpp"
  47 #include "opto/connode.hpp"
  48 #include "opto/convertnode.hpp"
  49 #include "opto/divnode.hpp"
  50 #include "opto/escape.hpp"
  51 #include "opto/idealGraphPrinter.hpp"
  52 #include "opto/loopnode.hpp"
  53 #include "opto/machnode.hpp"
  54 #include "opto/macro.hpp"
  55 #include "opto/matcher.hpp"
  56 #include "opto/mathexactnode.hpp"
  57 #include "opto/memnode.hpp"
  58 #include "opto/mulnode.hpp"
  59 #include "opto/narrowptrnode.hpp"
  60 #include "opto/node.hpp"
  61 #include "opto/opcodes.hpp"
  62 #include "opto/output.hpp"
  63 #include "opto/parse.hpp"
  64 #include "opto/phaseX.hpp"
  65 #include "opto/rootnode.hpp"
  66 #include "opto/runtime.hpp"
  67 #include "opto/stringopts.hpp"
  68 #include "opto/type.hpp"
  69 #include "opto/vectornode.hpp"
  70 #include "runtime/arguments.hpp"
  71 #include "runtime/sharedRuntime.hpp"
  72 #include "runtime/signature.hpp"
  73 #include "runtime/stubRoutines.hpp"
  74 #include "runtime/timer.hpp"
  75 #include "utilities/copy.hpp"
  76 
  77 
  78 // -------------------- Compile::mach_constant_base_node -----------------------
  79 // Constant table base node singleton.
  80 MachConstantBaseNode* Compile::mach_constant_base_node() {
  81   if (_mach_constant_base_node == NULL) {
  82     _mach_constant_base_node = new MachConstantBaseNode();
  83     _mach_constant_base_node-&gt;add_req(C-&gt;root());
  84   }
  85   return _mach_constant_base_node;
  86 }
  87 
  88 
  89 /// Support for intrinsics.
  90 
  91 // Return the index at which m must be inserted (or already exists).
  92 // The sort order is by the address of the ciMethod, with is_virtual as minor key.
  93 class IntrinsicDescPair {
  94  private:
  95   ciMethod* _m;
  96   bool _is_virtual;
  97  public:
  98   IntrinsicDescPair(ciMethod* m, bool is_virtual) : _m(m), _is_virtual(is_virtual) {}
  99   static int compare(IntrinsicDescPair* const&amp; key, CallGenerator* const&amp; elt) {
 100     ciMethod* m= elt-&gt;method();
 101     ciMethod* key_m = key-&gt;_m;
 102     if (key_m &lt; m)      return -1;
 103     else if (key_m &gt; m) return 1;
 104     else {
 105       bool is_virtual = elt-&gt;is_virtual();
 106       bool key_virtual = key-&gt;_is_virtual;
 107       if (key_virtual &lt; is_virtual)      return -1;
 108       else if (key_virtual &gt; is_virtual) return 1;
 109       else                               return 0;
 110     }
 111   }
 112 };
 113 int Compile::intrinsic_insertion_index(ciMethod* m, bool is_virtual, bool&amp; found) {
 114 #ifdef ASSERT
 115   for (int i = 1; i &lt; _intrinsics-&gt;length(); i++) {
 116     CallGenerator* cg1 = _intrinsics-&gt;at(i-1);
 117     CallGenerator* cg2 = _intrinsics-&gt;at(i);
 118     assert(cg1-&gt;method() != cg2-&gt;method()
 119            ? cg1-&gt;method()     &lt; cg2-&gt;method()
 120            : cg1-&gt;is_virtual() &lt; cg2-&gt;is_virtual(),
 121            "compiler intrinsics list must stay sorted");
 122   }
 123 #endif
 124   IntrinsicDescPair pair(m, is_virtual);
 125   return _intrinsics-&gt;find_sorted&lt;IntrinsicDescPair*, IntrinsicDescPair::compare&gt;(&amp;pair, found);
 126 }
 127 
 128 void Compile::register_intrinsic(CallGenerator* cg) {
 129   if (_intrinsics == NULL) {
 130     _intrinsics = new (comp_arena())GrowableArray&lt;CallGenerator*&gt;(comp_arena(), 60, 0, NULL);
 131   }
 132   int len = _intrinsics-&gt;length();
 133   bool found = false;
 134   int index = intrinsic_insertion_index(cg-&gt;method(), cg-&gt;is_virtual(), found);
 135   assert(!found, "registering twice");
 136   _intrinsics-&gt;insert_before(index, cg);
 137   assert(find_intrinsic(cg-&gt;method(), cg-&gt;is_virtual()) == cg, "registration worked");
 138 }
 139 
 140 CallGenerator* Compile::find_intrinsic(ciMethod* m, bool is_virtual) {
 141   assert(m-&gt;is_loaded(), "don't try this on unloaded methods");
 142   if (_intrinsics != NULL) {
 143     bool found = false;
 144     int index = intrinsic_insertion_index(m, is_virtual, found);
 145      if (found) {
 146       return _intrinsics-&gt;at(index);
 147     }
 148   }
 149   // Lazily create intrinsics for intrinsic IDs well-known in the runtime.
 150   if (m-&gt;intrinsic_id() != vmIntrinsics::_none &amp;&amp;
 151       m-&gt;intrinsic_id() &lt;= vmIntrinsics::LAST_COMPILER_INLINE) {
 152     CallGenerator* cg = make_vm_intrinsic(m, is_virtual);
 153     if (cg != NULL) {
 154       // Save it for next time:
 155       register_intrinsic(cg);
 156       return cg;
 157     } else {
 158       gather_intrinsic_statistics(m-&gt;intrinsic_id(), is_virtual, _intrinsic_disabled);
 159     }
 160   }
 161   return NULL;
 162 }
 163 
 164 // Compile:: register_library_intrinsics and make_vm_intrinsic are defined
 165 // in library_call.cpp.
 166 
 167 
 168 #ifndef PRODUCT
 169 // statistics gathering...
 170 
 171 juint  Compile::_intrinsic_hist_count[vmIntrinsics::ID_LIMIT] = {0};
 172 jubyte Compile::_intrinsic_hist_flags[vmIntrinsics::ID_LIMIT] = {0};
 173 
 174 bool Compile::gather_intrinsic_statistics(vmIntrinsics::ID id, bool is_virtual, int flags) {
 175   assert(id &gt; vmIntrinsics::_none &amp;&amp; id &lt; vmIntrinsics::ID_LIMIT, "oob");
 176   int oflags = _intrinsic_hist_flags[id];
 177   assert(flags != 0, "what happened?");
 178   if (is_virtual) {
 179     flags |= _intrinsic_virtual;
 180   }
 181   bool changed = (flags != oflags);
 182   if ((flags &amp; _intrinsic_worked) != 0) {
 183     juint count = (_intrinsic_hist_count[id] += 1);
 184     if (count == 1) {
 185       changed = true;           // first time
 186     }
 187     // increment the overall count also:
 188     _intrinsic_hist_count[vmIntrinsics::_none] += 1;
 189   }
 190   if (changed) {
 191     if (((oflags ^ flags) &amp; _intrinsic_virtual) != 0) {
 192       // Something changed about the intrinsic's virtuality.
 193       if ((flags &amp; _intrinsic_virtual) != 0) {
 194         // This is the first use of this intrinsic as a virtual call.
 195         if (oflags != 0) {
 196           // We already saw it as a non-virtual, so note both cases.
 197           flags |= _intrinsic_both;
 198         }
 199       } else if ((oflags &amp; _intrinsic_both) == 0) {
 200         // This is the first use of this intrinsic as a non-virtual
 201         flags |= _intrinsic_both;
 202       }
 203     }
 204     _intrinsic_hist_flags[id] = (jubyte) (oflags | flags);
 205   }
 206   // update the overall flags also:
 207   _intrinsic_hist_flags[vmIntrinsics::_none] |= (jubyte) flags;
 208   return changed;
 209 }
 210 
 211 static char* format_flags(int flags, char* buf) {
 212   buf[0] = 0;
 213   if ((flags &amp; Compile::_intrinsic_worked) != 0)    strcat(buf, ",worked");
 214   if ((flags &amp; Compile::_intrinsic_failed) != 0)    strcat(buf, ",failed");
 215   if ((flags &amp; Compile::_intrinsic_disabled) != 0)  strcat(buf, ",disabled");
 216   if ((flags &amp; Compile::_intrinsic_virtual) != 0)   strcat(buf, ",virtual");
 217   if ((flags &amp; Compile::_intrinsic_both) != 0)      strcat(buf, ",nonvirtual");
 218   if (buf[0] == 0)  strcat(buf, ",");
 219   assert(buf[0] == ',', "must be");
 220   return &amp;buf[1];
 221 }
 222 
 223 void Compile::print_intrinsic_statistics() {
 224   char flagsbuf[100];
 225   ttyLocker ttyl;
 226   if (xtty != NULL)  xtty-&gt;head("statistics type='intrinsic'");
 227   tty-&gt;print_cr("Compiler intrinsic usage:");
 228   juint total = _intrinsic_hist_count[vmIntrinsics::_none];
 229   if (total == 0)  total = 1;  // avoid div0 in case of no successes
 230   #define PRINT_STAT_LINE(name, c, f) \
 231     tty-&gt;print_cr("  %4d (%4.1f%%) %s (%s)", (int)(c), ((c) * 100.0) / total, name, f);
 232   for (int index = 1 + (int)vmIntrinsics::_none; index &lt; (int)vmIntrinsics::ID_LIMIT; index++) {
 233     vmIntrinsics::ID id = (vmIntrinsics::ID) index;
 234     int   flags = _intrinsic_hist_flags[id];
 235     juint count = _intrinsic_hist_count[id];
 236     if ((flags | count) != 0) {
 237       PRINT_STAT_LINE(vmIntrinsics::name_at(id), count, format_flags(flags, flagsbuf));
 238     }
 239   }
 240   PRINT_STAT_LINE("total", total, format_flags(_intrinsic_hist_flags[vmIntrinsics::_none], flagsbuf));
 241   if (xtty != NULL)  xtty-&gt;tail("statistics");
 242 }
 243 
 244 void Compile::print_statistics() {
 245   { ttyLocker ttyl;
 246     if (xtty != NULL)  xtty-&gt;head("statistics type='opto'");
 247     Parse::print_statistics();
 248     PhaseCCP::print_statistics();
 249     PhaseRegAlloc::print_statistics();
 250     Scheduling::print_statistics();
 251     PhasePeephole::print_statistics();
 252     PhaseIdealLoop::print_statistics();
 253     if (xtty != NULL)  xtty-&gt;tail("statistics");
 254   }
 255   if (_intrinsic_hist_flags[vmIntrinsics::_none] != 0) {
 256     // put this under its own &lt;statistics&gt; element.
 257     print_intrinsic_statistics();
 258   }
 259 }
 260 #endif //PRODUCT
 261 
 262 // Support for bundling info
 263 Bundle* Compile::node_bundling(const Node *n) {
 264   assert(valid_bundle_info(n), "oob");
 265   return &amp;_node_bundling_base[n-&gt;_idx];
 266 }
 267 
 268 bool Compile::valid_bundle_info(const Node *n) {
 269   return (_node_bundling_limit &gt; n-&gt;_idx);
 270 }
 271 
 272 
 273 void Compile::gvn_replace_by(Node* n, Node* nn) {
 274   for (DUIterator_Last imin, i = n-&gt;last_outs(imin); i &gt;= imin; ) {
 275     Node* use = n-&gt;last_out(i);
 276     bool is_in_table = initial_gvn()-&gt;hash_delete(use);
 277     uint uses_found = 0;
 278     for (uint j = 0; j &lt; use-&gt;len(); j++) {
 279       if (use-&gt;in(j) == n) {
 280         if (j &lt; use-&gt;req())
 281           use-&gt;set_req(j, nn);
 282         else
 283           use-&gt;set_prec(j, nn);
 284         uses_found++;
 285       }
 286     }
 287     if (is_in_table) {
 288       // reinsert into table
 289       initial_gvn()-&gt;hash_find_insert(use);
 290     }
 291     record_for_igvn(use);
 292     i -= uses_found;    // we deleted 1 or more copies of this edge
 293   }
 294 }
 295 
 296 
 297 static inline bool not_a_node(const Node* n) {
 298   if (n == NULL)                   return true;
 299   if (((intptr_t)n &amp; 1) != 0)      return true;  // uninitialized, etc.
 300   if (*(address*)n == badAddress)  return true;  // kill by Node::destruct
 301   return false;
 302 }
 303 
 304 // Identify all nodes that are reachable from below, useful.
 305 // Use breadth-first pass that records state in a Unique_Node_List,
 306 // recursive traversal is slower.
 307 void Compile::identify_useful_nodes(Unique_Node_List &amp;useful) {
 308   int estimated_worklist_size = live_nodes();
 309   useful.map( estimated_worklist_size, NULL );  // preallocate space
 310 
 311   // Initialize worklist
 312   if (root() != NULL)     { useful.push(root()); }
 313   // If 'top' is cached, declare it useful to preserve cached node
 314   if( cached_top_node() ) { useful.push(cached_top_node()); }
 315 
 316   // Push all useful nodes onto the list, breadthfirst
 317   for( uint next = 0; next &lt; useful.size(); ++next ) {
 318     assert( next &lt; unique(), "Unique useful nodes &lt; total nodes");
 319     Node *n  = useful.at(next);
 320     uint max = n-&gt;len();
 321     for( uint i = 0; i &lt; max; ++i ) {
 322       Node *m = n-&gt;in(i);
 323       if (not_a_node(m))  continue;
 324       useful.push(m);
 325     }
 326   }
 327 }
 328 
 329 // Update dead_node_list with any missing dead nodes using useful
 330 // list. Consider all non-useful nodes to be useless i.e., dead nodes.
 331 void Compile::update_dead_node_list(Unique_Node_List &amp;useful) {
 332   uint max_idx = unique();
 333   VectorSet&amp; useful_node_set = useful.member_set();
 334 
 335   for (uint node_idx = 0; node_idx &lt; max_idx; node_idx++) {
 336     // If node with index node_idx is not in useful set,
 337     // mark it as dead in dead node list.
 338     if (! useful_node_set.test(node_idx) ) {
 339       record_dead_node(node_idx);
 340     }
 341   }
 342 }
 343 
 344 void Compile::remove_useless_late_inlines(GrowableArray&lt;CallGenerator*&gt;* inlines, Unique_Node_List &amp;useful) {
 345   int shift = 0;
 346   for (int i = 0; i &lt; inlines-&gt;length(); i++) {
 347     CallGenerator* cg = inlines-&gt;at(i);
 348     CallNode* call = cg-&gt;call_node();
 349     if (shift &gt; 0) {
 350       inlines-&gt;at_put(i-shift, cg);
 351     }
 352     if (!useful.member(call)) {
 353       shift++;
 354     }
 355   }
 356   inlines-&gt;trunc_to(inlines-&gt;length()-shift);
 357 }
 358 
 359 // Disconnect all useless nodes by disconnecting those at the boundary.
 360 void Compile::remove_useless_nodes(Unique_Node_List &amp;useful) {
 361   uint next = 0;
 362   while (next &lt; useful.size()) {
 363     Node *n = useful.at(next++);
 364     if (n-&gt;is_SafePoint()) {
 365       // We're done with a parsing phase. Replaced nodes are not valid
 366       // beyond that point.
 367       n-&gt;as_SafePoint()-&gt;delete_replaced_nodes();
 368     }
 369     // Use raw traversal of out edges since this code removes out edges
 370     int max = n-&gt;outcnt();
 371     for (int j = 0; j &lt; max; ++j) {
 372       Node* child = n-&gt;raw_out(j);
 373       if (! useful.member(child)) {
 374         assert(!child-&gt;is_top() || child != top(),
 375                "If top is cached in Compile object it is in useful list");
 376         // Only need to remove this out-edge to the useless node
 377         n-&gt;raw_del_out(j);
 378         --j;
 379         --max;
 380       }
 381     }
 382     if (n-&gt;outcnt() == 1 &amp;&amp; n-&gt;has_special_unique_user()) {
 383       record_for_igvn(n-&gt;unique_out());
 384     }
 385   }
 386   // Remove useless macro and predicate opaq nodes
 387   for (int i = C-&gt;macro_count()-1; i &gt;= 0; i--) {
 388     Node* n = C-&gt;macro_node(i);
 389     if (!useful.member(n)) {
 390       remove_macro_node(n);
 391     }
 392   }
 393   // Remove useless CastII nodes with range check dependency
 394   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 395     Node* cast = range_check_cast_node(i);
 396     if (!useful.member(cast)) {
 397       remove_range_check_cast(cast);
 398     }
 399   }
 400   // Remove useless expensive node
 401   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 402     Node* n = C-&gt;expensive_node(i);
 403     if (!useful.member(n)) {
 404       remove_expensive_node(n);
 405     }
 406   }
 407   // clean up the late inline lists
 408   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 409   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 410   remove_useless_late_inlines(&amp;_late_inlines, useful);
 411   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 412 }
 413 
 414 //------------------------------frame_size_in_words-----------------------------
 415 // frame_slots in units of words
 416 int Compile::frame_size_in_words() const {
 417   // shift is 0 in LP32 and 1 in LP64
 418   const int shift = (LogBytesPerWord - LogBytesPerInt);
 419   int words = _frame_slots &gt;&gt; shift;
 420   assert( words &lt;&lt; shift == _frame_slots, "frame size must be properly aligned in LP64" );
 421   return words;
 422 }
 423 
 424 // To bang the stack of this compiled method we use the stack size
 425 // that the interpreter would need in case of a deoptimization. This
 426 // removes the need to bang the stack in the deoptimization blob which
 427 // in turn simplifies stack overflow handling.
 428 int Compile::bang_size_in_bytes() const {
 429   return MAX2(frame_size_in_bytes() + os::extra_bang_size_in_bytes(), _interpreter_frame_size);
 430 }
 431 
 432 // ============================================================================
 433 //------------------------------CompileWrapper---------------------------------
 434 class CompileWrapper : public StackObj {
 435   Compile *const _compile;
 436  public:
 437   CompileWrapper(Compile* compile);
 438 
 439   ~CompileWrapper();
 440 };
 441 
 442 CompileWrapper::CompileWrapper(Compile* compile) : _compile(compile) {
 443   // the Compile* pointer is stored in the current ciEnv:
 444   ciEnv* env = compile-&gt;env();
 445   assert(env == ciEnv::current(), "must already be a ciEnv active");
 446   assert(env-&gt;compiler_data() == NULL, "compile already active?");
 447   env-&gt;set_compiler_data(compile);
 448   assert(compile == Compile::current(), "sanity");
 449 
 450   compile-&gt;set_type_dict(NULL);
 451   compile-&gt;set_clone_map(new Dict(cmpkey, hashkey, _compile-&gt;comp_arena()));
 452   compile-&gt;clone_map().set_clone_idx(0);
 453   compile-&gt;set_type_hwm(NULL);
 454   compile-&gt;set_type_last_size(0);
 455   compile-&gt;set_last_tf(NULL, NULL);
 456   compile-&gt;set_indexSet_arena(NULL);
 457   compile-&gt;set_indexSet_free_block_list(NULL);
 458   compile-&gt;init_type_arena();
 459   Type::Initialize(compile);
 460   _compile-&gt;set_scratch_buffer_blob(NULL);
 461   _compile-&gt;begin_method();
 462   _compile-&gt;clone_map().set_debug(_compile-&gt;has_method() &amp;&amp; _compile-&gt;directive()-&gt;CloneMapDebugOption);
 463 }
 464 CompileWrapper::~CompileWrapper() {
 465   _compile-&gt;end_method();
 466   if (_compile-&gt;scratch_buffer_blob() != NULL)
 467     BufferBlob::free(_compile-&gt;scratch_buffer_blob());
 468   _compile-&gt;env()-&gt;set_compiler_data(NULL);
 469 }
 470 
 471 
 472 //----------------------------print_compile_messages---------------------------
 473 void Compile::print_compile_messages() {
 474 #ifndef PRODUCT
 475   // Check if recompiling
 476   if (_subsume_loads == false &amp;&amp; PrintOpto) {
 477     // Recompiling without allowing machine instructions to subsume loads
 478     tty-&gt;print_cr("*********************************************************");
 479     tty-&gt;print_cr("** Bailout: Recompile without subsuming loads          **");
 480     tty-&gt;print_cr("*********************************************************");
 481   }
 482   if (_do_escape_analysis != DoEscapeAnalysis &amp;&amp; PrintOpto) {
 483     // Recompiling without escape analysis
 484     tty-&gt;print_cr("*********************************************************");
 485     tty-&gt;print_cr("** Bailout: Recompile without escape analysis          **");
 486     tty-&gt;print_cr("*********************************************************");
 487   }
 488   if (_eliminate_boxing != EliminateAutoBox &amp;&amp; PrintOpto) {
 489     // Recompiling without boxing elimination
 490     tty-&gt;print_cr("*********************************************************");
 491     tty-&gt;print_cr("** Bailout: Recompile without boxing elimination       **");
 492     tty-&gt;print_cr("*********************************************************");
 493   }
 494   if (C-&gt;directive()-&gt;BreakAtCompileOption) {
 495     // Open the debugger when compiling this method.
 496     tty-&gt;print("### Breaking when compiling: ");
 497     method()-&gt;print_short_name();
 498     tty-&gt;cr();
 499     BREAKPOINT;
 500   }
 501 
 502   if( PrintOpto ) {
 503     if (is_osr_compilation()) {
 504       tty-&gt;print("[OSR]%3d", _compile_id);
 505     } else {
 506       tty-&gt;print("%3d", _compile_id);
 507     }
 508   }
 509 #endif
 510 }
 511 
 512 
 513 //-----------------------init_scratch_buffer_blob------------------------------
 514 // Construct a temporary BufferBlob and cache it for this compile.
 515 void Compile::init_scratch_buffer_blob(int const_size) {
 516   // If there is already a scratch buffer blob allocated and the
 517   // constant section is big enough, use it.  Otherwise free the
 518   // current and allocate a new one.
 519   BufferBlob* blob = scratch_buffer_blob();
 520   if ((blob != NULL) &amp;&amp; (const_size &lt;= _scratch_const_size)) {
 521     // Use the current blob.
 522   } else {
 523     if (blob != NULL) {
 524       BufferBlob::free(blob);
 525     }
 526 
 527     ResourceMark rm;
 528     _scratch_const_size = const_size;
 529     int size = (MAX_inst_size + MAX_stubs_size + _scratch_const_size);
 530     blob = BufferBlob::create("Compile::scratch_buffer", size);
 531     // Record the buffer blob for next time.
 532     set_scratch_buffer_blob(blob);
 533     // Have we run out of code space?
 534     if (scratch_buffer_blob() == NULL) {
 535       // Let CompilerBroker disable further compilations.
 536       record_failure("Not enough space for scratch buffer in CodeCache");
 537       return;
 538     }
 539   }
 540 
 541   // Initialize the relocation buffers
 542   relocInfo* locs_buf = (relocInfo*) blob-&gt;content_end() - MAX_locs_size;
 543   set_scratch_locs_memory(locs_buf);
 544 }
 545 
 546 
 547 //-----------------------scratch_emit_size-------------------------------------
 548 // Helper function that computes size by emitting code
 549 uint Compile::scratch_emit_size(const Node* n) {
 550   // Start scratch_emit_size section.
 551   set_in_scratch_emit_size(true);
 552 
 553   // Emit into a trash buffer and count bytes emitted.
 554   // This is a pretty expensive way to compute a size,
 555   // but it works well enough if seldom used.
 556   // All common fixed-size instructions are given a size
 557   // method by the AD file.
 558   // Note that the scratch buffer blob and locs memory are
 559   // allocated at the beginning of the compile task, and
 560   // may be shared by several calls to scratch_emit_size.
 561   // The allocation of the scratch buffer blob is particularly
 562   // expensive, since it has to grab the code cache lock.
 563   BufferBlob* blob = this-&gt;scratch_buffer_blob();
 564   assert(blob != NULL, "Initialize BufferBlob at start");
 565   assert(blob-&gt;size() &gt; MAX_inst_size, "sanity");
 566   relocInfo* locs_buf = scratch_locs_memory();
 567   address blob_begin = blob-&gt;content_begin();
 568   address blob_end   = (address)locs_buf;
 569   assert(blob-&gt;content_contains(blob_end), "sanity");
 570   CodeBuffer buf(blob_begin, blob_end - blob_begin);
 571   buf.initialize_consts_size(_scratch_const_size);
 572   buf.initialize_stubs_size(MAX_stubs_size);
 573   assert(locs_buf != NULL, "sanity");
 574   int lsize = MAX_locs_size / 3;
 575   buf.consts()-&gt;initialize_shared_locs(&amp;locs_buf[lsize * 0], lsize);
 576   buf.insts()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 1], lsize);
 577   buf.stubs()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 2], lsize);
 578 
 579   // Do the emission.
 580 
 581   Label fakeL; // Fake label for branch instructions.
 582   Label*   saveL = NULL;
 583   uint save_bnum = 0;
 584   bool is_branch = n-&gt;is_MachBranch();
 585   if (is_branch) {
 586     MacroAssembler masm(&amp;buf);
 587     masm.bind(fakeL);
 588     n-&gt;as_MachBranch()-&gt;save_label(&amp;saveL, &amp;save_bnum);
 589     n-&gt;as_MachBranch()-&gt;label_set(&amp;fakeL, 0);
 590   }
 591   n-&gt;emit(buf, this-&gt;regalloc());
 592 
 593   // Emitting into the scratch buffer should not fail
 594   assert (!failing(), "Must not have pending failure. Reason is: %s", failure_reason());
 595 
 596   if (is_branch) // Restore label.
 597     n-&gt;as_MachBranch()-&gt;label_set(saveL, save_bnum);
 598 
 599   // End scratch_emit_size section.
 600   set_in_scratch_emit_size(false);
 601 
 602   return buf.insts_size();
 603 }
 604 
 605 
 606 // ============================================================================
 607 //------------------------------Compile standard-------------------------------
 608 debug_only( int Compile::_debug_idx = 100000; )
 609 
 610 // Compile a method.  entry_bci is -1 for normal compilations and indicates
 611 // the continuation bci for on stack replacement.
 612 
 613 
 614 Compile::Compile( ciEnv* ci_env, C2Compiler* compiler, ciMethod* target, int osr_bci,
 615                   bool subsume_loads, bool do_escape_analysis, bool eliminate_boxing, DirectiveSet* directive)
 616                 : Phase(Compiler),
 617                   _env(ci_env),
 618                   _directive(directive),
 619                   _log(ci_env-&gt;log()),
 620                   _compile_id(ci_env-&gt;compile_id()),
 621                   _save_argument_registers(false),
 622                   _stub_name(NULL),
 623                   _stub_function(NULL),
 624                   _stub_entry_point(NULL),
 625                   _method(target),
 626                   _entry_bci(osr_bci),
 627                   _initial_gvn(NULL),
 628                   _for_igvn(NULL),
 629                   _warm_calls(NULL),
 630                   _subsume_loads(subsume_loads),
 631                   _do_escape_analysis(do_escape_analysis),
 632                   _eliminate_boxing(eliminate_boxing),
 633                   _failure_reason(NULL),
 634                   _code_buffer("Compile::Fill_buffer"),
 635                   _orig_pc_slot(0),
 636                   _orig_pc_slot_offset_in_bytes(0),
 637                   _has_method_handle_invokes(false),
 638                   _mach_constant_base_node(NULL),
 639                   _node_bundling_limit(0),
 640                   _node_bundling_base(NULL),
 641                   _java_calls(0),
 642                   _inner_loops(0),
 643                   _scratch_const_size(-1),
 644                   _in_scratch_emit_size(false),
 645                   _dead_node_list(comp_arena()),
 646                   _dead_node_count(0),
 647 #ifndef PRODUCT
 648                   _trace_opto_output(directive-&gt;TraceOptoOutputOption),
 649                   _in_dump_cnt(0),
 650                   _printer(IdealGraphPrinter::printer()),
 651 #endif
 652                   _congraph(NULL),
 653                   _comp_arena(mtCompiler),
 654                   _node_arena(mtCompiler),
 655                   _old_arena(mtCompiler),
 656                   _Compile_types(mtCompiler),
 657                   _replay_inline_data(NULL),
 658                   _late_inlines(comp_arena(), 2, 0, NULL),
 659                   _string_late_inlines(comp_arena(), 2, 0, NULL),
 660                   _boxing_late_inlines(comp_arena(), 2, 0, NULL),
 661                   _late_inlines_pos(0),
 662                   _number_of_mh_late_inlines(0),
 663                   _inlining_progress(false),
 664                   _inlining_incrementally(false),
 665                   _print_inlining_list(NULL),
 666                   _print_inlining_stream(NULL),
 667                   _print_inlining_idx(0),
 668                   _print_inlining_output(NULL),
 669                   _interpreter_frame_size(0),
 670                   _max_node_limit(MaxNodeLimit),
 671                   _has_reserved_stack_access(target-&gt;has_reserved_stack_access()) {
 672   C = this;
 673 #ifndef PRODUCT
 674   if (_printer != NULL) {
 675     _printer-&gt;set_compile(this);
 676   }
 677 #endif
 678   CompileWrapper cw(this);
 679 
 680   if (CITimeVerbose) {
 681     tty-&gt;print(" ");
 682     target-&gt;holder()-&gt;name()-&gt;print();
 683     tty-&gt;print(".");
 684     target-&gt;print_short_name();
 685     tty-&gt;print("  ");
 686   }
 687   TraceTime t1("Total compilation time", &amp;_t_totalCompilation, CITime, CITimeVerbose);
 688   TraceTime t2(NULL, &amp;_t_methodCompilation, CITime, false);
 689 
 690 #ifndef PRODUCT
 691   bool print_opto_assembly = directive-&gt;PrintOptoAssemblyOption;
 692   if (!print_opto_assembly) {
 693     bool print_assembly = directive-&gt;PrintAssemblyOption;
 694     if (print_assembly &amp;&amp; !Disassembler::can_decode()) {
 695       tty-&gt;print_cr("PrintAssembly request changed to PrintOptoAssembly");
 696       print_opto_assembly = true;
 697     }
 698   }
 699   set_print_assembly(print_opto_assembly);
 700   set_parsed_irreducible_loop(false);
 701 
 702   if (directive-&gt;ReplayInlineOption) {
 703     _replay_inline_data = ciReplay::load_inline_data(method(), entry_bci(), ci_env-&gt;comp_level());
 704   }
 705 #endif
 706   set_print_inlining(directive-&gt;PrintInliningOption || PrintOptoInlining);
 707   set_print_intrinsics(directive-&gt;PrintIntrinsicsOption);
 708   set_has_irreducible_loop(true); // conservative until build_loop_tree() reset it
 709 
 710   if (ProfileTraps RTM_OPT_ONLY( || UseRTMLocking )) {
 711     // Make sure the method being compiled gets its own MDO,
 712     // so we can at least track the decompile_count().
 713     // Need MDO to record RTM code generation state.
 714     method()-&gt;ensure_method_data();
 715   }
 716 
 717   Init(::AliasLevel);
 718 
 719 
 720   print_compile_messages();
 721 
 722   _ilt = InlineTree::build_inline_tree_root();
 723 
 724   // Even if NO memory addresses are used, MergeMem nodes must have at least 1 slice
 725   assert(num_alias_types() &gt;= AliasIdxRaw, "");
 726 
 727 #define MINIMUM_NODE_HASH  1023
 728   // Node list that Iterative GVN will start with
 729   Unique_Node_List for_igvn(comp_arena());
 730   set_for_igvn(&amp;for_igvn);
 731 
 732   // GVN that will be run immediately on new nodes
 733   uint estimated_size = method()-&gt;code_size()*4+64;
 734   estimated_size = (estimated_size &lt; MINIMUM_NODE_HASH ? MINIMUM_NODE_HASH : estimated_size);
 735   PhaseGVN gvn(node_arena(), estimated_size);
 736   set_initial_gvn(&amp;gvn);
 737 
 738   print_inlining_init();
 739   { // Scope for timing the parser
 740     TracePhase tp("parse", &amp;timers[_t_parser]);
 741 
 742     // Put top into the hash table ASAP.
 743     initial_gvn()-&gt;transform_no_reclaim(top());
 744 
 745     // Set up tf(), start(), and find a CallGenerator.
 746     CallGenerator* cg = NULL;
 747     if (is_osr_compilation()) {
 748       const TypeTuple *domain = StartOSRNode::osr_domain();
 749       const TypeTuple *range = TypeTuple::make_range(method()-&gt;signature());
 750       init_tf(TypeFunc::make(domain, range));
 751       StartNode* s = new StartOSRNode(root(), domain);
 752       initial_gvn()-&gt;set_type_bottom(s);
 753       init_start(s);
 754       cg = CallGenerator::for_osr(method(), entry_bci());
 755     } else {
 756       // Normal case.
 757       init_tf(TypeFunc::make(method()));
 758       StartNode* s = new StartNode(root(), tf()-&gt;domain());
 759       initial_gvn()-&gt;set_type_bottom(s);
 760       init_start(s);
 761       if (method()-&gt;intrinsic_id() == vmIntrinsics::_Reference_get &amp;&amp; UseG1GC) {
 762         // With java.lang.ref.reference.get() we must go through the
 763         // intrinsic when G1 is enabled - even when get() is the root
 764         // method of the compile - so that, if necessary, the value in
 765         // the referent field of the reference object gets recorded by
 766         // the pre-barrier code.
 767         // Specifically, if G1 is enabled, the value in the referent
 768         // field is recorded by the G1 SATB pre barrier. This will
 769         // result in the referent being marked live and the reference
 770         // object removed from the list of discovered references during
 771         // reference processing.
 772         cg = find_intrinsic(method(), false);
 773       }
 774       if (cg == NULL) {
 775         float past_uses = method()-&gt;interpreter_invocation_count();
 776         float expected_uses = past_uses;
 777         cg = CallGenerator::for_inline(method(), expected_uses);
 778       }
 779     }
 780     if (failing())  return;
 781     if (cg == NULL) {
 782       record_method_not_compilable_all_tiers("cannot parse method");
 783       return;
 784     }
 785     JVMState* jvms = build_start_state(start(), tf());
 786     if ((jvms = cg-&gt;generate(jvms)) == NULL) {
 787       if (!failure_reason_is(C2Compiler::retry_class_loading_during_parsing())) {
 788         record_method_not_compilable("method parse failed");
 789       }
 790       return;
 791     }
 792     GraphKit kit(jvms);
 793 
 794     if (!kit.stopped()) {
 795       // Accept return values, and transfer control we know not where.
 796       // This is done by a special, unique ReturnNode bound to root.
 797       return_values(kit.jvms());
 798     }
 799 
 800     if (kit.has_exceptions()) {
 801       // Any exceptions that escape from this call must be rethrown
 802       // to whatever caller is dynamically above us on the stack.
 803       // This is done by a special, unique RethrowNode bound to root.
 804       rethrow_exceptions(kit.transfer_exceptions_into_jvms());
 805     }
 806 
 807     assert(IncrementalInline || (_late_inlines.length() == 0 &amp;&amp; !has_mh_late_inlines()), "incremental inlining is off");
 808 
 809     if (_late_inlines.length() == 0 &amp;&amp; !has_mh_late_inlines() &amp;&amp; !failing() &amp;&amp; has_stringbuilder()) {
 810       inline_string_calls(true);
 811     }
 812 
 813     if (failing())  return;
 814 
 815     print_method(PHASE_BEFORE_REMOVEUSELESS, 3);
 816 
 817     // Remove clutter produced by parsing.
 818     if (!failing()) {
 819       ResourceMark rm;
 820       PhaseRemoveUseless pru(initial_gvn(), &amp;for_igvn);
 821     }
 822   }
 823 
 824   // Note:  Large methods are capped off in do_one_bytecode().
 825   if (failing())  return;
 826 
 827   // After parsing, node notes are no longer automagic.
 828   // They must be propagated by register_new_node_with_optimizer(),
 829   // clone(), or the like.
 830   set_default_node_notes(NULL);
 831 
 832   for (;;) {
 833     int successes = Inline_Warm();
 834     if (failing())  return;
 835     if (successes == 0)  break;
 836   }
 837 
 838   // Drain the list.
 839   Finish_Warm();
 840 #ifndef PRODUCT
 841   if (_printer &amp;&amp; _printer-&gt;should_print(1)) {
 842     _printer-&gt;print_inlining();
 843   }
 844 #endif
 845 
 846   if (failing())  return;
 847   NOT_PRODUCT( verify_graph_edges(); )
 848 
 849   // Now optimize
 850   Optimize();
 851   if (failing())  return;
 852   NOT_PRODUCT( verify_graph_edges(); )
 853 
 854 #ifndef PRODUCT
 855   if (PrintIdeal) {
 856     ttyLocker ttyl;  // keep the following output all in one block
 857     // This output goes directly to the tty, not the compiler log.
 858     // To enable tools to match it up with the compilation activity,
 859     // be sure to tag this tty output with the compile ID.
 860     if (xtty != NULL) {
 861       xtty-&gt;head("ideal compile_id='%d'%s", compile_id(),
 862                  is_osr_compilation()    ? " compile_kind='osr'" :
 863                  "");
 864     }
 865     root()-&gt;dump(9999);
 866     if (xtty != NULL) {
 867       xtty-&gt;tail("ideal");
 868     }
 869   }
 870 #endif
 871 
 872   NOT_PRODUCT( verify_barriers(); )
 873 
 874   // Dump compilation data to replay it.
 875   if (directive-&gt;DumpReplayOption) {
 876     env()-&gt;dump_replay_data(_compile_id);
 877   }
 878   if (directive-&gt;DumpInlineOption &amp;&amp; (ilt() != NULL)) {
 879     env()-&gt;dump_inline_data(_compile_id);
 880   }
 881   // Dump profile to allow profile caching
 882   if(env()-&gt;comp_level()&gt;CompLevel_limited_profile &amp;&amp; env()-&gt;comp_level() &gt;= DumpProfilesMinTier) {
 883     if ((DumpProfiles || method()-&gt;has_option("DumpProfile")) &amp;&amp; (!method()-&gt;has_option("IgnoreDumpProfile"))) {
 884                 //tty-&gt;print("###Dump: %s\n",method()-&gt;holder()-&gt;name()-&gt;as_utf8());
 885                 _env-&gt;dump_cache_profiles(0, method()-&gt;name()-&gt;as_utf8());
 886     }
 887   }
 888 
 889   // Now that we know the size of all the monitors we can add a fixed slot
 890   // for the original deopt pc.
 891 
 892   _orig_pc_slot =  fixed_slots();
 893   int next_slot = _orig_pc_slot + (sizeof(address) / VMRegImpl::stack_slot_size);
 894   set_fixed_slots(next_slot);
 895 
 896   // Compute when to use implicit null checks. Used by matching trap based
 897   // nodes and NullCheck optimization.
 898   set_allowed_deopt_reasons();
 899 
 900   // Now generate code
 901   Code_Gen();
 902   if (failing())  return;
 903 
 904   // Check if we want to skip execution of all compiled code.
 905   {
 906 #ifndef PRODUCT
 907     if (OptoNoExecute) {
 908       record_method_not_compilable("+OptoNoExecute");  // Flag as failed
 909       return;
 910     }
 911 #endif
 912     TracePhase tp("install_code", &amp;timers[_t_registerMethod]);
 913 
 914     if (is_osr_compilation()) {
 915       _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);
 916       _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);
 917     } else {
 918       _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);
 919       _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);
 920     }
 921 
 922     env()-&gt;register_method(_method, _entry_bci,
 923                            &amp;_code_offsets,
 924                            _orig_pc_slot_offset_in_bytes,
 925                            code_buffer(),
 926                            frame_size_in_words(), _oop_map_set,
 927                            &amp;_handler_table, &amp;_inc_table,
 928                            compiler,
 929                            has_unsafe_access(),
 930                            SharedRuntime::is_wide_vector(max_vector_size()),
 931                            rtm_state()
 932                            );
 933 
 934     if (log() != NULL) // Print code cache state into compiler log
 935       log()-&gt;code_cache_state();
 936   }
 937 }
 938 
 939 //------------------------------Compile----------------------------------------
 940 // Compile a runtime stub
 941 Compile::Compile( ciEnv* ci_env,
 942                   TypeFunc_generator generator,
 943                   address stub_function,
 944                   const char *stub_name,
 945                   int is_fancy_jump,
 946                   bool pass_tls,
 947                   bool save_arg_registers,
 948                   bool return_pc,
 949                   DirectiveSet* directive)
 950   : Phase(Compiler),
 951     _env(ci_env),
 952     _directive(directive),
 953     _log(ci_env-&gt;log()),
 954     _compile_id(0),
 955     _save_argument_registers(save_arg_registers),
 956     _method(NULL),
 957     _stub_name(stub_name),
 958     _stub_function(stub_function),
 959     _stub_entry_point(NULL),
 960     _entry_bci(InvocationEntryBci),
 961     _initial_gvn(NULL),
 962     _for_igvn(NULL),
 963     _warm_calls(NULL),
 964     _orig_pc_slot(0),
 965     _orig_pc_slot_offset_in_bytes(0),
 966     _subsume_loads(true),
 967     _do_escape_analysis(false),
 968     _eliminate_boxing(false),
 969     _failure_reason(NULL),
 970     _code_buffer("Compile::Fill_buffer"),
 971     _has_method_handle_invokes(false),
 972     _mach_constant_base_node(NULL),
 973     _node_bundling_limit(0),
 974     _node_bundling_base(NULL),
 975     _java_calls(0),
 976     _inner_loops(0),
 977 #ifndef PRODUCT
 978     _trace_opto_output(TraceOptoOutput),
 979     _in_dump_cnt(0),
 980     _printer(NULL),
 981 #endif
 982     _comp_arena(mtCompiler),
 983     _node_arena(mtCompiler),
 984     _old_arena(mtCompiler),
 985     _Compile_types(mtCompiler),
 986     _dead_node_list(comp_arena()),
 987     _dead_node_count(0),
 988     _congraph(NULL),
 989     _replay_inline_data(NULL),
 990     _number_of_mh_late_inlines(0),
 991     _inlining_progress(false),
 992     _inlining_incrementally(false),
 993     _print_inlining_list(NULL),
 994     _print_inlining_stream(NULL),
 995     _print_inlining_idx(0),
 996     _print_inlining_output(NULL),
 997     _allowed_reasons(0),
 998     _interpreter_frame_size(0),
 999     _max_node_limit(MaxNodeLimit) {
1000   C = this;
1001 
1002   TraceTime t1(NULL, &amp;_t_totalCompilation, CITime, false);
1003   TraceTime t2(NULL, &amp;_t_stubCompilation, CITime, false);
1004 
1005 #ifndef PRODUCT
1006   set_print_assembly(PrintFrameConverterAssembly);
1007   set_parsed_irreducible_loop(false);
1008 #endif
1009   set_has_irreducible_loop(false); // no loops
1010 
1011   CompileWrapper cw(this);
1012   Init(/*AliasLevel=*/ 0);
1013   init_tf((*generator)());
1014 
1015   {
1016     // The following is a dummy for the sake of GraphKit::gen_stub
1017     Unique_Node_List for_igvn(comp_arena());
1018     set_for_igvn(&amp;for_igvn);  // not used, but some GraphKit guys push on this
1019     PhaseGVN gvn(Thread::current()-&gt;resource_area(),255);
1020     set_initial_gvn(&amp;gvn);    // not significant, but GraphKit guys use it pervasively
1021     gvn.transform_no_reclaim(top());
1022 
1023     GraphKit kit;
1024     kit.gen_stub(stub_function, stub_name, is_fancy_jump, pass_tls, return_pc);
1025   }
1026 
1027   NOT_PRODUCT( verify_graph_edges(); )
1028   Code_Gen();
1029   if (failing())  return;
1030 
1031 
1032   // Entry point will be accessed using compile-&gt;stub_entry_point();
1033   if (code_buffer() == NULL) {
1034     Matcher::soft_match_failure();
1035   } else {
1036     if (PrintAssembly &amp;&amp; (WizardMode || Verbose))
1037       tty-&gt;print_cr("### Stub::%s", stub_name);
1038 
1039     if (!failing()) {
1040       assert(_fixed_slots == 0, "no fixed slots used for runtime stubs");
1041 
1042       // Make the NMethod
1043       // For now we mark the frame as never safe for profile stackwalking
1044       RuntimeStub *rs = RuntimeStub::new_runtime_stub(stub_name,
1045                                                       code_buffer(),
1046                                                       CodeOffsets::frame_never_safe,
1047                                                       // _code_offsets.value(CodeOffsets::Frame_Complete),
1048                                                       frame_size_in_words(),
1049                                                       _oop_map_set,
1050                                                       save_arg_registers);
1051       assert(rs != NULL &amp;&amp; rs-&gt;is_runtime_stub(), "sanity check");
1052 
1053       _stub_entry_point = rs-&gt;entry_point();
1054     }
1055   }
1056 }
1057 
1058 //------------------------------Init-------------------------------------------
1059 // Prepare for a single compilation
1060 void Compile::Init(int aliaslevel) {
1061   _unique  = 0;
1062   _regalloc = NULL;
1063 
1064   _tf      = NULL;  // filled in later
1065   _top     = NULL;  // cached later
1066   _matcher = NULL;  // filled in later
1067   _cfg     = NULL;  // filled in later
1068 
1069   set_24_bit_selection_and_mode(Use24BitFP, false);
1070 
1071   _node_note_array = NULL;
1072   _default_node_notes = NULL;
1073   DEBUG_ONLY( _modified_nodes = NULL; ) // Used in Optimize()
1074 
1075   _immutable_memory = NULL; // filled in at first inquiry
1076 
1077   // Globally visible Nodes
1078   // First set TOP to NULL to give safe behavior during creation of RootNode
1079   set_cached_top_node(NULL);
1080   set_root(new RootNode());
1081   // Now that you have a Root to point to, create the real TOP
1082   set_cached_top_node( new ConNode(Type::TOP) );
1083   set_recent_alloc(NULL, NULL);
1084 
1085   // Create Debug Information Recorder to record scopes, oopmaps, etc.
1086   env()-&gt;set_oop_recorder(new OopRecorder(env()-&gt;arena()));
1087   env()-&gt;set_debug_info(new DebugInformationRecorder(env()-&gt;oop_recorder()));
1088   env()-&gt;set_dependencies(new Dependencies(env()));
1089 
1090   _fixed_slots = 0;
1091   set_has_split_ifs(false);
1092   set_has_loops(has_method() &amp;&amp; method()-&gt;has_loops()); // first approximation
1093   set_has_stringbuilder(false);
1094   set_has_boxed_value(false);
1095   _trap_can_recompile = false;  // no traps emitted yet
1096   _major_progress = true; // start out assuming good things will happen
1097   set_has_unsafe_access(false);
1098   set_max_vector_size(0);
1099   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
1100   set_decompile_count(0);
1101 
1102   set_do_freq_based_layout(_directive-&gt;BlockLayoutByFrequencyOption);
1103   set_num_loop_opts(LoopOptsCount);
1104   set_do_inlining(Inline);
1105   set_max_inline_size(MaxInlineSize);
1106   set_freq_inline_size(FreqInlineSize);
1107   set_do_scheduling(OptoScheduling);
1108   set_do_count_invocations(false);
1109   set_do_method_data_update(false);
1110 
1111   set_do_vector_loop(false);
1112 
1113   if (AllowVectorizeOnDemand) {
1114     if (has_method() &amp;&amp; (_directive-&gt;VectorizeOption || _directive-&gt;VectorizeDebugOption)) {
1115       set_do_vector_loop(true);
1116       NOT_PRODUCT(if (do_vector_loop() &amp;&amp; Verbose) {tty-&gt;print("Compile::Init: do vectorized loops (SIMD like) for method %s\n",  method()-&gt;name()-&gt;as_quoted_ascii());})
1117     } else if (has_method() &amp;&amp; method()-&gt;name() != 0 &amp;&amp;
1118                method()-&gt;intrinsic_id() == vmIntrinsics::_forEachRemaining) {
1119       set_do_vector_loop(true);
1120     }
1121   }
1122   set_use_cmove(UseCMoveUnconditionally /* || do_vector_loop()*/); //TODO: consider do_vector_loop() mandate use_cmove unconditionally
1123   NOT_PRODUCT(if (use_cmove() &amp;&amp; Verbose &amp;&amp; has_method()) {tty-&gt;print("Compile::Init: use CMove without profitability tests for method %s\n",  method()-&gt;name()-&gt;as_quoted_ascii());})
1124 
1125   set_age_code(has_method() &amp;&amp; method()-&gt;profile_aging());
1126   set_rtm_state(NoRTM); // No RTM lock eliding by default
1127   _max_node_limit = _directive-&gt;MaxNodeLimitOption;
1128 
1129 #if INCLUDE_RTM_OPT
1130   if (UseRTMLocking &amp;&amp; has_method() &amp;&amp; (method()-&gt;method_data_or_null() != NULL)) {
1131     int rtm_state = method()-&gt;method_data()-&gt;rtm_state();
1132     if (method_has_option("NoRTMLockEliding") || ((rtm_state &amp; NoRTM) != 0)) {
1133       // Don't generate RTM lock eliding code.
1134       set_rtm_state(NoRTM);
1135     } else if (method_has_option("UseRTMLockEliding") || ((rtm_state &amp; UseRTM) != 0) || !UseRTMDeopt) {
1136       // Generate RTM lock eliding code without abort ratio calculation code.
1137       set_rtm_state(UseRTM);
1138     } else if (UseRTMDeopt) {
1139       // Generate RTM lock eliding code and include abort ratio calculation
1140       // code if UseRTMDeopt is on.
1141       set_rtm_state(ProfileRTM);
1142     }
1143   }
1144 #endif
1145   if (debug_info()-&gt;recording_non_safepoints()) {
1146     set_node_note_array(new(comp_arena()) GrowableArray&lt;Node_Notes*&gt;
1147                         (comp_arena(), 8, 0, NULL));
1148     set_default_node_notes(Node_Notes::make(this));
1149   }
1150 
1151   // // -- Initialize types before each compile --
1152   // // Update cached type information
1153   // if( _method &amp;&amp; _method-&gt;constants() )
1154   //   Type::update_loaded_types(_method, _method-&gt;constants());
1155 
1156   // Init alias_type map.
1157   if (!_do_escape_analysis &amp;&amp; aliaslevel == 3)
1158     aliaslevel = 2;  // No unique types without escape analysis
1159   _AliasLevel = aliaslevel;
1160   const int grow_ats = 16;
1161   _max_alias_types = grow_ats;
1162   _alias_types   = NEW_ARENA_ARRAY(comp_arena(), AliasType*, grow_ats);
1163   AliasType* ats = NEW_ARENA_ARRAY(comp_arena(), AliasType,  grow_ats);
1164   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
1165   {
1166     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
1167   }
1168   // Initialize the first few types.
1169   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
1170   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1171   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1172   _num_alias_types = AliasIdxRaw+1;
1173   // Zero out the alias type cache.
1174   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1175   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1176   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1177 
1178   _intrinsics = NULL;
1179   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1180   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1181   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1182   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1183   register_library_intrinsics();
1184 }
1185 
1186 //---------------------------init_start----------------------------------------
1187 // Install the StartNode on this compile object.
1188 void Compile::init_start(StartNode* s) {
1189   if (failing())
1190     return; // already failing
1191   assert(s == start(), "");
1192 }
1193 
1194 /**
1195  * Return the 'StartNode'. We must not have a pending failure, since the ideal graph
1196  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1197  * the ideal graph.
1198  */
1199 StartNode* Compile::start() const {
1200   assert (!failing(), "Must not have pending failure. Reason is: %s", failure_reason());
1201   for (DUIterator_Fast imax, i = root()-&gt;fast_outs(imax); i &lt; imax; i++) {
1202     Node* start = root()-&gt;fast_out(i);
1203     if (start-&gt;is_Start()) {
1204       return start-&gt;as_Start();
1205     }
1206   }
1207   fatal("Did not find Start node!");
1208   return NULL;
1209 }
1210 
1211 //-------------------------------immutable_memory-------------------------------------
1212 // Access immutable memory
1213 Node* Compile::immutable_memory() {
1214   if (_immutable_memory != NULL) {
1215     return _immutable_memory;
1216   }
1217   StartNode* s = start();
1218   for (DUIterator_Fast imax, i = s-&gt;fast_outs(imax); true; i++) {
1219     Node *p = s-&gt;fast_out(i);
1220     if (p != s &amp;&amp; p-&gt;as_Proj()-&gt;_con == TypeFunc::Memory) {
1221       _immutable_memory = p;
1222       return _immutable_memory;
1223     }
1224   }
1225   ShouldNotReachHere();
1226   return NULL;
1227 }
1228 
1229 //----------------------set_cached_top_node------------------------------------
1230 // Install the cached top node, and make sure Node::is_top works correctly.
1231 void Compile::set_cached_top_node(Node* tn) {
1232   if (tn != NULL)  verify_top(tn);
1233   Node* old_top = _top;
1234   _top = tn;
1235   // Calling Node::setup_is_top allows the nodes the chance to adjust
1236   // their _out arrays.
1237   if (_top != NULL)     _top-&gt;setup_is_top();
1238   if (old_top != NULL)  old_top-&gt;setup_is_top();
1239   assert(_top == NULL || top()-&gt;is_top(), "");
1240 }
1241 
1242 #ifdef ASSERT
1243 uint Compile::count_live_nodes_by_graph_walk() {
1244   Unique_Node_List useful(comp_arena());
1245   // Get useful node list by walking the graph.
1246   identify_useful_nodes(useful);
1247   return useful.size();
1248 }
1249 
1250 void Compile::print_missing_nodes() {
1251 
1252   // Return if CompileLog is NULL and PrintIdealNodeCount is false.
1253   if ((_log == NULL) &amp;&amp; (! PrintIdealNodeCount)) {
1254     return;
1255   }
1256 
1257   // This is an expensive function. It is executed only when the user
1258   // specifies VerifyIdealNodeCount option or otherwise knows the
1259   // additional work that needs to be done to identify reachable nodes
1260   // by walking the flow graph and find the missing ones using
1261   // _dead_node_list.
1262 
1263   Unique_Node_List useful(comp_arena());
1264   // Get useful node list by walking the graph.
1265   identify_useful_nodes(useful);
1266 
1267   uint l_nodes = C-&gt;live_nodes();
1268   uint l_nodes_by_walk = useful.size();
1269 
1270   if (l_nodes != l_nodes_by_walk) {
1271     if (_log != NULL) {
1272       _log-&gt;begin_head("mismatched_nodes count='%d'", abs((int) (l_nodes - l_nodes_by_walk)));
1273       _log-&gt;stamp();
1274       _log-&gt;end_head();
1275     }
1276     VectorSet&amp; useful_member_set = useful.member_set();
1277     int last_idx = l_nodes_by_walk;
1278     for (int i = 0; i &lt; last_idx; i++) {
1279       if (useful_member_set.test(i)) {
1280         if (_dead_node_list.test(i)) {
1281           if (_log != NULL) {
1282             _log-&gt;elem("mismatched_node_info node_idx='%d' type='both live and dead'", i);
1283           }
1284           if (PrintIdealNodeCount) {
1285             // Print the log message to tty
1286               tty-&gt;print_cr("mismatched_node idx='%d' both live and dead'", i);
1287               useful.at(i)-&gt;dump();
1288           }
1289         }
1290       }
1291       else if (! _dead_node_list.test(i)) {
1292         if (_log != NULL) {
1293           _log-&gt;elem("mismatched_node_info node_idx='%d' type='neither live nor dead'", i);
1294         }
1295         if (PrintIdealNodeCount) {
1296           // Print the log message to tty
1297           tty-&gt;print_cr("mismatched_node idx='%d' type='neither live nor dead'", i);
1298         }
1299       }
1300     }
1301     if (_log != NULL) {
1302       _log-&gt;tail("mismatched_nodes");
1303     }
1304   }
1305 }
1306 void Compile::record_modified_node(Node* n) {
1307   if (_modified_nodes != NULL &amp;&amp; !_inlining_incrementally &amp;&amp;
1308       n-&gt;outcnt() != 0 &amp;&amp; !n-&gt;is_Con()) {
1309     _modified_nodes-&gt;push(n);
1310   }
1311 }
1312 
1313 void Compile::remove_modified_node(Node* n) {
1314   if (_modified_nodes != NULL) {
1315     _modified_nodes-&gt;remove(n);
1316   }
1317 }
1318 #endif
1319 
1320 #ifndef PRODUCT
1321 void Compile::verify_top(Node* tn) const {
1322   if (tn != NULL) {
1323     assert(tn-&gt;is_Con(), "top node must be a constant");
1324     assert(((ConNode*)tn)-&gt;type() == Type::TOP, "top node must have correct type");
1325     assert(tn-&gt;in(0) != NULL, "must have live top node");
1326   }
1327 }
1328 #endif
1329 
1330 
1331 ///-------------------Managing Per-Node Debug &amp; Profile Info-------------------
1332 
1333 void Compile::grow_node_notes(GrowableArray&lt;Node_Notes*&gt;* arr, int grow_by) {
1334   guarantee(arr != NULL, "");
1335   int num_blocks = arr-&gt;length();
1336   if (grow_by &lt; num_blocks)  grow_by = num_blocks;
1337   int num_notes = grow_by * _node_notes_block_size;
1338   Node_Notes* notes = NEW_ARENA_ARRAY(node_arena(), Node_Notes, num_notes);
1339   Copy::zero_to_bytes(notes, num_notes * sizeof(Node_Notes));
1340   while (num_notes &gt; 0) {
1341     arr-&gt;append(notes);
1342     notes     += _node_notes_block_size;
1343     num_notes -= _node_notes_block_size;
1344   }
1345   assert(num_notes == 0, "exact multiple, please");
1346 }
1347 
1348 bool Compile::copy_node_notes_to(Node* dest, Node* source) {
1349   if (source == NULL || dest == NULL)  return false;
1350 
1351   if (dest-&gt;is_Con())
1352     return false;               // Do not push debug info onto constants.
1353 
1354 #ifdef ASSERT
1355   // Leave a bread crumb trail pointing to the original node:
1356   if (dest != NULL &amp;&amp; dest != source &amp;&amp; dest-&gt;debug_orig() == NULL) {
1357     dest-&gt;set_debug_orig(source);
1358   }
1359 #endif
1360 
1361   if (node_note_array() == NULL)
1362     return false;               // Not collecting any notes now.
1363 
1364   // This is a copy onto a pre-existing node, which may already have notes.
1365   // If both nodes have notes, do not overwrite any pre-existing notes.
1366   Node_Notes* source_notes = node_notes_at(source-&gt;_idx);
1367   if (source_notes == NULL || source_notes-&gt;is_clear())  return false;
1368   Node_Notes* dest_notes   = node_notes_at(dest-&gt;_idx);
1369   if (dest_notes == NULL || dest_notes-&gt;is_clear()) {
1370     return set_node_notes_at(dest-&gt;_idx, source_notes);
1371   }
1372 
1373   Node_Notes merged_notes = (*source_notes);
1374   // The order of operations here ensures that dest notes will win...
1375   merged_notes.update_from(dest_notes);
1376   return set_node_notes_at(dest-&gt;_idx, &amp;merged_notes);
1377 }
1378 
1379 
1380 //--------------------------allow_range_check_smearing-------------------------
1381 // Gating condition for coalescing similar range checks.
1382 // Sometimes we try 'speculatively' replacing a series of a range checks by a
1383 // single covering check that is at least as strong as any of them.
1384 // If the optimization succeeds, the simplified (strengthened) range check
1385 // will always succeed.  If it fails, we will deopt, and then give up
1386 // on the optimization.
1387 bool Compile::allow_range_check_smearing() const {
1388   // If this method has already thrown a range-check,
1389   // assume it was because we already tried range smearing
1390   // and it failed.
1391   uint already_trapped = trap_count(Deoptimization::Reason_range_check);
1392   return !already_trapped;
1393 }
1394 
1395 
1396 //------------------------------flatten_alias_type-----------------------------
1397 const TypePtr *Compile::flatten_alias_type( const TypePtr *tj ) const {
1398   int offset = tj-&gt;offset();
1399   TypePtr::PTR ptr = tj-&gt;ptr();
1400 
1401   // Known instance (scalarizable allocation) alias only with itself.
1402   bool is_known_inst = tj-&gt;isa_oopptr() != NULL &amp;&amp;
1403                        tj-&gt;is_oopptr()-&gt;is_known_instance();
1404 
1405   // Process weird unsafe references.
1406   if (offset == Type::OffsetBot &amp;&amp; (tj-&gt;isa_instptr() /*|| tj-&gt;isa_klassptr()*/)) {
1407     assert(InlineUnsafeOps, "indeterminate pointers come only from unsafe ops");
1408     assert(!is_known_inst, "scalarizable allocation should not have unsafe references");
1409     tj = TypeOopPtr::BOTTOM;
1410     ptr = tj-&gt;ptr();
1411     offset = tj-&gt;offset();
1412   }
1413 
1414   // Array pointers need some flattening
1415   const TypeAryPtr *ta = tj-&gt;isa_aryptr();
1416   if (ta &amp;&amp; ta-&gt;is_stable()) {
1417     // Erase stability property for alias analysis.
1418     tj = ta = ta-&gt;cast_to_stable(false);
1419   }
1420   if( ta &amp;&amp; is_known_inst ) {
1421     if ( offset != Type::OffsetBot &amp;&amp;
1422          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1423       offset = Type::OffsetBot; // Flatten constant access into array body only
1424       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, offset, ta-&gt;instance_id());
1425     }
1426   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1427     // For arrays indexed by constant indices, we flatten the alias
1428     // space to include all of the array body.  Only the header, klass
1429     // and array length can be accessed un-aliased.
1430     if( offset != Type::OffsetBot ) {
1431       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1432         offset = Type::OffsetBot;   // Flatten constant access into array body
1433         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,offset);
1434       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1435         // range is OK as-is.
1436         tj = ta = TypeAryPtr::RANGE;
1437       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1438         tj = TypeInstPtr::KLASS; // all klass loads look alike
1439         ta = TypeAryPtr::RANGE; // generic ignored junk
1440         ptr = TypePtr::BotPTR;
1441       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1442         tj = TypeInstPtr::MARK;
1443         ta = TypeAryPtr::RANGE; // generic ignored junk
1444         ptr = TypePtr::BotPTR;
1445       } else {                  // Random constant offset into array body
1446         offset = Type::OffsetBot;   // Flatten constant access into array body
1447         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,offset);
1448       }
1449     }
1450     // Arrays of fixed size alias with arrays of unknown size.
1451     if (ta-&gt;size() != TypeInt::POS) {
1452       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
1453       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,offset);
1454     }
1455     // Arrays of known objects become arrays of unknown objects.
1456     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1457       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
1458       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);
1459     }
1460     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1461       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
1462       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);
1463     }
1464     // Arrays of bytes and of booleans both use 'bastore' and 'baload' so
1465     // cannot be distinguished by bytecode alone.
1466     if (ta-&gt;elem() == TypeInt::BOOL) {
1467       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1468       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
1469       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,offset);
1470     }
1471     // During the 2nd round of IterGVN, NotNull castings are removed.
1472     // Make sure the Bottom and NotNull variants alias the same.
1473     // Also, make sure exact and non-exact variants alias the same.
1474     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
1475       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,offset);
1476     }
1477   }
1478 
1479   // Oop pointers need some flattening
1480   const TypeInstPtr *to = tj-&gt;isa_instptr();
1481   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1482     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1483     if( ptr == TypePtr::Constant ) {
1484       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1485           offset &lt; k-&gt;size_helper() * wordSize) {
1486         // No constant oop pointers (such as Strings); they alias with
1487         // unknown strings.
1488         assert(!is_known_inst, "not scalarizable allocation");
1489         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);
1490       }
1491     } else if( is_known_inst ) {
1492       tj = to; // Keep NotNull and klass_is_exact for instance type
1493     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1494       // During the 2nd round of IterGVN, NotNull castings are removed.
1495       // Make sure the Bottom and NotNull variants alias the same.
1496       // Also, make sure exact and non-exact variants alias the same.
1497       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);
1498     }
1499     if (to-&gt;speculative() != NULL) {
1500       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),to-&gt;offset(), to-&gt;instance_id());
1501     }
1502     // Canonicalize the holder of this field
1503     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1504       // First handle header references such as a LoadKlassNode, even if the
1505       // object's klass is unloaded at compile time (4965979).
1506       if (!is_known_inst) { // Do it only for non-instance types
1507         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, offset);
1508       }
1509     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1510       // Static fields are in the space above the normal instance
1511       // fields in the java.lang.Class instance.
1512       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1513         to = NULL;
1514         tj = TypeOopPtr::BOTTOM;
1515         offset = tj-&gt;offset();
1516       }
1517     } else {
1518       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1519       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1520         if( is_known_inst ) {
1521           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, offset, to-&gt;instance_id());
1522         } else {
1523           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, offset);
1524         }
1525       }
1526     }
1527   }
1528 
1529   // Klass pointers to object array klasses need some flattening
1530   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1531   if( tk ) {
1532     // If we are referencing a field within a Klass, we need
1533     // to assume the worst case of an Object.  Both exact and
1534     // inexact types must flatten to the same alias class so
1535     // use NotNull as the PTR.
1536     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1537 
1538       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1539                                    TypeKlassPtr::OBJECT-&gt;klass(),
1540                                    offset);
1541     }
1542 
1543     ciKlass* klass = tk-&gt;klass();
1544     if( klass-&gt;is_obj_array_klass() ) {
1545       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1546       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1547         k = TypeInstPtr::BOTTOM-&gt;klass();
1548       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );
1549     }
1550 
1551     // Check for precise loads from the primary supertype array and force them
1552     // to the supertype cache alias index.  Check for generic array loads from
1553     // the primary supertype array and also force them to the supertype cache
1554     // alias index.  Since the same load can reach both, we need to merge
1555     // these 2 disparate memories into the same alias class.  Since the
1556     // primary supertype array is read-only, there's no chance of confusion
1557     // where we bypass an array load and an array store.
1558     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1559     if (offset == Type::OffsetBot ||
1560         (offset &gt;= primary_supers_offset &amp;&amp;
1561          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1562         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1563       offset = in_bytes(Klass::secondary_super_cache_offset());
1564       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk-&gt;klass(), offset );
1565     }
1566   }
1567 
1568   // Flatten all Raw pointers together.
1569   if (tj-&gt;base() == Type::RawPtr)
1570     tj = TypeRawPtr::BOTTOM;
1571 
1572   if (tj-&gt;base() == Type::AnyPtr)
1573     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1574 
1575   // Flatten all to bottom for now
1576   switch( _AliasLevel ) {
1577   case 0:
1578     tj = TypePtr::BOTTOM;
1579     break;
1580   case 1:                       // Flatten to: oop, static, field or array
1581     switch (tj-&gt;base()) {
1582     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1583     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1584     case Type::AryPtr:   // do not distinguish arrays at all
1585     case Type::InstPtr:  tj = TypeInstPtr::BOTTOM;  break;
1586     case Type::KlassPtr: tj = TypeKlassPtr::OBJECT; break;
1587     case Type::AnyPtr:   tj = TypePtr::BOTTOM;      break;  // caller checks it
1588     default: ShouldNotReachHere();
1589     }
1590     break;
1591   case 2:                       // No collapsing at level 2; keep all splits
1592   case 3:                       // No collapsing at level 3; keep all splits
1593     break;
1594   default:
1595     Unimplemented();
1596   }
1597 
1598   offset = tj-&gt;offset();
1599   assert( offset != Type::OffsetTop, "Offset has fallen from constant" );
1600 
1601   assert( (offset != Type::OffsetBot &amp;&amp; tj-&gt;base() != Type::AryPtr) ||
1602           (offset == Type::OffsetBot &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1603           (offset == Type::OffsetBot &amp;&amp; tj == TypeOopPtr::BOTTOM) ||
1604           (offset == Type::OffsetBot &amp;&amp; tj == TypePtr::BOTTOM) ||
1605           (offset == oopDesc::mark_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1606           (offset == oopDesc::klass_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1607           (offset == arrayOopDesc::length_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr)  ,
1608           "For oops, klasses, raw offset must be constant; for arrays the offset is never known" );
1609   assert( tj-&gt;ptr() != TypePtr::TopPTR &amp;&amp;
1610           tj-&gt;ptr() != TypePtr::AnyNull &amp;&amp;
1611           tj-&gt;ptr() != TypePtr::Null, "No imprecise addresses" );
1612 //    assert( tj-&gt;ptr() != TypePtr::Constant ||
1613 //            tj-&gt;base() == Type::RawPtr ||
1614 //            tj-&gt;base() == Type::KlassPtr, "No constant oop addresses" );
1615 
1616   return tj;
1617 }
1618 
1619 void Compile::AliasType::Init(int i, const TypePtr* at) {
1620   _index = i;
1621   _adr_type = at;
1622   _field = NULL;
1623   _element = NULL;
1624   _is_rewritable = true; // default
1625   const TypeOopPtr *atoop = (at != NULL) ? at-&gt;isa_oopptr() : NULL;
1626   if (atoop != NULL &amp;&amp; atoop-&gt;is_known_instance()) {
1627     const TypeOopPtr *gt = atoop-&gt;cast_to_instance_id(TypeOopPtr::InstanceBot);
1628     _general_index = Compile::current()-&gt;get_alias_index(gt);
1629   } else {
1630     _general_index = 0;
1631   }
1632 }
1633 
1634 //---------------------------------print_on------------------------------------
1635 #ifndef PRODUCT
1636 void Compile::AliasType::print_on(outputStream* st) {
1637   if (index() &lt; 10)
1638         st-&gt;print("@ &lt;%d&gt; ", index());
1639   else  st-&gt;print("@ &lt;%d&gt;",  index());
1640   st-&gt;print(is_rewritable() ? "   " : " RO");
1641   int offset = adr_type()-&gt;offset();
1642   if (offset == Type::OffsetBot)
1643         st-&gt;print(" +any");
1644   else  st-&gt;print(" +%-3d", offset);
1645   st-&gt;print(" in ");
1646   adr_type()-&gt;dump_on(st);
1647   const TypeOopPtr* tjp = adr_type()-&gt;isa_oopptr();
1648   if (field() != NULL &amp;&amp; tjp) {
1649     if (tjp-&gt;klass()  != field()-&gt;holder() ||
1650         tjp-&gt;offset() != field()-&gt;offset_in_bytes()) {
1651       st-&gt;print(" != ");
1652       field()-&gt;print();
1653       st-&gt;print(" ***");
1654     }
1655   }
1656 }
1657 
1658 void print_alias_types() {
1659   Compile* C = Compile::current();
1660   tty-&gt;print_cr("--- Alias types, AliasIdxBot .. %d", C-&gt;num_alias_types()-1);
1661   for (int idx = Compile::AliasIdxBot; idx &lt; C-&gt;num_alias_types(); idx++) {
1662     C-&gt;alias_type(idx)-&gt;print_on(tty);
1663     tty-&gt;cr();
1664   }
1665 }
1666 #endif
1667 
1668 
1669 //----------------------------probe_alias_cache--------------------------------
1670 Compile::AliasCacheEntry* Compile::probe_alias_cache(const TypePtr* adr_type) {
1671   intptr_t key = (intptr_t) adr_type;
1672   key ^= key &gt;&gt; logAliasCacheSize;
1673   return &amp;_alias_cache[key &amp; right_n_bits(logAliasCacheSize)];
1674 }
1675 
1676 
1677 //-----------------------------grow_alias_types--------------------------------
1678 void Compile::grow_alias_types() {
1679   const int old_ats  = _max_alias_types; // how many before?
1680   const int new_ats  = old_ats;          // how many more?
1681   const int grow_ats = old_ats+new_ats;  // how many now?
1682   _max_alias_types = grow_ats;
1683   _alias_types =  REALLOC_ARENA_ARRAY(comp_arena(), AliasType*, _alias_types, old_ats, grow_ats);
1684   AliasType* ats =    NEW_ARENA_ARRAY(comp_arena(), AliasType, new_ats);
1685   Copy::zero_to_bytes(ats, sizeof(AliasType)*new_ats);
1686   for (int i = 0; i &lt; new_ats; i++)  _alias_types[old_ats+i] = &amp;ats[i];
1687 }
1688 
1689 
1690 //--------------------------------find_alias_type------------------------------
1691 Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {
1692   if (_AliasLevel == 0)
1693     return alias_type(AliasIdxBot);
1694 
1695   AliasCacheEntry* ace = probe_alias_cache(adr_type);
1696   if (ace-&gt;_adr_type == adr_type) {
1697     return alias_type(ace-&gt;_index);
1698   }
1699 
1700   // Handle special cases.
1701   if (adr_type == NULL)             return alias_type(AliasIdxTop);
1702   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
1703 
1704   // Do it the slow way.
1705   const TypePtr* flat = flatten_alias_type(adr_type);
1706 
1707 #ifdef ASSERT
1708   assert(flat == flatten_alias_type(flat), "idempotent");
1709   assert(flat != TypePtr::BOTTOM,     "cannot alias-analyze an untyped ptr");
1710   if (flat-&gt;isa_oopptr() &amp;&amp; !flat-&gt;isa_klassptr()) {
1711     const TypeOopPtr* foop = flat-&gt;is_oopptr();
1712     // Scalarizable allocations have exact klass always.
1713     bool exact = !foop-&gt;klass_is_exact() || foop-&gt;is_known_instance();
1714     const TypePtr* xoop = foop-&gt;cast_to_exactness(exact)-&gt;is_ptr();
1715     assert(foop == flatten_alias_type(xoop), "exactness must not affect alias type");
1716   }
1717   assert(flat == flatten_alias_type(flat), "exact bit doesn't matter");
1718 #endif
1719 
1720   int idx = AliasIdxTop;
1721   for (int i = 0; i &lt; num_alias_types(); i++) {
1722     if (alias_type(i)-&gt;adr_type() == flat) {
1723       idx = i;
1724       break;
1725     }
1726   }
1727 
1728   if (idx == AliasIdxTop) {
1729     if (no_create)  return NULL;
1730     // Grow the array if necessary.
1731     if (_num_alias_types == _max_alias_types)  grow_alias_types();
1732     // Add a new alias type.
1733     idx = _num_alias_types++;
1734     _alias_types[idx]-&gt;Init(idx, flat);
1735     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1736     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1737     if (flat-&gt;isa_instptr()) {
1738       if (flat-&gt;offset() == java_lang_Class::klass_offset_in_bytes()
1739           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1740         alias_type(idx)-&gt;set_rewritable(false);
1741     }
1742     if (flat-&gt;isa_aryptr()) {
1743 #ifdef ASSERT
1744       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1745       // (T_BYTE has the weakest alignment and size restrictions...)
1746       assert(flat-&gt;offset() &lt; header_size_min, "array body reference must be OffsetBot");
1747 #endif
1748       if (flat-&gt;offset() == TypePtr::OffsetBot) {
1749         alias_type(idx)-&gt;set_element(flat-&gt;is_aryptr()-&gt;elem());
1750       }
1751     }
1752     if (flat-&gt;isa_klassptr()) {
1753       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1754         alias_type(idx)-&gt;set_rewritable(false);
1755       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1756         alias_type(idx)-&gt;set_rewritable(false);
1757       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1758         alias_type(idx)-&gt;set_rewritable(false);
1759       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1760         alias_type(idx)-&gt;set_rewritable(false);
1761     }
1762     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1763     // but the base pointer type is not distinctive enough to identify
1764     // references into JavaThread.)
1765 
1766     // Check for final fields.
1767     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1768     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {
1769       ciField* field;
1770       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1771           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1772           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1773         // static field
1774         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1775         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
1776       } else {
1777         ciInstanceKlass *k = tinst-&gt;klass()-&gt;as_instance_klass();
1778         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1779       }
1780       assert(field == NULL ||
1781              original_field == NULL ||
1782              (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;
1783               field-&gt;offset() == original_field-&gt;offset() &amp;&amp;
1784               field-&gt;is_static() == original_field-&gt;is_static()), "wrong field?");
1785       // Set field() and is_rewritable() attributes.
1786       if (field != NULL)  alias_type(idx)-&gt;set_field(field);
1787     }
1788   }
1789 
1790   // Fill the cache for next time.
1791   ace-&gt;_adr_type = adr_type;
1792   ace-&gt;_index    = idx;
1793   assert(alias_type(adr_type) == alias_type(idx),  "type must be installed");
1794 
1795   // Might as well try to fill the cache for the flattened version, too.
1796   AliasCacheEntry* face = probe_alias_cache(flat);
1797   if (face-&gt;_adr_type == NULL) {
1798     face-&gt;_adr_type = flat;
1799     face-&gt;_index    = idx;
1800     assert(alias_type(flat) == alias_type(idx), "flat type must work too");
1801   }
1802 
1803   return alias_type(idx);
1804 }
1805 
1806 
1807 Compile::AliasType* Compile::alias_type(ciField* field) {
1808   const TypeOopPtr* t;
1809   if (field-&gt;is_static())
1810     t = TypeInstPtr::make(field-&gt;holder()-&gt;java_mirror());
1811   else
1812     t = TypeOopPtr::make_from_klass_raw(field-&gt;holder());
1813   AliasType* atp = alias_type(t-&gt;add_offset(field-&gt;offset_in_bytes()), field);
1814   assert((field-&gt;is_final() || field-&gt;is_stable()) == !atp-&gt;is_rewritable(), "must get the rewritable bits correct");
1815   return atp;
1816 }
1817 
1818 
1819 //------------------------------have_alias_type--------------------------------
1820 bool Compile::have_alias_type(const TypePtr* adr_type) {
1821   AliasCacheEntry* ace = probe_alias_cache(adr_type);
1822   if (ace-&gt;_adr_type == adr_type) {
1823     return true;
1824   }
1825 
1826   // Handle special cases.
1827   if (adr_type == NULL)             return true;
1828   if (adr_type == TypePtr::BOTTOM)  return true;
1829 
1830   return find_alias_type(adr_type, true, NULL) != NULL;
1831 }
1832 
1833 //-----------------------------must_alias--------------------------------------
1834 // True if all values of the given address type are in the given alias category.
1835 bool Compile::must_alias(const TypePtr* adr_type, int alias_idx) {
1836   if (alias_idx == AliasIdxBot)         return true;  // the universal category
1837   if (adr_type == NULL)                 return true;  // NULL serves as TypePtr::TOP
1838   if (alias_idx == AliasIdxTop)         return false; // the empty category
1839   if (adr_type-&gt;base() == Type::AnyPtr) return false; // TypePtr::BOTTOM or its twins
1840 
1841   // the only remaining possible overlap is identity
1842   int adr_idx = get_alias_index(adr_type);
1843   assert(adr_idx != AliasIdxBot &amp;&amp; adr_idx != AliasIdxTop, "");
1844   assert(adr_idx == alias_idx ||
1845          (alias_type(alias_idx)-&gt;adr_type() != TypeOopPtr::BOTTOM
1846           &amp;&amp; adr_type                       != TypeOopPtr::BOTTOM),
1847          "should not be testing for overlap with an unsafe pointer");
1848   return adr_idx == alias_idx;
1849 }
1850 
1851 //------------------------------can_alias--------------------------------------
1852 // True if any values of the given address type are in the given alias category.
1853 bool Compile::can_alias(const TypePtr* adr_type, int alias_idx) {
1854   if (alias_idx == AliasIdxTop)         return false; // the empty category
1855   if (adr_type == NULL)                 return false; // NULL serves as TypePtr::TOP
1856   if (alias_idx == AliasIdxBot)         return true;  // the universal category
1857   if (adr_type-&gt;base() == Type::AnyPtr) return true;  // TypePtr::BOTTOM or its twins
1858 
1859   // the only remaining possible overlap is identity
1860   int adr_idx = get_alias_index(adr_type);
1861   assert(adr_idx != AliasIdxBot &amp;&amp; adr_idx != AliasIdxTop, "");
1862   return adr_idx == alias_idx;
1863 }
1864 
1865 
1866 
1867 //---------------------------pop_warm_call-------------------------------------
1868 WarmCallInfo* Compile::pop_warm_call() {
1869   WarmCallInfo* wci = _warm_calls;
1870   if (wci != NULL)  _warm_calls = wci-&gt;remove_from(wci);
1871   return wci;
1872 }
1873 
1874 //----------------------------Inline_Warm--------------------------------------
1875 int Compile::Inline_Warm() {
1876   // If there is room, try to inline some more warm call sites.
1877   // %%% Do a graph index compaction pass when we think we're out of space?
1878   if (!InlineWarmCalls)  return 0;
1879 
1880   int calls_made_hot = 0;
1881   int room_to_grow   = NodeCountInliningCutoff - unique();
1882   int amount_to_grow = MIN2(room_to_grow, (int)NodeCountInliningStep);
1883   int amount_grown   = 0;
1884   WarmCallInfo* call;
1885   while (amount_to_grow &gt; 0 &amp;&amp; (call = pop_warm_call()) != NULL) {
1886     int est_size = (int)call-&gt;size();
1887     if (est_size &gt; (room_to_grow - amount_grown)) {
1888       // This one won't fit anyway.  Get rid of it.
1889       call-&gt;make_cold();
1890       continue;
1891     }
1892     call-&gt;make_hot();
1893     calls_made_hot++;
1894     amount_grown   += est_size;
1895     amount_to_grow -= est_size;
1896   }
1897 
1898   if (calls_made_hot &gt; 0)  set_major_progress();
1899   return calls_made_hot;
1900 }
1901 
1902 
1903 //----------------------------Finish_Warm--------------------------------------
1904 void Compile::Finish_Warm() {
1905   if (!InlineWarmCalls)  return;
1906   if (failing())  return;
1907   if (warm_calls() == NULL)  return;
1908 
1909   // Clean up loose ends, if we are out of space for inlining.
1910   WarmCallInfo* call;
1911   while ((call = pop_warm_call()) != NULL) {
1912     call-&gt;make_cold();
1913   }
1914 }
1915 
1916 //---------------------cleanup_loop_predicates-----------------------
1917 // Remove the opaque nodes that protect the predicates so that all unused
1918 // checks and uncommon_traps will be eliminated from the ideal graph
1919 void Compile::cleanup_loop_predicates(PhaseIterGVN &amp;igvn) {
1920   if (predicate_count()==0) return;
1921   for (int i = predicate_count(); i &gt; 0; i--) {
1922     Node * n = predicate_opaque1_node(i-1);
1923     assert(n-&gt;Opcode() == Op_Opaque1, "must be");
1924     igvn.replace_node(n, n-&gt;in(1));
1925   }
1926   assert(predicate_count()==0, "should be clean!");
1927 }
1928 
1929 void Compile::add_range_check_cast(Node* n) {
1930   assert(n-&gt;isa_CastII()-&gt;has_range_check(), "CastII should have range check dependency");
1931   assert(!_range_check_casts-&gt;contains(n), "duplicate entry in range check casts");
1932   _range_check_casts-&gt;append(n);
1933 }
1934 
1935 // Remove all range check dependent CastIINodes.
1936 void Compile::remove_range_check_casts(PhaseIterGVN &amp;igvn) {
1937   for (int i = range_check_cast_count(); i &gt; 0; i--) {
1938     Node* cast = range_check_cast_node(i-1);
1939     assert(cast-&gt;isa_CastII()-&gt;has_range_check(), "CastII should have range check dependency");
1940     igvn.replace_node(cast, cast-&gt;in(1));
1941   }
1942   assert(range_check_cast_count() == 0, "should be empty");
1943 }
1944 
1945 // StringOpts and late inlining of string methods
1946 void Compile::inline_string_calls(bool parse_time) {
1947   {
1948     // remove useless nodes to make the usage analysis simpler
1949     ResourceMark rm;
1950     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
1951   }
1952 
1953   {
1954     ResourceMark rm;
1955     print_method(PHASE_BEFORE_STRINGOPTS, 3);
1956     PhaseStringOpts pso(initial_gvn(), for_igvn());
1957     print_method(PHASE_AFTER_STRINGOPTS, 3);
1958   }
1959 
1960   // now inline anything that we skipped the first time around
1961   if (!parse_time) {
1962     _late_inlines_pos = _late_inlines.length();
1963   }
1964 
1965   while (_string_late_inlines.length() &gt; 0) {
1966     CallGenerator* cg = _string_late_inlines.pop();
1967     cg-&gt;do_late_inline();
1968     if (failing())  return;
1969   }
1970   _string_late_inlines.trunc_to(0);
1971 }
1972 
1973 // Late inlining of boxing methods
1974 void Compile::inline_boxing_calls(PhaseIterGVN&amp; igvn) {
1975   if (_boxing_late_inlines.length() &gt; 0) {
1976     assert(has_boxed_value(), "inconsistent");
1977 
1978     PhaseGVN* gvn = initial_gvn();
1979     set_inlining_incrementally(true);
1980 
1981     assert( igvn._worklist.size() == 0, "should be done with igvn" );
1982     for_igvn()-&gt;clear();
1983     gvn-&gt;replace_with(&amp;igvn);
1984 
1985     _late_inlines_pos = _late_inlines.length();
1986 
1987     while (_boxing_late_inlines.length() &gt; 0) {
1988       CallGenerator* cg = _boxing_late_inlines.pop();
1989       cg-&gt;do_late_inline();
1990       if (failing())  return;
1991     }
1992     _boxing_late_inlines.trunc_to(0);
1993 
1994     {
1995       ResourceMark rm;
1996       PhaseRemoveUseless pru(gvn, for_igvn());
1997     }
1998 
1999     igvn = PhaseIterGVN(gvn);
2000     igvn.optimize();
2001 
2002     set_inlining_progress(false);
2003     set_inlining_incrementally(false);
2004   }
2005 }
2006 
2007 void Compile::inline_incrementally_one(PhaseIterGVN&amp; igvn) {
2008   assert(IncrementalInline, "incremental inlining should be on");
2009   PhaseGVN* gvn = initial_gvn();
2010 
2011   set_inlining_progress(false);
2012   for_igvn()-&gt;clear();
2013   gvn-&gt;replace_with(&amp;igvn);
2014 
2015   {
2016     TracePhase tp("incrementalInline_inline", &amp;timers[_t_incrInline_inline]);
2017     int i = 0;
2018     for (; i &lt;_late_inlines.length() &amp;&amp; !inlining_progress(); i++) {
2019       CallGenerator* cg = _late_inlines.at(i);
2020       _late_inlines_pos = i+1;
2021       cg-&gt;do_late_inline();
2022       if (failing())  return;
2023     }
2024     int j = 0;
2025     for (; i &lt; _late_inlines.length(); i++, j++) {
2026       _late_inlines.at_put(j, _late_inlines.at(i));
2027     }
2028     _late_inlines.trunc_to(j);
2029   }
2030 
2031   {
2032     TracePhase tp("incrementalInline_pru", &amp;timers[_t_incrInline_pru]);
2033     ResourceMark rm;
2034     PhaseRemoveUseless pru(gvn, for_igvn());
2035   }
2036 
2037   {
2038     TracePhase tp("incrementalInline_igvn", &amp;timers[_t_incrInline_igvn]);
2039     igvn = PhaseIterGVN(gvn);
2040   }
2041 }
2042 
2043 // Perform incremental inlining until bound on number of live nodes is reached
2044 void Compile::inline_incrementally(PhaseIterGVN&amp; igvn) {
2045   TracePhase tp("incrementalInline", &amp;timers[_t_incrInline]);
2046 
2047   PhaseGVN* gvn = initial_gvn();
2048 
2049   set_inlining_incrementally(true);
2050   set_inlining_progress(true);
2051   uint low_live_nodes = 0;
2052 
2053   while(inlining_progress() &amp;&amp; _late_inlines.length() &gt; 0) {
2054 
2055     if (live_nodes() &gt; (uint)LiveNodeCountInliningCutoff) {
2056       if (low_live_nodes &lt; (uint)LiveNodeCountInliningCutoff * 8 / 10) {
2057         TracePhase tp("incrementalInline_ideal", &amp;timers[_t_incrInline_ideal]);
2058         // PhaseIdealLoop is expensive so we only try it once we are
2059         // out of live nodes and we only try it again if the previous
2060         // helped got the number of nodes down significantly
2061         PhaseIdealLoop ideal_loop( igvn, false, true );
2062         if (failing())  return;
2063         low_live_nodes = live_nodes();
2064         _major_progress = true;
2065       }
2066 
2067       if (live_nodes() &gt; (uint)LiveNodeCountInliningCutoff) {
2068         break;
2069       }
2070     }
2071 
2072     inline_incrementally_one(igvn);
2073 
2074     if (failing())  return;
2075 
2076     {
2077       TracePhase tp("incrementalInline_igvn", &amp;timers[_t_incrInline_igvn]);
2078       igvn.optimize();
2079     }
2080 
2081     if (failing())  return;
2082   }
2083 
2084   assert( igvn._worklist.size() == 0, "should be done with igvn" );
2085 
2086   if (_string_late_inlines.length() &gt; 0) {
2087     assert(has_stringbuilder(), "inconsistent");
2088     for_igvn()-&gt;clear();
2089     initial_gvn()-&gt;replace_with(&amp;igvn);
2090 
2091     inline_string_calls(false);
2092 
2093     if (failing())  return;
2094 
2095     {
2096       TracePhase tp("incrementalInline_pru", &amp;timers[_t_incrInline_pru]);
2097       ResourceMark rm;
2098       PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2099     }
2100 
2101     {
2102       TracePhase tp("incrementalInline_igvn", &amp;timers[_t_incrInline_igvn]);
2103       igvn = PhaseIterGVN(gvn);
2104       igvn.optimize();
2105     }
2106   }
2107 
2108   set_inlining_incrementally(false);
2109 }
2110 
2111 
2112 //------------------------------Optimize---------------------------------------
2113 // Given a graph, optimize it.
2114 void Compile::Optimize() {
2115   TracePhase tp("optimizer", &amp;timers[_t_optimizer]);
2116 
2117 #ifndef PRODUCT
2118   if (_directive-&gt;BreakAtCompileOption) {
2119     BREAKPOINT;
2120   }
2121 
2122 #endif
2123 
2124   ResourceMark rm;
2125   int          loop_opts_cnt;
2126 
2127   print_inlining_reinit();
2128 
2129   NOT_PRODUCT( verify_graph_edges(); )
2130 
2131   print_method(PHASE_AFTER_PARSING);
2132 
2133  {
2134   // Iterative Global Value Numbering, including ideal transforms
2135   // Initialize IterGVN with types and values from parse-time GVN
2136   PhaseIterGVN igvn(initial_gvn());
2137 #ifdef ASSERT
2138   _modified_nodes = new (comp_arena()) Unique_Node_List(comp_arena());
2139 #endif
2140   {
2141     TracePhase tp("iterGVN", &amp;timers[_t_iterGVN]);
2142     igvn.optimize();
2143   }
2144 
2145   print_method(PHASE_ITER_GVN1, 2);
2146 
2147   if (failing())  return;
2148 
2149   inline_incrementally(igvn);
2150 
2151   print_method(PHASE_INCREMENTAL_INLINE, 2);
2152 
2153   if (failing())  return;
2154 
2155   if (eliminate_boxing()) {
2156     // Inline valueOf() methods now.
2157     inline_boxing_calls(igvn);
2158 
2159     if (AlwaysIncrementalInline) {
2160       inline_incrementally(igvn);
2161     }
2162 
2163     print_method(PHASE_INCREMENTAL_BOXING_INLINE, 2);
2164 
2165     if (failing())  return;
2166   }
2167 
2168   // Remove the speculative part of types and clean up the graph from
2169   // the extra CastPP nodes whose only purpose is to carry them. Do
2170   // that early so that optimizations are not disrupted by the extra
2171   // CastPP nodes.
2172   remove_speculative_types(igvn);
2173 
2174   // No more new expensive nodes will be added to the list from here
2175   // so keep only the actual candidates for optimizations.
2176   cleanup_expensive_nodes(igvn);
2177 
2178   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2179     Compile::TracePhase tp("", &amp;timers[_t_renumberLive]);
2180     initial_gvn()-&gt;replace_with(&amp;igvn);
2181     for_igvn()-&gt;clear();
2182     Unique_Node_List new_worklist(C-&gt;comp_arena());
2183     {
2184       ResourceMark rm;
2185       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2186     }
2187     set_for_igvn(&amp;new_worklist);
2188     igvn = PhaseIterGVN(initial_gvn());
2189     igvn.optimize();
2190   }
2191 
2192   // Perform escape analysis
2193   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2194     if (has_loops()) {
2195       // Cleanup graph (remove dead nodes).
2196       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2197       PhaseIdealLoop ideal_loop( igvn, false, true );
2198       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2199       if (failing())  return;
2200     }
2201     ConnectionGraph::do_analysis(this, &amp;igvn);
2202 
2203     if (failing())  return;
2204 
2205     // Optimize out fields loads from scalar replaceable allocations.
2206     igvn.optimize();
2207     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2208 
2209     if (failing())  return;
2210 
2211     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
2212       TracePhase tp("macroEliminate", &amp;timers[_t_macroEliminate]);
2213       PhaseMacroExpand mexp(igvn);
2214       mexp.eliminate_macro_nodes();
2215       igvn.set_delay_transform(false);
2216 
2217       igvn.optimize();
2218       print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);
2219 
2220       if (failing())  return;
2221     }
2222   }
2223 
2224   // Loop transforms on the ideal graph.  Range Check Elimination,
2225   // peeling, unrolling, etc.
2226 
2227   // Set loop opts counter
2228   loop_opts_cnt = num_loop_opts();
2229   if((loop_opts_cnt &gt; 0) &amp;&amp; (has_loops() || has_split_ifs())) {
2230     {
2231       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2232       PhaseIdealLoop ideal_loop( igvn, true );
2233       loop_opts_cnt--;
2234       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP1, 2);
2235       if (failing())  return;
2236     }
2237     // Loop opts pass if partial peeling occurred in previous pass
2238     if(PartialPeelLoop &amp;&amp; major_progress() &amp;&amp; (loop_opts_cnt &gt; 0)) {
2239       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2240       PhaseIdealLoop ideal_loop( igvn, false );
2241       loop_opts_cnt--;
2242       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP2, 2);
2243       if (failing())  return;
2244     }
2245     // Loop opts pass for loop-unrolling before CCP
2246     if(major_progress() &amp;&amp; (loop_opts_cnt &gt; 0)) {
2247       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2248       PhaseIdealLoop ideal_loop( igvn, false );
2249       loop_opts_cnt--;
2250       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP3, 2);
2251     }
2252     if (!failing()) {
2253       // Verify that last round of loop opts produced a valid graph
2254       TracePhase tp("idealLoopVerify", &amp;timers[_t_idealLoopVerify]);
2255       PhaseIdealLoop::verify(igvn);
2256     }
2257   }
2258   if (failing())  return;
2259 
2260   // Conditional Constant Propagation;
2261   PhaseCCP ccp( &amp;igvn );
2262   assert( true, "Break here to ccp.dump_nodes_and_types(_root,999,1)");
2263   {
2264     TracePhase tp("ccp", &amp;timers[_t_ccp]);
2265     ccp.do_transform();
2266   }
2267   print_method(PHASE_CPP1, 2);
2268 
2269   assert( true, "Break here to ccp.dump_old2new_map()");
2270 
2271   // Iterative Global Value Numbering, including ideal transforms
2272   {
2273     TracePhase tp("iterGVN2", &amp;timers[_t_iterGVN2]);
2274     igvn = ccp;
2275     igvn.optimize();
2276   }
2277 
2278   print_method(PHASE_ITER_GVN2, 2);
2279 
2280   if (failing())  return;
2281 
2282   // Loop transforms on the ideal graph.  Range Check Elimination,
2283   // peeling, unrolling, etc.
2284   if(loop_opts_cnt &gt; 0) {
2285     debug_only( int cnt = 0; );
2286     while(major_progress() &amp;&amp; (loop_opts_cnt &gt; 0)) {
2287       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2288       assert( cnt++ &lt; 40, "infinite cycle in loop optimization" );
2289       PhaseIdealLoop ideal_loop( igvn, true);
2290       loop_opts_cnt--;
2291       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP_ITERATIONS, 2);
2292       if (failing())  return;
2293     }
2294   }
2295   // Ensure that major progress is now clear
2296   C-&gt;clear_major_progress();
2297 
2298   {
2299     // Verify that all previous optimizations produced a valid graph
2300     // at least to this point, even if no loop optimizations were done.
2301     TracePhase tp("idealLoopVerify", &amp;timers[_t_idealLoopVerify]);
2302     PhaseIdealLoop::verify(igvn);
2303   }
2304 
2305   if (range_check_cast_count() &gt; 0) {
2306     // No more loop optimizations. Remove all range check dependent CastIINodes.
2307     C-&gt;remove_range_check_casts(igvn);
2308     igvn.optimize();
2309   }
2310 
2311   {
2312     TracePhase tp("macroExpand", &amp;timers[_t_macroExpand]);
2313     PhaseMacroExpand  mex(igvn);
2314     if (mex.expand_macro_nodes()) {
2315       assert(failing(), "must bail out w/ explicit message");
2316       return;
2317     }
2318   }
2319 
2320   DEBUG_ONLY( _modified_nodes = NULL; )
2321  } // (End scope of igvn; run destructor if necessary for asserts.)
2322 
2323  process_print_inlining();
2324  // A method with only infinite loops has no edges entering loops from root
2325  {
2326    TracePhase tp("graphReshape", &amp;timers[_t_graphReshaping]);
2327    if (final_graph_reshaping()) {
2328      assert(failing(), "must bail out w/ explicit message");
2329      return;
2330    }
2331  }
2332 
2333  print_method(PHASE_OPTIMIZE_FINISHED, 2);
2334 }
2335 
2336 
2337 //------------------------------Code_Gen---------------------------------------
2338 // Given a graph, generate code for it
2339 void Compile::Code_Gen() {
2340   if (failing()) {
2341     return;
2342   }
2343 
2344   // Perform instruction selection.  You might think we could reclaim Matcher
2345   // memory PDQ, but actually the Matcher is used in generating spill code.
2346   // Internals of the Matcher (including some VectorSets) must remain live
2347   // for awhile - thus I cannot reclaim Matcher memory lest a VectorSet usage
2348   // set a bit in reclaimed memory.
2349 
2350   // In debug mode can dump m._nodes.dump() for mapping of ideal to machine
2351   // nodes.  Mapping is only valid at the root of each matched subtree.
2352   NOT_PRODUCT( verify_graph_edges(); )
2353 
2354   Matcher matcher;
2355   _matcher = &amp;matcher;
2356   {
2357     TracePhase tp("matcher", &amp;timers[_t_matcher]);
2358     matcher.match();
2359   }
2360   // In debug mode can dump m._nodes.dump() for mapping of ideal to machine
2361   // nodes.  Mapping is only valid at the root of each matched subtree.
2362   NOT_PRODUCT( verify_graph_edges(); )
2363 
2364   // If you have too many nodes, or if matching has failed, bail out
2365   check_node_count(0, "out of nodes matching instructions");
2366   if (failing()) {
2367     return;
2368   }
2369 
2370   // Build a proper-looking CFG
2371   PhaseCFG cfg(node_arena(), root(), matcher);
2372   _cfg = &amp;cfg;
2373   {
2374     TracePhase tp("scheduler", &amp;timers[_t_scheduler]);
2375     bool success = cfg.do_global_code_motion();
2376     if (!success) {
2377       return;
2378     }
2379 
2380     print_method(PHASE_GLOBAL_CODE_MOTION, 2);
2381     NOT_PRODUCT( verify_graph_edges(); )
2382     debug_only( cfg.verify(); )
2383   }
2384 
2385   PhaseChaitin regalloc(unique(), cfg, matcher, false);
2386   _regalloc = &amp;regalloc;
2387   {
2388     TracePhase tp("regalloc", &amp;timers[_t_registerAllocation]);
2389     // Perform register allocation.  After Chaitin, use-def chains are
2390     // no longer accurate (at spill code) and so must be ignored.
2391     // Node-&gt;LRG-&gt;reg mappings are still accurate.
2392     _regalloc-&gt;Register_Allocate();
2393 
2394     // Bail out if the allocator builds too many nodes
2395     if (failing()) {
2396       return;
2397     }
2398   }
2399 
2400   // Prior to register allocation we kept empty basic blocks in case the
2401   // the allocator needed a place to spill.  After register allocation we
2402   // are not adding any new instructions.  If any basic block is empty, we
2403   // can now safely remove it.
2404   {
2405     TracePhase tp("blockOrdering", &amp;timers[_t_blockOrdering]);
2406     cfg.remove_empty_blocks();
2407     if (do_freq_based_layout()) {
2408       PhaseBlockLayout layout(cfg);
2409     } else {
2410       cfg.set_loop_alignment();
2411     }
2412     cfg.fixup_flow();
2413   }
2414 
2415   // Apply peephole optimizations
2416   if( OptoPeephole ) {
2417     TracePhase tp("peephole", &amp;timers[_t_peephole]);
2418     PhasePeephole peep( _regalloc, cfg);
2419     peep.do_transform();
2420   }
2421 
2422   // Do late expand if CPU requires this.
2423   if (Matcher::require_postalloc_expand) {
2424     TracePhase tp("postalloc_expand", &amp;timers[_t_postalloc_expand]);
2425     cfg.postalloc_expand(_regalloc);
2426   }
2427 
2428   // Convert Nodes to instruction bits in a buffer
2429   {
2430     TraceTime tp("output", &amp;timers[_t_output], CITime);
2431     Output();
2432   }
2433 
2434   print_method(PHASE_FINAL_CODE);
2435 
2436   // He's dead, Jim.
2437   _cfg     = (PhaseCFG*)0xdeadbeef;
2438   _regalloc = (PhaseChaitin*)0xdeadbeef;
2439 }
2440 
2441 
2442 //------------------------------dump_asm---------------------------------------
2443 // Dump formatted assembly
2444 #ifndef PRODUCT
2445 void Compile::dump_asm(int *pcs, uint pc_limit) {
2446   bool cut_short = false;
2447   tty-&gt;print_cr("#");
2448   tty-&gt;print("#  ");  _tf-&gt;dump();  tty-&gt;cr();
2449   tty-&gt;print_cr("#");
2450 
2451   // For all blocks
2452   int pc = 0x0;                 // Program counter
2453   char starts_bundle = ' ';
2454   _regalloc-&gt;dump_frame();
2455 
2456   Node *n = NULL;
2457   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
2458     if (VMThread::should_terminate()) {
2459       cut_short = true;
2460       break;
2461     }
2462     Block* block = _cfg-&gt;get_block(i);
2463     if (block-&gt;is_connector() &amp;&amp; !Verbose) {
2464       continue;
2465     }
2466     n = block-&gt;head();
2467     if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit) {
2468       tty-&gt;print("%3.3x   ", pcs[n-&gt;_idx]);
2469     } else {
2470       tty-&gt;print("      ");
2471     }
2472     block-&gt;dump_head(_cfg);
2473     if (block-&gt;is_connector()) {
2474       tty-&gt;print_cr("        # Empty connector block");
2475     } else if (block-&gt;num_preds() == 2 &amp;&amp; block-&gt;pred(1)-&gt;is_CatchProj() &amp;&amp; block-&gt;pred(1)-&gt;as_CatchProj()-&gt;_con == CatchProjNode::fall_through_index) {
2476       tty-&gt;print_cr("        # Block is sole successor of call");
2477     }
2478 
2479     // For all instructions
2480     Node *delay = NULL;
2481     for (uint j = 0; j &lt; block-&gt;number_of_nodes(); j++) {
2482       if (VMThread::should_terminate()) {
2483         cut_short = true;
2484         break;
2485       }
2486       n = block-&gt;get_node(j);
2487       if (valid_bundle_info(n)) {
2488         Bundle* bundle = node_bundling(n);
2489         if (bundle-&gt;used_in_unconditional_delay()) {
2490           delay = n;
2491           continue;
2492         }
2493         if (bundle-&gt;starts_bundle()) {
2494           starts_bundle = '+';
2495         }
2496       }
2497 
2498       if (WizardMode) {
2499         n-&gt;dump();
2500       }
2501 
2502       if( !n-&gt;is_Region() &amp;&amp;    // Dont print in the Assembly
2503           !n-&gt;is_Phi() &amp;&amp;       // a few noisely useless nodes
2504           !n-&gt;is_Proj() &amp;&amp;
2505           !n-&gt;is_MachTemp() &amp;&amp;
2506           !n-&gt;is_SafePointScalarObject() &amp;&amp;
2507           !n-&gt;is_Catch() &amp;&amp;     // Would be nice to print exception table targets
2508           !n-&gt;is_MergeMem() &amp;&amp;  // Not very interesting
2509           !n-&gt;is_top() &amp;&amp;       // Debug info table constants
2510           !(n-&gt;is_Con() &amp;&amp; !n-&gt;is_Mach())// Debug info table constants
2511           ) {
2512         if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit)
2513           tty-&gt;print("%3.3x", pcs[n-&gt;_idx]);
2514         else
2515           tty-&gt;print("   ");
2516         tty-&gt;print(" %c ", starts_bundle);
2517         starts_bundle = ' ';
2518         tty-&gt;print("\t");
2519         n-&gt;format(_regalloc, tty);
2520         tty-&gt;cr();
2521       }
2522 
2523       // If we have an instruction with a delay slot, and have seen a delay,
2524       // then back up and print it
2525       if (valid_bundle_info(n) &amp;&amp; node_bundling(n)-&gt;use_unconditional_delay()) {
2526         assert(delay != NULL, "no unconditional delay instruction");
2527         if (WizardMode) delay-&gt;dump();
2528 
2529         if (node_bundling(delay)-&gt;starts_bundle())
2530           starts_bundle = '+';
2531         if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit)
2532           tty-&gt;print("%3.3x", pcs[n-&gt;_idx]);
2533         else
2534           tty-&gt;print("   ");
2535         tty-&gt;print(" %c ", starts_bundle);
2536         starts_bundle = ' ';
2537         tty-&gt;print("\t");
2538         delay-&gt;format(_regalloc, tty);
2539         tty-&gt;cr();
2540         delay = NULL;
2541       }
2542 
2543       // Dump the exception table as well
2544       if( n-&gt;is_Catch() &amp;&amp; (Verbose || WizardMode) ) {
2545         // Print the exception table for this offset
2546         _handler_table.print_subtable_for(pc);
2547       }
2548     }
2549 
2550     if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit)
2551       tty-&gt;print_cr("%3.3x", pcs[n-&gt;_idx]);
2552     else
2553       tty-&gt;cr();
2554 
2555     assert(cut_short || delay == NULL, "no unconditional delay branch");
2556 
2557   } // End of per-block dump
2558   tty-&gt;cr();
2559 
2560   if (cut_short)  tty-&gt;print_cr("*** disassembly is cut short ***");
2561 }
2562 #endif
2563 
2564 //------------------------------Final_Reshape_Counts---------------------------
2565 // This class defines counters to help identify when a method
2566 // may/must be executed using hardware with only 24-bit precision.
2567 struct Final_Reshape_Counts : public StackObj {
2568   int  _call_count;             // count non-inlined 'common' calls
2569   int  _float_count;            // count float ops requiring 24-bit precision
2570   int  _double_count;           // count double ops requiring more precision
2571   int  _java_call_count;        // count non-inlined 'java' calls
2572   int  _inner_loop_count;       // count loops which need alignment
2573   VectorSet _visited;           // Visitation flags
2574   Node_List _tests;             // Set of IfNodes &amp; PCTableNodes
2575 
2576   Final_Reshape_Counts() :
2577     _call_count(0), _float_count(0), _double_count(0),
2578     _java_call_count(0), _inner_loop_count(0),
2579     _visited( Thread::current()-&gt;resource_area() ) { }
2580 
2581   void inc_call_count  () { _call_count  ++; }
2582   void inc_float_count () { _float_count ++; }
2583   void inc_double_count() { _double_count++; }
2584   void inc_java_call_count() { _java_call_count++; }
2585   void inc_inner_loop_count() { _inner_loop_count++; }
2586 
2587   int  get_call_count  () const { return _call_count  ; }
2588   int  get_float_count () const { return _float_count ; }
2589   int  get_double_count() const { return _double_count; }
2590   int  get_java_call_count() const { return _java_call_count; }
2591   int  get_inner_loop_count() const { return _inner_loop_count; }
2592 };
2593 
2594 #ifdef ASSERT
2595 static bool oop_offset_is_sane(const TypeInstPtr* tp) {
2596   ciInstanceKlass *k = tp-&gt;klass()-&gt;as_instance_klass();
2597   // Make sure the offset goes inside the instance layout.
2598   return k-&gt;contains_field_offset(tp-&gt;offset());
2599   // Note that OffsetBot and OffsetTop are very negative.
2600 }
2601 #endif
2602 
2603 // Eliminate trivially redundant StoreCMs and accumulate their
2604 // precedence edges.
2605 void Compile::eliminate_redundant_card_marks(Node* n) {
2606   assert(n-&gt;Opcode() == Op_StoreCM, "expected StoreCM");
2607   if (n-&gt;in(MemNode::Address)-&gt;outcnt() &gt; 1) {
2608     // There are multiple users of the same address so it might be
2609     // possible to eliminate some of the StoreCMs
2610     Node* mem = n-&gt;in(MemNode::Memory);
2611     Node* adr = n-&gt;in(MemNode::Address);
2612     Node* val = n-&gt;in(MemNode::ValueIn);
2613     Node* prev = n;
2614     bool done = false;
2615     // Walk the chain of StoreCMs eliminating ones that match.  As
2616     // long as it's a chain of single users then the optimization is
2617     // safe.  Eliminating partially redundant StoreCMs would require
2618     // cloning copies down the other paths.
2619     while (mem-&gt;Opcode() == Op_StoreCM &amp;&amp; mem-&gt;outcnt() == 1 &amp;&amp; !done) {
2620       if (adr == mem-&gt;in(MemNode::Address) &amp;&amp;
2621           val == mem-&gt;in(MemNode::ValueIn)) {
2622         // redundant StoreCM
2623         if (mem-&gt;req() &gt; MemNode::OopStore) {
2624           // Hasn't been processed by this code yet.
2625           n-&gt;add_prec(mem-&gt;in(MemNode::OopStore));
2626         } else {
2627           // Already converted to precedence edge
2628           for (uint i = mem-&gt;req(); i &lt; mem-&gt;len(); i++) {
2629             // Accumulate any precedence edges
2630             if (mem-&gt;in(i) != NULL) {
2631               n-&gt;add_prec(mem-&gt;in(i));
2632             }
2633           }
2634           // Everything above this point has been processed.
2635           done = true;
2636         }
2637         // Eliminate the previous StoreCM
2638         prev-&gt;set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2639         assert(mem-&gt;outcnt() == 0, "should be dead");
2640         mem-&gt;disconnect_inputs(NULL, this);
2641       } else {
2642         prev = mem;
2643       }
2644       mem = prev-&gt;in(MemNode::Memory);
2645     }
2646   }
2647 }
2648 
2649 //------------------------------final_graph_reshaping_impl----------------------
2650 // Implement items 1-5 from final_graph_reshaping below.
2651 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &amp;frc) {
2652 
2653   if ( n-&gt;outcnt() == 0 ) return; // dead node
2654   uint nop = n-&gt;Opcode();
2655 
2656   // Check for 2-input instruction with "last use" on right input.
2657   // Swap to left input.  Implements item (2).
2658   if( n-&gt;req() == 3 &amp;&amp;          // two-input instruction
2659       n-&gt;in(1)-&gt;outcnt() &gt; 1 &amp;&amp; // left use is NOT a last use
2660       (!n-&gt;in(1)-&gt;is_Phi() || n-&gt;in(1)-&gt;in(2) != n) &amp;&amp; // it is not data loop
2661       n-&gt;in(2)-&gt;outcnt() == 1 &amp;&amp;// right use IS a last use
2662       !n-&gt;in(2)-&gt;is_Con() ) {   // right use is not a constant
2663     // Check for commutative opcode
2664     switch( nop ) {
2665     case Op_AddI:  case Op_AddF:  case Op_AddD:  case Op_AddL:
2666     case Op_MaxI:  case Op_MinI:
2667     case Op_MulI:  case Op_MulF:  case Op_MulD:  case Op_MulL:
2668     case Op_AndL:  case Op_XorL:  case Op_OrL:
2669     case Op_AndI:  case Op_XorI:  case Op_OrI: {
2670       // Move "last use" input to left by swapping inputs
2671       n-&gt;swap_edges(1, 2);
2672       break;
2673     }
2674     default:
2675       break;
2676     }
2677   }
2678 
2679 #ifdef ASSERT
2680   if( n-&gt;is_Mem() ) {
2681     int alias_idx = get_alias_index(n-&gt;as_Mem()-&gt;adr_type());
2682     assert( n-&gt;in(0) != NULL || alias_idx != Compile::AliasIdxRaw ||
2683             // oop will be recorded in oop map if load crosses safepoint
2684             n-&gt;is_Load() &amp;&amp; (n-&gt;as_Load()-&gt;bottom_type()-&gt;isa_oopptr() ||
2685                              LoadNode::is_immutable_value(n-&gt;in(MemNode::Address))),
2686             "raw memory operations should have control edge");
2687   }
2688 #endif
2689   // Count FPU ops and common calls, implements item (3)
2690   switch( nop ) {
2691   // Count all float operations that may use FPU
2692   case Op_AddF:
2693   case Op_SubF:
2694   case Op_MulF:
2695   case Op_DivF:
2696   case Op_NegF:
2697   case Op_ModF:
2698   case Op_ConvI2F:
2699   case Op_ConF:
2700   case Op_CmpF:
2701   case Op_CmpF3:
2702   // case Op_ConvL2F: // longs are split into 32-bit halves
2703     frc.inc_float_count();
2704     break;
2705 
2706   case Op_ConvF2D:
2707   case Op_ConvD2F:
2708     frc.inc_float_count();
2709     frc.inc_double_count();
2710     break;
2711 
2712   // Count all double operations that may use FPU
2713   case Op_AddD:
2714   case Op_SubD:
2715   case Op_MulD:
2716   case Op_DivD:
2717   case Op_NegD:
2718   case Op_ModD:
2719   case Op_ConvI2D:
2720   case Op_ConvD2I:
2721   // case Op_ConvL2D: // handled by leaf call
2722   // case Op_ConvD2L: // handled by leaf call
2723   case Op_ConD:
2724   case Op_CmpD:
2725   case Op_CmpD3:
2726     frc.inc_double_count();
2727     break;
2728   case Op_Opaque1:              // Remove Opaque Nodes before matching
2729   case Op_Opaque2:              // Remove Opaque Nodes before matching
2730   case Op_Opaque3:
2731     n-&gt;subsume_by(n-&gt;in(1), this);
2732     break;
2733   case Op_CallStaticJava:
2734   case Op_CallJava:
2735   case Op_CallDynamicJava:
2736     frc.inc_java_call_count(); // Count java call site;
2737   case Op_CallRuntime:
2738   case Op_CallLeaf:
2739   case Op_CallLeafNoFP: {
2740     assert( n-&gt;is_Call(), "" );
2741     CallNode *call = n-&gt;as_Call();
2742     // Count call sites where the FP mode bit would have to be flipped.
2743     // Do not count uncommon runtime calls:
2744     // uncommon_trap, _complete_monitor_locking, _complete_monitor_unlocking,
2745     // _new_Java, _new_typeArray, _new_objArray, _rethrow_Java, ...
2746     if( !call-&gt;is_CallStaticJava() || !call-&gt;as_CallStaticJava()-&gt;_name ) {
2747       frc.inc_call_count();   // Count the call site
2748     } else {                  // See if uncommon argument is shared
2749       Node *n = call-&gt;in(TypeFunc::Parms);
2750       int nop = n-&gt;Opcode();
2751       // Clone shared simple arguments to uncommon calls, item (1).
2752       if( n-&gt;outcnt() &gt; 1 &amp;&amp;
2753           !n-&gt;is_Proj() &amp;&amp;
2754           nop != Op_CreateEx &amp;&amp;
2755           nop != Op_CheckCastPP &amp;&amp;
2756           nop != Op_DecodeN &amp;&amp;
2757           nop != Op_DecodeNKlass &amp;&amp;
2758           !n-&gt;is_Mem() ) {
2759         Node *x = n-&gt;clone();
2760         call-&gt;set_req( TypeFunc::Parms, x );
2761       }
2762     }
2763     break;
2764   }
2765 
2766   case Op_StoreD:
2767   case Op_LoadD:
2768   case Op_LoadD_unaligned:
2769     frc.inc_double_count();
2770     goto handle_mem;
2771   case Op_StoreF:
2772   case Op_LoadF:
2773     frc.inc_float_count();
2774     goto handle_mem;
2775 
2776   case Op_StoreCM:
2777     {
2778       // Convert OopStore dependence into precedence edge
2779       Node* prec = n-&gt;in(MemNode::OopStore);
2780       n-&gt;del_req(MemNode::OopStore);
2781       n-&gt;add_prec(prec);
2782       eliminate_redundant_card_marks(n);
2783     }
2784 
2785     // fall through
2786 
2787   case Op_StoreB:
2788   case Op_StoreC:
2789   case Op_StorePConditional:
2790   case Op_StoreI:
2791   case Op_StoreL:
2792   case Op_StoreIConditional:
2793   case Op_StoreLConditional:
2794   case Op_CompareAndSwapI:
2795   case Op_CompareAndSwapL:
2796   case Op_CompareAndSwapP:
2797   case Op_CompareAndSwapN:
2798   case Op_WeakCompareAndSwapI:
2799   case Op_WeakCompareAndSwapL:
2800   case Op_WeakCompareAndSwapP:
2801   case Op_WeakCompareAndSwapN:
2802   case Op_CompareAndExchangeI:
2803   case Op_CompareAndExchangeL:
2804   case Op_CompareAndExchangeP:
2805   case Op_CompareAndExchangeN:
2806   case Op_GetAndAddI:
2807   case Op_GetAndAddL:
2808   case Op_GetAndSetI:
2809   case Op_GetAndSetL:
2810   case Op_GetAndSetP:
2811   case Op_GetAndSetN:
2812   case Op_StoreP:
2813   case Op_StoreN:
2814   case Op_StoreNKlass:
2815   case Op_LoadB:
2816   case Op_LoadUB:
2817   case Op_LoadUS:
2818   case Op_LoadI:
2819   case Op_LoadKlass:
2820   case Op_LoadNKlass:
2821   case Op_LoadL:
2822   case Op_LoadL_unaligned:
2823   case Op_LoadPLocked:
2824   case Op_LoadP:
2825   case Op_LoadN:
2826   case Op_LoadRange:
2827   case Op_LoadS: {
2828   handle_mem:
2829 #ifdef ASSERT
2830     if( VerifyOptoOopOffsets ) {
2831       assert( n-&gt;is_Mem(), "" );
2832       MemNode *mem  = (MemNode*)n;
2833       // Check to see if address types have grounded out somehow.
2834       const TypeInstPtr *tp = mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_instptr();
2835       assert( !tp || oop_offset_is_sane(tp), "" );
2836     }
2837 #endif
2838     break;
2839   }
2840 
2841   case Op_AddP: {               // Assert sane base pointers
2842     Node *addp = n-&gt;in(AddPNode::Address);
2843     assert( !addp-&gt;is_AddP() ||
2844             addp-&gt;in(AddPNode::Base)-&gt;is_top() || // Top OK for allocation
2845             addp-&gt;in(AddPNode::Base) == n-&gt;in(AddPNode::Base),
2846             "Base pointers must match" );
2847 #ifdef _LP64
2848     if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp;
2849         addp-&gt;Opcode() == Op_ConP &amp;&amp;
2850         addp == n-&gt;in(AddPNode::Base) &amp;&amp;
2851         n-&gt;in(AddPNode::Offset)-&gt;is_Con()) {
2852       // Use addressing with narrow klass to load with offset on x86.
2853       // On sparc loading 32-bits constant and decoding it have less
2854       // instructions (4) then load 64-bits constant (7).
2855       // Do this transformation here since IGVN will convert ConN back to ConP.
2856       const Type* t = addp-&gt;bottom_type();
2857       if (t-&gt;isa_oopptr() || t-&gt;isa_klassptr()) {
2858         Node* nn = NULL;
2859 
2860         int op = t-&gt;isa_oopptr() ? Op_ConN : Op_ConNKlass;
2861 
2862         // Look for existing ConN node of the same exact type.
2863         Node* r  = root();
2864         uint cnt = r-&gt;outcnt();
2865         for (uint i = 0; i &lt; cnt; i++) {
2866           Node* m = r-&gt;raw_out(i);
2867           if (m!= NULL &amp;&amp; m-&gt;Opcode() == op &amp;&amp;
2868               m-&gt;bottom_type()-&gt;make_ptr() == t) {
2869             nn = m;
2870             break;
2871           }
2872         }
2873         if (nn != NULL) {
2874           // Decode a narrow oop to match address
2875           // [R12 + narrow_oop_reg&lt;&lt;3 + offset]
2876           if (t-&gt;isa_oopptr()) {
2877             nn = new DecodeNNode(nn, t);
2878           } else {
2879             nn = new DecodeNKlassNode(nn, t);
2880           }
2881           n-&gt;set_req(AddPNode::Base, nn);
2882           n-&gt;set_req(AddPNode::Address, nn);
2883           if (addp-&gt;outcnt() == 0) {
2884             addp-&gt;disconnect_inputs(NULL, this);
2885           }
2886         }
2887       }
2888     }
2889 #endif
2890     break;
2891   }
2892 
2893   case Op_CastPP: {
2894     // Remove CastPP nodes to gain more freedom during scheduling but
2895     // keep the dependency they encode as control or precedence edges
2896     // (if control is set already) on memory operations. Some CastPP
2897     // nodes don't have a control (don't carry a dependency): skip
2898     // those.
2899     if (n-&gt;in(0) != NULL) {
2900       ResourceMark rm;
2901       Unique_Node_List wq;
2902       wq.push(n);
2903       for (uint next = 0; next &lt; wq.size(); ++next) {
2904         Node *m = wq.at(next);
2905         for (DUIterator_Fast imax, i = m-&gt;fast_outs(imax); i &lt; imax; i++) {
2906           Node* use = m-&gt;fast_out(i);
2907           if (use-&gt;is_Mem() || use-&gt;is_EncodeNarrowPtr()) {
2908             use-&gt;ensure_control_or_add_prec(n-&gt;in(0));
2909           } else {
2910             switch(use-&gt;Opcode()) {
2911             case Op_AddP:
2912             case Op_DecodeN:
2913             case Op_DecodeNKlass:
2914             case Op_CheckCastPP:
2915             case Op_CastPP:
2916               wq.push(use);
2917               break;
2918             }
2919           }
2920         }
2921       }
2922     }
2923     const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);
2924     if (is_LP64 &amp;&amp; n-&gt;in(1)-&gt;is_DecodeN() &amp;&amp; Matcher::gen_narrow_oop_implicit_null_checks()) {
2925       Node* in1 = n-&gt;in(1);
2926       const Type* t = n-&gt;bottom_type();
2927       Node* new_in1 = in1-&gt;clone();
2928       new_in1-&gt;as_DecodeN()-&gt;set_type(t);
2929 
2930       if (!Matcher::narrow_oop_use_complex_address()) {
2931         //
2932         // x86, ARM and friends can handle 2 adds in addressing mode
2933         // and Matcher can fold a DecodeN node into address by using
2934         // a narrow oop directly and do implicit NULL check in address:
2935         //
2936         // [R12 + narrow_oop_reg&lt;&lt;3 + offset]
2937         // NullCheck narrow_oop_reg
2938         //
2939         // On other platforms (Sparc) we have to keep new DecodeN node and
2940         // use it to do implicit NULL check in address:
2941         //
2942         // decode_not_null narrow_oop_reg, base_reg
2943         // [base_reg + offset]
2944         // NullCheck base_reg
2945         //
2946         // Pin the new DecodeN node to non-null path on these platform (Sparc)
2947         // to keep the information to which NULL check the new DecodeN node
2948         // corresponds to use it as value in implicit_null_check().
2949         //
2950         new_in1-&gt;set_req(0, n-&gt;in(0));
2951       }
2952 
2953       n-&gt;subsume_by(new_in1, this);
2954       if (in1-&gt;outcnt() == 0) {
2955         in1-&gt;disconnect_inputs(NULL, this);
2956       }
2957     } else {
2958       n-&gt;subsume_by(n-&gt;in(1), this);
2959       if (n-&gt;outcnt() == 0) {
2960         n-&gt;disconnect_inputs(NULL, this);
2961       }
2962     }
2963     break;
2964   }
2965 #ifdef _LP64
2966   case Op_CmpP:
2967     // Do this transformation here to preserve CmpPNode::sub() and
2968     // other TypePtr related Ideal optimizations (for example, ptr nullness).
2969     if (n-&gt;in(1)-&gt;is_DecodeNarrowPtr() || n-&gt;in(2)-&gt;is_DecodeNarrowPtr()) {
2970       Node* in1 = n-&gt;in(1);
2971       Node* in2 = n-&gt;in(2);
2972       if (!in1-&gt;is_DecodeNarrowPtr()) {
2973         in2 = in1;
2974         in1 = n-&gt;in(2);
2975       }
2976       assert(in1-&gt;is_DecodeNarrowPtr(), "sanity");
2977 
2978       Node* new_in2 = NULL;
2979       if (in2-&gt;is_DecodeNarrowPtr()) {
2980         assert(in2-&gt;Opcode() == in1-&gt;Opcode(), "must be same node type");
2981         new_in2 = in2-&gt;in(1);
2982       } else if (in2-&gt;Opcode() == Op_ConP) {
2983         const Type* t = in2-&gt;bottom_type();
2984         if (t == TypePtr::NULL_PTR) {
2985           assert(in1-&gt;is_DecodeN(), "compare klass to null?");
2986           // Don't convert CmpP null check into CmpN if compressed
2987           // oops implicit null check is not generated.
2988           // This will allow to generate normal oop implicit null check.
2989           if (Matcher::gen_narrow_oop_implicit_null_checks())
2990             new_in2 = ConNode::make(TypeNarrowOop::NULL_PTR);
2991           //
2992           // This transformation together with CastPP transformation above
2993           // will generated code for implicit NULL checks for compressed oops.
2994           //
2995           // The original code after Optimize()
2996           //
2997           //    LoadN memory, narrow_oop_reg
2998           //    decode narrow_oop_reg, base_reg
2999           //    CmpP base_reg, NULL
3000           //    CastPP base_reg // NotNull
3001           //    Load [base_reg + offset], val_reg
3002           //
3003           // after these transformations will be
3004           //
3005           //    LoadN memory, narrow_oop_reg
3006           //    CmpN narrow_oop_reg, NULL
3007           //    decode_not_null narrow_oop_reg, base_reg
3008           //    Load [base_reg + offset], val_reg
3009           //
3010           // and the uncommon path (== NULL) will use narrow_oop_reg directly
3011           // since narrow oops can be used in debug info now (see the code in
3012           // final_graph_reshaping_walk()).
3013           //
3014           // At the end the code will be matched to
3015           // on x86:
3016           //
3017           //    Load_narrow_oop memory, narrow_oop_reg
3018           //    Load [R12 + narrow_oop_reg&lt;&lt;3 + offset], val_reg
3019           //    NullCheck narrow_oop_reg
3020           //
3021           // and on sparc:
3022           //
3023           //    Load_narrow_oop memory, narrow_oop_reg
3024           //    decode_not_null narrow_oop_reg, base_reg
3025           //    Load [base_reg + offset], val_reg
3026           //    NullCheck base_reg
3027           //
3028         } else if (t-&gt;isa_oopptr()) {
3029           new_in2 = ConNode::make(t-&gt;make_narrowoop());
3030         } else if (t-&gt;isa_klassptr()) {
3031           new_in2 = ConNode::make(t-&gt;make_narrowklass());
3032         }
3033       }
3034       if (new_in2 != NULL) {
3035         Node* cmpN = new CmpNNode(in1-&gt;in(1), new_in2);
3036         n-&gt;subsume_by(cmpN, this);
3037         if (in1-&gt;outcnt() == 0) {
3038           in1-&gt;disconnect_inputs(NULL, this);
3039         }
3040         if (in2-&gt;outcnt() == 0) {
3041           in2-&gt;disconnect_inputs(NULL, this);
3042         }
3043       }
3044     }
3045     break;
3046 
3047   case Op_DecodeN:
3048   case Op_DecodeNKlass:
3049     assert(!n-&gt;in(1)-&gt;is_EncodeNarrowPtr(), "should be optimized out");
3050     // DecodeN could be pinned when it can't be fold into
3051     // an address expression, see the code for Op_CastPP above.
3052     assert(n-&gt;in(0) == NULL || (UseCompressedOops &amp;&amp; !Matcher::narrow_oop_use_complex_address()), "no control");
3053     break;
3054 
3055   case Op_EncodeP:
3056   case Op_EncodePKlass: {
3057     Node* in1 = n-&gt;in(1);
3058     if (in1-&gt;is_DecodeNarrowPtr()) {
3059       n-&gt;subsume_by(in1-&gt;in(1), this);
3060     } else if (in1-&gt;Opcode() == Op_ConP) {
3061       const Type* t = in1-&gt;bottom_type();
3062       if (t == TypePtr::NULL_PTR) {
3063         assert(t-&gt;isa_oopptr(), "null klass?");
3064         n-&gt;subsume_by(ConNode::make(TypeNarrowOop::NULL_PTR), this);
3065       } else if (t-&gt;isa_oopptr()) {
3066         n-&gt;subsume_by(ConNode::make(t-&gt;make_narrowoop()), this);
3067       } else if (t-&gt;isa_klassptr()) {
3068         n-&gt;subsume_by(ConNode::make(t-&gt;make_narrowklass()), this);
3069       }
3070     }
3071     if (in1-&gt;outcnt() == 0) {
3072       in1-&gt;disconnect_inputs(NULL, this);
3073     }
3074     break;
3075   }
3076 
3077   case Op_Proj: {
3078     if (OptimizeStringConcat) {
3079       ProjNode* p = n-&gt;as_Proj();
3080       if (p-&gt;_is_io_use) {
3081         // Separate projections were used for the exception path which
3082         // are normally removed by a late inline.  If it wasn't inlined
3083         // then they will hang around and should just be replaced with
3084         // the original one.
3085         Node* proj = NULL;
3086         // Replace with just one
3087         for (SimpleDUIterator i(p-&gt;in(0)); i.has_next(); i.next()) {
3088           Node *use = i.get();
3089           if (use-&gt;is_Proj() &amp;&amp; p != use &amp;&amp; use-&gt;as_Proj()-&gt;_con == p-&gt;_con) {
3090             proj = use;
3091             break;
3092           }
3093         }
3094         assert(proj != NULL, "must be found");
3095         p-&gt;subsume_by(proj, this);
3096       }
3097     }
3098     break;
3099   }
3100 
3101   case Op_Phi:
3102     if (n-&gt;as_Phi()-&gt;bottom_type()-&gt;isa_narrowoop() || n-&gt;as_Phi()-&gt;bottom_type()-&gt;isa_narrowklass()) {
3103       // The EncodeP optimization may create Phi with the same edges
3104       // for all paths. It is not handled well by Register Allocator.
3105       Node* unique_in = n-&gt;in(1);
3106       assert(unique_in != NULL, "");
3107       uint cnt = n-&gt;req();
3108       for (uint i = 2; i &lt; cnt; i++) {
3109         Node* m = n-&gt;in(i);
3110         assert(m != NULL, "");
3111         if (unique_in != m)
3112           unique_in = NULL;
3113       }
3114       if (unique_in != NULL) {
3115         n-&gt;subsume_by(unique_in, this);
3116       }
3117     }
3118     break;
3119 
3120 #endif
3121 
3122 #ifdef ASSERT
3123   case Op_CastII:
3124     // Verify that all range check dependent CastII nodes were removed.
3125     if (n-&gt;isa_CastII()-&gt;has_range_check()) {
3126       n-&gt;dump(3);
3127       assert(false, "Range check dependent CastII node was not removed");
3128     }
3129     break;
3130 #endif
3131 
3132   case Op_ModI:
3133     if (UseDivMod) {
3134       // Check if a%b and a/b both exist
3135       Node* d = n-&gt;find_similar(Op_DivI);
3136       if (d) {
3137         // Replace them with a fused divmod if supported
3138         if (Matcher::has_match_rule(Op_DivModI)) {
3139           DivModINode* divmod = DivModINode::make(n);
3140           d-&gt;subsume_by(divmod-&gt;div_proj(), this);
3141           n-&gt;subsume_by(divmod-&gt;mod_proj(), this);
3142         } else {
3143           // replace a%b with a-((a/b)*b)
3144           Node* mult = new MulINode(d, d-&gt;in(2));
3145           Node* sub  = new SubINode(d-&gt;in(1), mult);
3146           n-&gt;subsume_by(sub, this);
3147         }
3148       }
3149     }
3150     break;
3151 
3152   case Op_ModL:
3153     if (UseDivMod) {
3154       // Check if a%b and a/b both exist
3155       Node* d = n-&gt;find_similar(Op_DivL);
3156       if (d) {
3157         // Replace them with a fused divmod if supported
3158         if (Matcher::has_match_rule(Op_DivModL)) {
3159           DivModLNode* divmod = DivModLNode::make(n);
3160           d-&gt;subsume_by(divmod-&gt;div_proj(), this);
3161           n-&gt;subsume_by(divmod-&gt;mod_proj(), this);
3162         } else {
3163           // replace a%b with a-((a/b)*b)
3164           Node* mult = new MulLNode(d, d-&gt;in(2));
3165           Node* sub  = new SubLNode(d-&gt;in(1), mult);
3166           n-&gt;subsume_by(sub, this);
3167         }
3168       }
3169     }
3170     break;
3171 
3172   case Op_LoadVector:
3173   case Op_StoreVector:
3174     break;
3175 
3176   case Op_AddReductionVI:
3177   case Op_AddReductionVL:
3178   case Op_AddReductionVF:
3179   case Op_AddReductionVD:
3180   case Op_MulReductionVI:
3181   case Op_MulReductionVL:
3182   case Op_MulReductionVF:
3183   case Op_MulReductionVD:
3184     break;
3185 
3186   case Op_PackB:
3187   case Op_PackS:
3188   case Op_PackI:
3189   case Op_PackF:
3190   case Op_PackL:
3191   case Op_PackD:
3192     if (n-&gt;req()-1 &gt; 2) {
3193       // Replace many operand PackNodes with a binary tree for matching
3194       PackNode* p = (PackNode*) n;
3195       Node* btp = p-&gt;binary_tree_pack(1, n-&gt;req());
3196       n-&gt;subsume_by(btp, this);
3197     }
3198     break;
3199   case Op_Loop:
3200   case Op_CountedLoop:
3201     if (n-&gt;as_Loop()-&gt;is_inner_loop()) {
3202       frc.inc_inner_loop_count();
3203     }
3204     break;
3205   case Op_LShiftI:
3206   case Op_RShiftI:
3207   case Op_URShiftI:
3208   case Op_LShiftL:
3209   case Op_RShiftL:
3210   case Op_URShiftL:
3211     if (Matcher::need_masked_shift_count) {
3212       // The cpu's shift instructions don't restrict the count to the
3213       // lower 5/6 bits. We need to do the masking ourselves.
3214       Node* in2 = n-&gt;in(2);
3215       juint mask = (n-&gt;bottom_type() == TypeInt::INT) ? (BitsPerInt - 1) : (BitsPerLong - 1);
3216       const TypeInt* t = in2-&gt;find_int_type();
3217       if (t != NULL &amp;&amp; t-&gt;is_con()) {
3218         juint shift = t-&gt;get_con();
3219         if (shift &gt; mask) { // Unsigned cmp
3220           n-&gt;set_req(2, ConNode::make(TypeInt::make(shift &amp; mask)));
3221         }
3222       } else {
3223         if (t == NULL || t-&gt;_lo &lt; 0 || t-&gt;_hi &gt; (int)mask) {
3224           Node* shift = new AndINode(in2, ConNode::make(TypeInt::make(mask)));
3225           n-&gt;set_req(2, shift);
3226         }
3227       }
3228       if (in2-&gt;outcnt() == 0) { // Remove dead node
3229         in2-&gt;disconnect_inputs(NULL, this);
3230       }
3231     }
3232     break;
3233   case Op_MemBarStoreStore:
3234   case Op_MemBarRelease:
3235     // Break the link with AllocateNode: it is no longer useful and
3236     // confuses register allocation.
3237     if (n-&gt;req() &gt; MemBarNode::Precedent) {
3238       n-&gt;set_req(MemBarNode::Precedent, top());
3239     }
3240     break;
3241   case Op_RangeCheck: {
3242     RangeCheckNode* rc = n-&gt;as_RangeCheck();
3243     Node* iff = new IfNode(rc-&gt;in(0), rc-&gt;in(1), rc-&gt;_prob, rc-&gt;_fcnt);
3244     n-&gt;subsume_by(iff, this);
3245     frc._tests.push(iff);
3246     break;
3247   }
3248   default:
3249     assert( !n-&gt;is_Call(), "" );
3250     assert( !n-&gt;is_Mem(), "" );
3251     assert( nop != Op_ProfileBoolean, "should be eliminated during IGVN");
3252     break;
3253   }
3254 
3255   // Collect CFG split points
3256   if (n-&gt;is_MultiBranch() &amp;&amp; !n-&gt;is_RangeCheck()) {
3257     frc._tests.push(n);
3258   }
3259 }
3260 
3261 //------------------------------final_graph_reshaping_walk---------------------
3262 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3263 // requires that the walk visits a node's inputs before visiting the node.
3264 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3265   ResourceArea *area = Thread::current()-&gt;resource_area();
3266   Unique_Node_List sfpt(area);
3267 
3268   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3269   uint cnt = root-&gt;req();
3270   Node *n = root;
3271   uint  i = 0;
3272   while (true) {
3273     if (i &lt; cnt) {
3274       // Place all non-visited non-null inputs onto stack
3275       Node* m = n-&gt;in(i);
3276       ++i;
3277       if (m != NULL &amp;&amp; !frc._visited.test_set(m-&gt;_idx)) {
3278         if (m-&gt;is_SafePoint() &amp;&amp; m-&gt;as_SafePoint()-&gt;jvms() != NULL) {
3279           // compute worst case interpreter size in case of a deoptimization
3280           update_interpreter_frame_size(m-&gt;as_SafePoint()-&gt;jvms()-&gt;interpreter_frame_size());
3281 
3282           sfpt.push(m);
3283         }
3284         cnt = m-&gt;req();
3285         nstack.push(n, i); // put on stack parent and next input's index
3286         n = m;
3287         i = 0;
3288       }
3289     } else {
3290       // Now do post-visit work
3291       final_graph_reshaping_impl( n, frc );
3292       if (nstack.is_empty())
3293         break;             // finished
3294       n = nstack.node();   // Get node from stack
3295       cnt = n-&gt;req();
3296       i = nstack.index();
3297       nstack.pop();        // Shift to the next node on stack
3298     }
3299   }
3300 
3301   // Skip next transformation if compressed oops are not used.
3302   if ((UseCompressedOops &amp;&amp; !Matcher::gen_narrow_oop_implicit_null_checks()) ||
3303       (!UseCompressedOops &amp;&amp; !UseCompressedClassPointers))
3304     return;
3305 
3306   // Go over safepoints nodes to skip DecodeN/DecodeNKlass nodes for debug edges.
3307   // It could be done for an uncommon traps or any safepoints/calls
3308   // if the DecodeN/DecodeNKlass node is referenced only in a debug info.
3309   while (sfpt.size() &gt; 0) {
3310     n = sfpt.pop();
3311     JVMState *jvms = n-&gt;as_SafePoint()-&gt;jvms();
3312     assert(jvms != NULL, "sanity");
3313     int start = jvms-&gt;debug_start();
3314     int end   = n-&gt;req();
3315     bool is_uncommon = (n-&gt;is_CallStaticJava() &amp;&amp;
3316                         n-&gt;as_CallStaticJava()-&gt;uncommon_trap_request() != 0);
3317     for (int j = start; j &lt; end; j++) {
3318       Node* in = n-&gt;in(j);
3319       if (in-&gt;is_DecodeNarrowPtr()) {
3320         bool safe_to_skip = true;
3321         if (!is_uncommon ) {
3322           // Is it safe to skip?
3323           for (uint i = 0; i &lt; in-&gt;outcnt(); i++) {
3324             Node* u = in-&gt;raw_out(i);
3325             if (!u-&gt;is_SafePoint() ||
3326                  u-&gt;is_Call() &amp;&amp; u-&gt;as_Call()-&gt;has_non_debug_use(n)) {
3327               safe_to_skip = false;
3328             }
3329           }
3330         }
3331         if (safe_to_skip) {
3332           n-&gt;set_req(j, in-&gt;in(1));
3333         }
3334         if (in-&gt;outcnt() == 0) {
3335           in-&gt;disconnect_inputs(NULL, this);
3336         }
3337       }
3338     }
3339   }
3340 }
3341 
3342 //------------------------------final_graph_reshaping--------------------------
3343 // Final Graph Reshaping.
3344 //
3345 // (1) Clone simple inputs to uncommon calls, so they can be scheduled late
3346 //     and not commoned up and forced early.  Must come after regular
3347 //     optimizations to avoid GVN undoing the cloning.  Clone constant
3348 //     inputs to Loop Phis; these will be split by the allocator anyways.
3349 //     Remove Opaque nodes.
3350 // (2) Move last-uses by commutative operations to the left input to encourage
3351 //     Intel update-in-place two-address operations and better register usage
3352 //     on RISCs.  Must come after regular optimizations to avoid GVN Ideal
3353 //     calls canonicalizing them back.
3354 // (3) Count the number of double-precision FP ops, single-precision FP ops
3355 //     and call sites.  On Intel, we can get correct rounding either by
3356 //     forcing singles to memory (requires extra stores and loads after each
3357 //     FP bytecode) or we can set a rounding mode bit (requires setting and
3358 //     clearing the mode bit around call sites).  The mode bit is only used
3359 //     if the relative frequency of single FP ops to calls is low enough.
3360 //     This is a key transform for SPEC mpeg_audio.
3361 // (4) Detect infinite loops; blobs of code reachable from above but not
3362 //     below.  Several of the Code_Gen algorithms fail on such code shapes,
3363 //     so we simply bail out.  Happens a lot in ZKM.jar, but also happens
3364 //     from time to time in other codes (such as -Xcomp finalizer loops, etc).
3365 //     Detection is by looking for IfNodes where only 1 projection is
3366 //     reachable from below or CatchNodes missing some targets.
3367 // (5) Assert for insane oop offsets in debug mode.
3368 
3369 bool Compile::final_graph_reshaping() {
3370   // an infinite loop may have been eliminated by the optimizer,
3371   // in which case the graph will be empty.
3372   if (root()-&gt;req() == 1) {
3373     record_method_not_compilable("trivial infinite loop");
3374     return true;
3375   }
3376 
3377   // Expensive nodes have their control input set to prevent the GVN
3378   // from freely commoning them. There's no GVN beyond this point so
3379   // no need to keep the control input. We want the expensive nodes to
3380   // be freely moved to the least frequent code path by gcm.
3381   assert(OptimizeExpensiveOps || expensive_count() == 0, "optimization off but list non empty?");
3382   for (int i = 0; i &lt; expensive_count(); i++) {
3383     _expensive_nodes-&gt;at(i)-&gt;set_req(0, NULL);
3384   }
3385 
3386   Final_Reshape_Counts frc;
3387 
3388   // Visit everybody reachable!
3389   // Allocate stack of size C-&gt;live_nodes()/2 to avoid frequent realloc
3390   Node_Stack nstack(live_nodes() &gt;&gt; 1);
3391   final_graph_reshaping_walk(nstack, root(), frc);
3392 
3393   // Check for unreachable (from below) code (i.e., infinite loops).
3394   for( uint i = 0; i &lt; frc._tests.size(); i++ ) {
3395     MultiBranchNode *n = frc._tests[i]-&gt;as_MultiBranch();
3396     // Get number of CFG targets.
3397     // Note that PCTables include exception targets after calls.
3398     uint required_outcnt = n-&gt;required_outcnt();
3399     if (n-&gt;outcnt() != required_outcnt) {
3400       // Check for a few special cases.  Rethrow Nodes never take the
3401       // 'fall-thru' path, so expected kids is 1 less.
3402       if (n-&gt;is_PCTable() &amp;&amp; n-&gt;in(0) &amp;&amp; n-&gt;in(0)-&gt;in(0)) {
3403         if (n-&gt;in(0)-&gt;in(0)-&gt;is_Call()) {
3404           CallNode *call = n-&gt;in(0)-&gt;in(0)-&gt;as_Call();
3405           if (call-&gt;entry_point() == OptoRuntime::rethrow_stub()) {
3406             required_outcnt--;      // Rethrow always has 1 less kid
3407           } else if (call-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
3408                      call-&gt;is_CallDynamicJava()) {
3409             // Check for null receiver. In such case, the optimizer has
3410             // detected that the virtual call will always result in a null
3411             // pointer exception. The fall-through projection of this CatchNode
3412             // will not be populated.
3413             Node *arg0 = call-&gt;in(TypeFunc::Parms);
3414             if (arg0-&gt;is_Type() &amp;&amp;
3415                 arg0-&gt;as_Type()-&gt;type()-&gt;higher_equal(TypePtr::NULL_PTR)) {
3416               required_outcnt--;
3417             }
3418           } else if (call-&gt;entry_point() == OptoRuntime::new_array_Java() &amp;&amp;
3419                      call-&gt;req() &gt; TypeFunc::Parms+1 &amp;&amp;
3420                      call-&gt;is_CallStaticJava()) {
3421             // Check for negative array length. In such case, the optimizer has
3422             // detected that the allocation attempt will always result in an
3423             // exception. There is no fall-through projection of this CatchNode .
3424             Node *arg1 = call-&gt;in(TypeFunc::Parms+1);
3425             if (arg1-&gt;is_Type() &amp;&amp;
3426                 arg1-&gt;as_Type()-&gt;type()-&gt;join(TypeInt::POS)-&gt;empty()) {
3427               required_outcnt--;
3428             }
3429           }
3430         }
3431       }
3432       // Recheck with a better notion of 'required_outcnt'
3433       if (n-&gt;outcnt() != required_outcnt) {
3434         record_method_not_compilable("malformed control flow");
3435         return true;            // Not all targets reachable!
3436       }
3437     }
3438     // Check that I actually visited all kids.  Unreached kids
3439     // must be infinite loops.
3440     for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++)
3441       if (!frc._visited.test(n-&gt;fast_out(j)-&gt;_idx)) {
3442         record_method_not_compilable("infinite loop");
3443         return true;            // Found unvisited kid; must be unreach
3444       }
3445   }
3446 
3447   // If original bytecodes contained a mixture of floats and doubles
3448   // check if the optimizer has made it homogenous, item (3).
3449   if( Use24BitFPMode &amp;&amp; Use24BitFP &amp;&amp; UseSSE == 0 &amp;&amp;
3450       frc.get_float_count() &gt; 32 &amp;&amp;
3451       frc.get_double_count() == 0 &amp;&amp;
3452       (10 * frc.get_call_count() &lt; frc.get_float_count()) ) {
3453     set_24_bit_selection_and_mode( false,  true );
3454   }
3455 
3456   set_java_calls(frc.get_java_call_count());
3457   set_inner_loops(frc.get_inner_loop_count());
3458 
3459   // No infinite loops, no reason to bail out.
3460   return false;
3461 }
3462 
3463 //-----------------------------too_many_traps----------------------------------
3464 // Report if there are too many traps at the current method and bci.
3465 // Return true if there was a trap, and/or PerMethodTrapLimit is exceeded.
3466 bool Compile::too_many_traps(ciMethod* method,
3467                              int bci,
3468                              Deoptimization::DeoptReason reason) {
3469   ciMethodData* md = method-&gt;method_data();
3470   if (md-&gt;is_empty()) {
3471     // Assume the trap has not occurred, or that it occurred only
3472     // because of a transient condition during start-up in the interpreter.
3473     return false;
3474   }
3475   ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this-&gt;method() : NULL;
3476   if (md-&gt;has_trap_at(bci, m, reason) != 0) {
3477     // Assume PerBytecodeTrapLimit==0, for a more conservative heuristic.
3478     // Also, if there are multiple reasons, or if there is no per-BCI record,
3479     // assume the worst.
3480     if (log())
3481       log()-&gt;elem("observe trap='%s' count='%d'",
3482                   Deoptimization::trap_reason_name(reason),
3483                   md-&gt;trap_count(reason));
3484     return true;
3485   } else {
3486     // Ignore method/bci and see if there have been too many globally.
3487     return too_many_traps(reason, md);
3488   }
3489 }
3490 
3491 // Less-accurate variant which does not require a method and bci.
3492 bool Compile::too_many_traps(Deoptimization::DeoptReason reason,
3493                              ciMethodData* logmd) {
3494   if (trap_count(reason) &gt;= Deoptimization::per_method_trap_limit(reason)) {
3495     // Too many traps globally.
3496     // Note that we use cumulative trap_count, not just md-&gt;trap_count.
3497     if (log()) {
3498       int mcount = (logmd == NULL)? -1: (int)logmd-&gt;trap_count(reason);
3499       log()-&gt;elem("observe trap='%s' count='0' mcount='%d' ccount='%d'",
3500                   Deoptimization::trap_reason_name(reason),
3501                   mcount, trap_count(reason));
3502     }
3503     return true;
3504   } else {
3505     // The coast is clear.
3506     return false;
3507   }
3508 }
3509 
3510 //--------------------------too_many_recompiles--------------------------------
3511 // Report if there are too many recompiles at the current method and bci.
3512 // Consults PerBytecodeRecompilationCutoff and PerMethodRecompilationCutoff.
3513 // Is not eager to return true, since this will cause the compiler to use
3514 // Action_none for a trap point, to avoid too many recompilations.
3515 bool Compile::too_many_recompiles(ciMethod* method,
3516                                   int bci,
3517                                   Deoptimization::DeoptReason reason) {
3518   ciMethodData* md = method-&gt;method_data();
3519   if (md-&gt;is_empty()) {
3520     // Assume the trap has not occurred, or that it occurred only
3521     // because of a transient condition during start-up in the interpreter.
3522     return false;
3523   }
3524   // Pick a cutoff point well within PerBytecodeRecompilationCutoff.
3525   uint bc_cutoff = (uint) PerBytecodeRecompilationCutoff / 8;
3526   uint m_cutoff  = (uint) PerMethodRecompilationCutoff / 2 + 1;  // not zero
3527   Deoptimization::DeoptReason per_bc_reason
3528     = Deoptimization::reason_recorded_per_bytecode_if_any(reason);
3529   ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this-&gt;method() : NULL;
3530   if ((per_bc_reason == Deoptimization::Reason_none
3531        || md-&gt;has_trap_at(bci, m, reason) != 0)
3532       // The trap frequency measure we care about is the recompile count:
3533       &amp;&amp; md-&gt;trap_recompiled_at(bci, m)
3534       &amp;&amp; md-&gt;overflow_recompile_count() &gt;= bc_cutoff) {
3535     // Do not emit a trap here if it has already caused recompilations.
3536     // Also, if there are multiple reasons, or if there is no per-BCI record,
3537     // assume the worst.
3538     if (log())
3539       log()-&gt;elem("observe trap='%s recompiled' count='%d' recompiles2='%d'",
3540                   Deoptimization::trap_reason_name(reason),
3541                   md-&gt;trap_count(reason),
3542                   md-&gt;overflow_recompile_count());
3543     return true;
3544   } else if (trap_count(reason) != 0
3545              &amp;&amp; decompile_count() &gt;= m_cutoff) {
3546     // Too many recompiles globally, and we have seen this sort of trap.
3547     // Use cumulative decompile_count, not just md-&gt;decompile_count.
3548     if (log())
3549       log()-&gt;elem("observe trap='%s' count='%d' mcount='%d' decompiles='%d' mdecompiles='%d'",
3550                   Deoptimization::trap_reason_name(reason),
3551                   md-&gt;trap_count(reason), trap_count(reason),
3552                   md-&gt;decompile_count(), decompile_count());
3553     return true;
3554   } else {
3555     // The coast is clear.
3556     return false;
3557   }
3558 }
3559 
3560 // Compute when not to trap. Used by matching trap based nodes and
3561 // NullCheck optimization.
3562 void Compile::set_allowed_deopt_reasons() {
3563   _allowed_reasons = 0;
3564   if (is_method_compilation()) {
3565     for (int rs = (int)Deoptimization::Reason_none+1; rs &lt; Compile::trapHistLength; rs++) {
3566       assert(rs &lt; BitsPerInt, "recode bit map");
3567       if (!too_many_traps((Deoptimization::DeoptReason) rs)) {
3568         _allowed_reasons |= nth_bit(rs);
3569       }
3570     }
3571   }
3572 }
3573 
3574 #ifndef PRODUCT
3575 //------------------------------verify_graph_edges---------------------------
3576 // Walk the Graph and verify that there is a one-to-one correspondence
3577 // between Use-Def edges and Def-Use edges in the graph.
3578 void Compile::verify_graph_edges(bool no_dead_code) {
3579   if (VerifyGraphEdges) {
3580     ResourceArea *area = Thread::current()-&gt;resource_area();
3581     Unique_Node_List visited(area);
3582     // Call recursive graph walk to check edges
3583     _root-&gt;verify_edges(visited);
3584     if (no_dead_code) {
3585       // Now make sure that no visited node is used by an unvisited node.
3586       bool dead_nodes = false;
3587       Unique_Node_List checked(area);
3588       while (visited.size() &gt; 0) {
3589         Node* n = visited.pop();
3590         checked.push(n);
3591         for (uint i = 0; i &lt; n-&gt;outcnt(); i++) {
3592           Node* use = n-&gt;raw_out(i);
3593           if (checked.member(use))  continue;  // already checked
3594           if (visited.member(use))  continue;  // already in the graph
3595           if (use-&gt;is_Con())        continue;  // a dead ConNode is OK
3596           // At this point, we have found a dead node which is DU-reachable.
3597           if (!dead_nodes) {
3598             tty-&gt;print_cr("*** Dead nodes reachable via DU edges:");
3599             dead_nodes = true;
3600           }
3601           use-&gt;dump(2);
3602           tty-&gt;print_cr("---");
3603           checked.push(use);  // No repeats; pretend it is now checked.
3604         }
3605       }
3606       assert(!dead_nodes, "using nodes must be reachable from root");
3607     }
3608   }
3609 }
3610 
3611 // Verify GC barriers consistency
3612 // Currently supported:
3613 // - G1 pre-barriers (see GraphKit::g1_write_barrier_pre())
3614 void Compile::verify_barriers() {
3615   if (UseG1GC) {
3616     // Verify G1 pre-barriers
3617     const int marking_offset = in_bytes(JavaThread::satb_mark_queue_offset() + SATBMarkQueue::byte_offset_of_active());
3618 
3619     ResourceArea *area = Thread::current()-&gt;resource_area();
3620     Unique_Node_List visited(area);
3621     Node_List worklist(area);
3622     // We're going to walk control flow backwards starting from the Root
3623     worklist.push(_root);
3624     while (worklist.size() &gt; 0) {
3625       Node* x = worklist.pop();
3626       if (x == NULL || x == top()) continue;
3627       if (visited.member(x)) {
3628         continue;
3629       } else {
3630         visited.push(x);
3631       }
3632 
3633       if (x-&gt;is_Region()) {
3634         for (uint i = 1; i &lt; x-&gt;req(); i++) {
3635           worklist.push(x-&gt;in(i));
3636         }
3637       } else {
3638         worklist.push(x-&gt;in(0));
3639         // We are looking for the pattern:
3640         //                            /-&gt;ThreadLocal
3641         // If-&gt;Bool-&gt;CmpI-&gt;LoadB-&gt;AddP-&gt;ConL(marking_offset)
3642         //              \-&gt;ConI(0)
3643         // We want to verify that the If and the LoadB have the same control
3644         // See GraphKit::g1_write_barrier_pre()
3645         if (x-&gt;is_If()) {
3646           IfNode *iff = x-&gt;as_If();
3647           if (iff-&gt;in(1)-&gt;is_Bool() &amp;&amp; iff-&gt;in(1)-&gt;in(1)-&gt;is_Cmp()) {
3648             CmpNode *cmp = iff-&gt;in(1)-&gt;in(1)-&gt;as_Cmp();
3649             if (cmp-&gt;Opcode() == Op_CmpI &amp;&amp; cmp-&gt;in(2)-&gt;is_Con() &amp;&amp; cmp-&gt;in(2)-&gt;bottom_type()-&gt;is_int()-&gt;get_con() == 0
3650                 &amp;&amp; cmp-&gt;in(1)-&gt;is_Load()) {
3651               LoadNode* load = cmp-&gt;in(1)-&gt;as_Load();
3652               if (load-&gt;Opcode() == Op_LoadB &amp;&amp; load-&gt;in(2)-&gt;is_AddP() &amp;&amp; load-&gt;in(2)-&gt;in(2)-&gt;Opcode() == Op_ThreadLocal
3653                   &amp;&amp; load-&gt;in(2)-&gt;in(3)-&gt;is_Con()
3654                   &amp;&amp; load-&gt;in(2)-&gt;in(3)-&gt;bottom_type()-&gt;is_intptr_t()-&gt;get_con() == marking_offset) {
3655 
3656                 Node* if_ctrl = iff-&gt;in(0);
3657                 Node* load_ctrl = load-&gt;in(0);
3658 
3659                 if (if_ctrl != load_ctrl) {
3660                   // Skip possible CProj-&gt;NeverBranch in infinite loops
3661                   if ((if_ctrl-&gt;is_Proj() &amp;&amp; if_ctrl-&gt;Opcode() == Op_CProj)
3662                       &amp;&amp; (if_ctrl-&gt;in(0)-&gt;is_MultiBranch() &amp;&amp; if_ctrl-&gt;in(0)-&gt;Opcode() == Op_NeverBranch)) {
3663                     if_ctrl = if_ctrl-&gt;in(0)-&gt;in(0);
3664                   }
3665                 }
3666                 assert(load_ctrl != NULL &amp;&amp; if_ctrl == load_ctrl, "controls must match");
3667               }
3668             }
3669           }
3670         }
3671       }
3672     }
3673   }
3674 }
3675 
3676 #endif
3677 
3678 // The Compile object keeps track of failure reasons separately from the ciEnv.
3679 // This is required because there is not quite a 1-1 relation between the
3680 // ciEnv and its compilation task and the Compile object.  Note that one
3681 // ciEnv might use two Compile objects, if C2Compiler::compile_method decides
3682 // to backtrack and retry without subsuming loads.  Other than this backtracking
3683 // behavior, the Compile's failure reason is quietly copied up to the ciEnv
3684 // by the logic in C2Compiler.
3685 void Compile::record_failure(const char* reason) {
3686   if (log() != NULL) {
3687     log()-&gt;elem("failure reason='%s' phase='compile'", reason);
3688   }
3689   if (_failure_reason == NULL) {
3690     // Record the first failure reason.
3691     _failure_reason = reason;
3692   }
3693 
3694   if (!C-&gt;failure_reason_is(C2Compiler::retry_no_subsuming_loads())) {
3695     C-&gt;print_method(PHASE_FAILURE);
3696   }
3697   _root = NULL;  // flush the graph, too
3698 }
3699 
3700 Compile::TracePhase::TracePhase(const char* name, elapsedTimer* accumulator)
3701   : TraceTime(name, accumulator, CITime, CITimeVerbose),
3702     _phase_name(name), _dolog(CITimeVerbose)
3703 {
3704   if (_dolog) {
3705     C = Compile::current();
3706     _log = C-&gt;log();
3707   } else {
3708     C = NULL;
3709     _log = NULL;
3710   }
3711   if (_log != NULL) {
3712     _log-&gt;begin_head("phase name='%s' nodes='%d' live='%d'", _phase_name, C-&gt;unique(), C-&gt;live_nodes());
3713     _log-&gt;stamp();
3714     _log-&gt;end_head();
3715   }
3716 }
3717 
3718 Compile::TracePhase::~TracePhase() {
3719 
3720   C = Compile::current();
3721   if (_dolog) {
3722     _log = C-&gt;log();
3723   } else {
3724     _log = NULL;
3725   }
3726 
3727 #ifdef ASSERT
3728   if (PrintIdealNodeCount) {
3729     tty-&gt;print_cr("phase name='%s' nodes='%d' live='%d' live_graph_walk='%d'",
3730                   _phase_name, C-&gt;unique(), C-&gt;live_nodes(), C-&gt;count_live_nodes_by_graph_walk());
3731   }
3732 
3733   if (VerifyIdealNodeCount) {
3734     Compile::current()-&gt;print_missing_nodes();
3735   }
3736 #endif
3737 
3738   if (_log != NULL) {
3739     _log-&gt;done("phase name='%s' nodes='%d' live='%d'", _phase_name, C-&gt;unique(), C-&gt;live_nodes());
3740   }
3741 }
3742 
3743 //=============================================================================
3744 // Two Constant's are equal when the type and the value are equal.
3745 bool Compile::Constant::operator==(const Constant&amp; other) {
3746   if (type()          != other.type()         )  return false;
3747   if (can_be_reused() != other.can_be_reused())  return false;
3748   // For floating point values we compare the bit pattern.
3749   switch (type()) {
3750   case T_FLOAT:   return (_v._value.i == other._v._value.i);
3751   case T_LONG:
3752   case T_DOUBLE:  return (_v._value.j == other._v._value.j);
3753   case T_OBJECT:
3754   case T_ADDRESS: return (_v._value.l == other._v._value.l);
3755   case T_VOID:    return (_v._value.l == other._v._value.l);  // jump-table entries
3756   case T_METADATA: return (_v._metadata == other._v._metadata);
3757   default: ShouldNotReachHere();
3758   }
3759   return false;
3760 }
3761 
3762 static int type_to_size_in_bytes(BasicType t) {
3763   switch (t) {
3764   case T_LONG:    return sizeof(jlong  );
3765   case T_FLOAT:   return sizeof(jfloat );
3766   case T_DOUBLE:  return sizeof(jdouble);
3767   case T_METADATA: return sizeof(Metadata*);
3768     // We use T_VOID as marker for jump-table entries (labels) which
3769     // need an internal word relocation.
3770   case T_VOID:
3771   case T_ADDRESS:
3772   case T_OBJECT:  return sizeof(jobject);
3773   }
3774 
3775   ShouldNotReachHere();
3776   return -1;
3777 }
3778 
3779 int Compile::ConstantTable::qsort_comparator(Constant* a, Constant* b) {
3780   // sort descending
3781   if (a-&gt;freq() &gt; b-&gt;freq())  return -1;
3782   if (a-&gt;freq() &lt; b-&gt;freq())  return  1;
3783   return 0;
3784 }
3785 
3786 void Compile::ConstantTable::calculate_offsets_and_size() {
3787   // First, sort the array by frequencies.
3788   _constants.sort(qsort_comparator);
3789 
3790 #ifdef ASSERT
3791   // Make sure all jump-table entries were sorted to the end of the
3792   // array (they have a negative frequency).
3793   bool found_void = false;
3794   for (int i = 0; i &lt; _constants.length(); i++) {
3795     Constant con = _constants.at(i);
3796     if (con.type() == T_VOID)
3797       found_void = true;  // jump-tables
3798     else
3799       assert(!found_void, "wrong sorting");
3800   }
3801 #endif
3802 
3803   int offset = 0;
3804   for (int i = 0; i &lt; _constants.length(); i++) {
3805     Constant* con = _constants.adr_at(i);
3806 
3807     // Align offset for type.
3808     int typesize = type_to_size_in_bytes(con-&gt;type());
3809     offset = align_size_up(offset, typesize);
3810     con-&gt;set_offset(offset);   // set constant's offset
3811 
3812     if (con-&gt;type() == T_VOID) {
3813       MachConstantNode* n = (MachConstantNode*) con-&gt;get_jobject();
3814       offset = offset + typesize * n-&gt;outcnt();  // expand jump-table
3815     } else {
3816       offset = offset + typesize;
3817     }
3818   }
3819 
3820   // Align size up to the next section start (which is insts; see
3821   // CodeBuffer::align_at_start).
3822   assert(_size == -1, "already set?");
3823   _size = align_size_up(offset, CodeEntryAlignment);
3824 }
3825 
3826 void Compile::ConstantTable::emit(CodeBuffer&amp; cb) {
3827   MacroAssembler _masm(&amp;cb);
3828   for (int i = 0; i &lt; _constants.length(); i++) {
3829     Constant con = _constants.at(i);
3830     address constant_addr = NULL;
3831     switch (con.type()) {
3832     case T_LONG:   constant_addr = _masm.long_constant(  con.get_jlong()  ); break;
3833     case T_FLOAT:  constant_addr = _masm.float_constant( con.get_jfloat() ); break;
3834     case T_DOUBLE: constant_addr = _masm.double_constant(con.get_jdouble()); break;
3835     case T_OBJECT: {
3836       jobject obj = con.get_jobject();
3837       int oop_index = _masm.oop_recorder()-&gt;find_index(obj);
3838       constant_addr = _masm.address_constant((address) obj, oop_Relocation::spec(oop_index));
3839       break;
3840     }
3841     case T_ADDRESS: {
3842       address addr = (address) con.get_jobject();
3843       constant_addr = _masm.address_constant(addr);
3844       break;
3845     }
3846     // We use T_VOID as marker for jump-table entries (labels) which
3847     // need an internal word relocation.
3848     case T_VOID: {
3849       MachConstantNode* n = (MachConstantNode*) con.get_jobject();
3850       // Fill the jump-table with a dummy word.  The real value is
3851       // filled in later in fill_jump_table.
3852       address dummy = (address) n;
3853       constant_addr = _masm.address_constant(dummy);
3854       // Expand jump-table
3855       for (uint i = 1; i &lt; n-&gt;outcnt(); i++) {
3856         address temp_addr = _masm.address_constant(dummy + i);
3857         assert(temp_addr, "consts section too small");
3858       }
3859       break;
3860     }
3861     case T_METADATA: {
3862       Metadata* obj = con.get_metadata();
3863       int metadata_index = _masm.oop_recorder()-&gt;find_index(obj);
3864       constant_addr = _masm.address_constant((address) obj, metadata_Relocation::spec(metadata_index));
3865       break;
3866     }
3867     default: ShouldNotReachHere();
3868     }
3869     assert(constant_addr, "consts section too small");
3870     assert((constant_addr - _masm.code()-&gt;consts()-&gt;start()) == con.offset(),
3871             "must be: %d == %d", (int) (constant_addr - _masm.code()-&gt;consts()-&gt;start()), (int)(con.offset()));
3872   }
3873 }
3874 
3875 int Compile::ConstantTable::find_offset(Constant&amp; con) const {
3876   int idx = _constants.find(con);
3877   assert(idx != -1, "constant must be in constant table");
3878   int offset = _constants.at(idx).offset();
3879   assert(offset != -1, "constant table not emitted yet?");
3880   return offset;
3881 }
3882 
3883 void Compile::ConstantTable::add(Constant&amp; con) {
3884   if (con.can_be_reused()) {
3885     int idx = _constants.find(con);
3886     if (idx != -1 &amp;&amp; _constants.at(idx).can_be_reused()) {
3887       _constants.adr_at(idx)-&gt;inc_freq(con.freq());  // increase the frequency by the current value
3888       return;
3889     }
3890   }
3891   (void) _constants.append(con);
3892 }
3893 
3894 Compile::Constant Compile::ConstantTable::add(MachConstantNode* n, BasicType type, jvalue value) {
3895   Block* b = Compile::current()-&gt;cfg()-&gt;get_block_for_node(n);
3896   Constant con(type, value, b-&gt;_freq);
3897   add(con);
3898   return con;
3899 }
3900 
3901 Compile::Constant Compile::ConstantTable::add(Metadata* metadata) {
3902   Constant con(metadata);
3903   add(con);
3904   return con;
3905 }
3906 
3907 Compile::Constant Compile::ConstantTable::add(MachConstantNode* n, MachOper* oper) {
3908   jvalue value;
3909   BasicType type = oper-&gt;type()-&gt;basic_type();
3910   switch (type) {
3911   case T_LONG:    value.j = oper-&gt;constantL(); break;
3912   case T_FLOAT:   value.f = oper-&gt;constantF(); break;
3913   case T_DOUBLE:  value.d = oper-&gt;constantD(); break;
3914   case T_OBJECT:
3915   case T_ADDRESS: value.l = (jobject) oper-&gt;constant(); break;
3916   case T_METADATA: return add((Metadata*)oper-&gt;constant()); break;
3917   default: guarantee(false, "unhandled type: %s", type2name(type));
3918   }
3919   return add(n, type, value);
3920 }
3921 
3922 Compile::Constant Compile::ConstantTable::add_jump_table(MachConstantNode* n) {
3923   jvalue value;
3924   // We can use the node pointer here to identify the right jump-table
3925   // as this method is called from Compile::Fill_buffer right before
3926   // the MachNodes are emitted and the jump-table is filled (means the
3927   // MachNode pointers do not change anymore).
3928   value.l = (jobject) n;
3929   Constant con(T_VOID, value, next_jump_table_freq(), false);  // Labels of a jump-table cannot be reused.
3930   add(con);
3931   return con;
3932 }
3933 
3934 void Compile::ConstantTable::fill_jump_table(CodeBuffer&amp; cb, MachConstantNode* n, GrowableArray&lt;Label*&gt; labels) const {
3935   // If called from Compile::scratch_emit_size do nothing.
3936   if (Compile::current()-&gt;in_scratch_emit_size())  return;
3937 
3938   assert(labels.is_nonempty(), "must be");
3939   assert((uint) labels.length() == n-&gt;outcnt(), "must be equal: %d == %d", labels.length(), n-&gt;outcnt());
3940 
3941   // Since MachConstantNode::constant_offset() also contains
3942   // table_base_offset() we need to subtract the table_base_offset()
3943   // to get the plain offset into the constant table.
3944   int offset = n-&gt;constant_offset() - table_base_offset();
3945 
3946   MacroAssembler _masm(&amp;cb);
3947   address* jump_table_base = (address*) (_masm.code()-&gt;consts()-&gt;start() + offset);
3948 
3949   for (uint i = 0; i &lt; n-&gt;outcnt(); i++) {
3950     address* constant_addr = &amp;jump_table_base[i];
3951     assert(*constant_addr == (((address) n) + i), "all jump-table entries must contain adjusted node pointer: " INTPTR_FORMAT " == " INTPTR_FORMAT, p2i(*constant_addr), p2i(((address) n) + i));
3952     *constant_addr = cb.consts()-&gt;target(*labels.at(i), (address) constant_addr);
3953     cb.consts()-&gt;relocate((address) constant_addr, relocInfo::internal_word_type);
3954   }
3955 }
3956 
3957 //----------------------------static_subtype_check-----------------------------
3958 // Shortcut important common cases when superklass is exact:
3959 // (0) superklass is java.lang.Object (can occur in reflective code)
3960 // (1) subklass is already limited to a subtype of superklass =&gt; always ok
3961 // (2) subklass does not overlap with superklass =&gt; always fail
3962 // (3) superklass has NO subtypes and we can check with a simple compare.
3963 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
3964   if (StressReflectiveCode) {
3965     return SSC_full_test;       // Let caller generate the general case.
3966   }
3967 
3968   if (superk == env()-&gt;Object_klass()) {
3969     return SSC_always_true;     // (0) this test cannot fail
3970   }
3971 
3972   ciType* superelem = superk;
3973   if (superelem-&gt;is_array_klass())
3974     superelem = superelem-&gt;as_array_klass()-&gt;base_element_type();
3975 
3976   if (!subk-&gt;is_interface()) {  // cannot trust static interface types yet
3977     if (subk-&gt;is_subtype_of(superk)) {
3978       return SSC_always_true;   // (1) false path dead; no dynamic test needed
3979     }
3980     if (!(superelem-&gt;is_klass() &amp;&amp; superelem-&gt;as_klass()-&gt;is_interface()) &amp;&amp;
3981         !superk-&gt;is_subtype_of(subk)) {
3982       return SSC_always_false;
3983     }
3984   }
3985 
3986   // If casting to an instance klass, it must have no subtypes
3987   if (superk-&gt;is_interface()) {
3988     // Cannot trust interfaces yet.
3989     // %%% S.B. superk-&gt;nof_implementors() == 1
3990   } else if (superelem-&gt;is_instance_klass()) {
3991     ciInstanceKlass* ik = superelem-&gt;as_instance_klass();
3992     if (!ik-&gt;has_subklass() &amp;&amp; !ik-&gt;is_interface()) {
3993       if (!ik-&gt;is_final()) {
3994         // Add a dependency if there is a chance of a later subclass.
3995         dependencies()-&gt;assert_leaf_type(ik);
3996       }
3997       return SSC_easy_test;     // (3) caller can do a simple ptr comparison
3998     }
3999   } else {
4000     // A primitive array type has no subtypes.
4001     return SSC_easy_test;       // (3) caller can do a simple ptr comparison
4002   }
4003 
4004   return SSC_full_test;
4005 }
4006 
4007 Node* Compile::conv_I2X_index(PhaseGVN* phase, Node* idx, const TypeInt* sizetype, Node* ctrl) {
4008 #ifdef _LP64
4009   // The scaled index operand to AddP must be a clean 64-bit value.
4010   // Java allows a 32-bit int to be incremented to a negative
4011   // value, which appears in a 64-bit register as a large
4012   // positive number.  Using that large positive number as an
4013   // operand in pointer arithmetic has bad consequences.
4014   // On the other hand, 32-bit overflow is rare, and the possibility
4015   // can often be excluded, if we annotate the ConvI2L node with
4016   // a type assertion that its value is known to be a small positive
4017   // number.  (The prior range check has ensured this.)
4018   // This assertion is used by ConvI2LNode::Ideal.
4019   int index_max = max_jint - 1;  // array size is max_jint, index is one less
4020   if (sizetype != NULL) index_max = sizetype-&gt;_hi - 1;
4021   const TypeInt* iidxtype = TypeInt::make(0, index_max, Type::WidenMax);
4022   idx = constrained_convI2L(phase, idx, iidxtype, ctrl);
4023 #endif
4024   return idx;
4025 }
4026 
4027 // Convert integer value to a narrowed long type dependent on ctrl (for example, a range check)
4028 Node* Compile::constrained_convI2L(PhaseGVN* phase, Node* value, const TypeInt* itype, Node* ctrl) {
4029   if (ctrl != NULL) {
4030     // Express control dependency by a CastII node with a narrow type.
4031     value = new CastIINode(value, itype, false, true /* range check dependency */);
4032     // Make the CastII node dependent on the control input to prevent the narrowed ConvI2L
4033     // node from floating above the range check during loop optimizations. Otherwise, the
4034     // ConvI2L node may be eliminated independently of the range check, causing the data path
4035     // to become TOP while the control path is still there (although it's unreachable).
4036     value-&gt;set_req(0, ctrl);
4037     // Save CastII node to remove it after loop optimizations.
4038     phase-&gt;C-&gt;add_range_check_cast(value);
4039     value = phase-&gt;transform(value);
4040   }
4041   const TypeLong* ltype = TypeLong::make(itype-&gt;_lo, itype-&gt;_hi, itype-&gt;_widen);
4042   return phase-&gt;transform(new ConvI2LNode(value, ltype));
4043 }
4044 
4045 // The message about the current inlining is accumulated in
4046 // _print_inlining_stream and transfered into the _print_inlining_list
4047 // once we know whether inlining succeeds or not. For regular
4048 // inlining, messages are appended to the buffer pointed by
4049 // _print_inlining_idx in the _print_inlining_list. For late inlining,
4050 // a new buffer is added after _print_inlining_idx in the list. This
4051 // way we can update the inlining message for late inlining call site
4052 // when the inlining is attempted again.
4053 void Compile::print_inlining_init() {
4054   if (print_inlining() || print_intrinsics()) {
4055     _print_inlining_stream = new stringStream();
4056     _print_inlining_list = new (comp_arena())GrowableArray&lt;PrintInliningBuffer&gt;(comp_arena(), 1, 1, PrintInliningBuffer());
4057   }
4058 }
4059 
4060 void Compile::print_inlining_reinit() {
4061   if (print_inlining() || print_intrinsics()) {
4062     // Re allocate buffer when we change ResourceMark
4063     _print_inlining_stream = new stringStream();
4064   }
4065 }
4066 
4067 void Compile::print_inlining_reset() {
4068   _print_inlining_stream-&gt;reset();
4069 }
4070 
4071 void Compile::print_inlining_commit() {
4072   assert(print_inlining() || print_intrinsics(), "PrintInlining off?");
4073   // Transfer the message from _print_inlining_stream to the current
4074   // _print_inlining_list buffer and clear _print_inlining_stream.
4075   _print_inlining_list-&gt;at(_print_inlining_idx).ss()-&gt;write(_print_inlining_stream-&gt;as_string(), _print_inlining_stream-&gt;size());
4076   print_inlining_reset();
4077 }
4078 
4079 void Compile::print_inlining_push() {
4080   // Add new buffer to the _print_inlining_list at current position
4081   _print_inlining_idx++;
4082   _print_inlining_list-&gt;insert_before(_print_inlining_idx, PrintInliningBuffer());
4083 }
4084 
4085 Compile::PrintInliningBuffer&amp; Compile::print_inlining_current() {
4086   return _print_inlining_list-&gt;at(_print_inlining_idx);
4087 }
4088 
4089 void Compile::print_inlining_update(CallGenerator* cg) {
4090   if (print_inlining() || print_intrinsics()) {
4091     if (!cg-&gt;is_late_inline()) {
4092       if (print_inlining_current().cg() != NULL) {
4093         print_inlining_push();
4094       }
4095       print_inlining_commit();
4096     } else {
4097       if (print_inlining_current().cg() != cg &amp;&amp;
4098           (print_inlining_current().cg() != NULL ||
4099            print_inlining_current().ss()-&gt;size() != 0)) {
4100         print_inlining_push();
4101       }
4102       print_inlining_commit();
4103       print_inlining_current().set_cg(cg);
4104     }
4105   }
4106 }
4107 
4108 void Compile::print_inlining_move_to(CallGenerator* cg) {
4109   // We resume inlining at a late inlining call site. Locate the
4110   // corresponding inlining buffer so that we can update it.
4111   if (print_inlining()) {
4112     for (int i = 0; i &lt; _print_inlining_list-&gt;length(); i++) {
4113       if (_print_inlining_list-&gt;adr_at(i)-&gt;cg() == cg) {
4114         _print_inlining_idx = i;
4115         return;
4116       }
4117     }
4118     ShouldNotReachHere();
4119   }
4120 }
4121 
4122 void Compile::print_inlining_update_delayed(CallGenerator* cg) {
4123   if (print_inlining()) {
4124     assert(_print_inlining_stream-&gt;size() &gt; 0, "missing inlining msg");
4125     assert(print_inlining_current().cg() == cg, "wrong entry");
4126     // replace message with new message
4127     _print_inlining_list-&gt;at_put(_print_inlining_idx, PrintInliningBuffer());
4128     print_inlining_commit();
4129     print_inlining_current().set_cg(cg);
4130   }
4131 }
4132 
4133 void Compile::print_inlining_assert_ready() {
4134   assert(!_print_inlining || _print_inlining_stream-&gt;size() == 0, "loosing data");
4135 }
4136 
4137 void Compile::process_print_inlining() {
4138   bool do_print_inlining = print_inlining() || print_intrinsics();
4139   if (do_print_inlining || log() != NULL) {
4140     // Print inlining message for candidates that we couldn't inline
4141     // for lack of space
4142     for (int i = 0; i &lt; _late_inlines.length(); i++) {
4143       CallGenerator* cg = _late_inlines.at(i);
4144       if (!cg-&gt;is_mh_late_inline()) {
4145         const char* msg = "live nodes &gt; LiveNodeCountInliningCutoff";
4146         if (do_print_inlining) {
4147           cg-&gt;print_inlining_late(msg);
4148         }
4149         log_late_inline_failure(cg, msg);
4150       }
4151     }
4152   }
4153   if (do_print_inlining) {
4154     ResourceMark rm;
4155     stringStream ss;
4156     for (int i = 0; i &lt; _print_inlining_list-&gt;length(); i++) {
4157       ss.print("%s", _print_inlining_list-&gt;adr_at(i)-&gt;ss()-&gt;as_string());
4158     }
4159     size_t end = ss.size();
4160     _print_inlining_output = NEW_ARENA_ARRAY(comp_arena(), char, end+1);
4161     strncpy(_print_inlining_output, ss.base(), end+1);
4162     _print_inlining_output[end] = 0;
4163   }
4164 }
4165 
4166 void Compile::dump_print_inlining() {
4167   if (_print_inlining_output != NULL) {
4168     tty-&gt;print_raw(_print_inlining_output);
4169   }
4170 }
4171 
4172 void Compile::log_late_inline(CallGenerator* cg) {
4173   if (log() != NULL) {
4174     log()-&gt;head("late_inline method='%d'  inline_id='" JLONG_FORMAT "'", log()-&gt;identify(cg-&gt;method()),
4175                 cg-&gt;unique_id());
4176     JVMState* p = cg-&gt;call_node()-&gt;jvms();
4177     while (p != NULL) {
4178       log()-&gt;elem("jvms bci='%d' method='%d'", p-&gt;bci(), log()-&gt;identify(p-&gt;method()));
4179       p = p-&gt;caller();
4180     }
4181     log()-&gt;tail("late_inline");
4182   }
4183 }
4184 
4185 void Compile::log_late_inline_failure(CallGenerator* cg, const char* msg) {
4186   log_late_inline(cg);
4187   if (log() != NULL) {
4188     log()-&gt;inline_fail(msg);
4189   }
4190 }
4191 
4192 void Compile::log_inline_id(CallGenerator* cg) {
4193   if (log() != NULL) {
4194     // The LogCompilation tool needs a unique way to identify late
4195     // inline call sites. This id must be unique for this call site in
4196     // this compilation. Try to have it unique across compilations as
4197     // well because it can be convenient when grepping through the log
4198     // file.
4199     // Distinguish OSR compilations from others in case CICountOSR is
4200     // on.
4201     jlong id = ((jlong)unique()) + (((jlong)compile_id()) &lt;&lt; 33) + (CICountOSR &amp;&amp; is_osr_compilation() ? ((jlong)1) &lt;&lt; 32 : 0);
4202     cg-&gt;set_unique_id(id);
4203     log()-&gt;elem("inline_id id='" JLONG_FORMAT "'", id);
4204   }
4205 }
4206 
4207 void Compile::log_inline_failure(const char* msg) {
4208   if (C-&gt;log() != NULL) {
4209     C-&gt;log()-&gt;inline_fail(msg);
4210   }
4211 }
4212 
4213 
4214 // Dump inlining replay data to the stream.
4215 // Don't change thread state and acquire any locks.
4216 void Compile::dump_inline_data(outputStream* out) {
4217   InlineTree* inl_tree = ilt();
4218   if (inl_tree != NULL) {
4219     //tty-&gt;print("&gt;DUMP_INLINE_DATA FOR:");method()-&gt;print_name(tty);tty-&gt;print("\n");
4220     //tty-&gt;print("&gt;INL_TREE-&gt;COUNT() = %d&lt;\n",inl_tree-&gt;count());
4221     out-&gt;print(" inline %d", inl_tree-&gt;count());
4222     inl_tree-&gt;dump_replay_data(out);
4223   }
4224 }
4225 
4226 int Compile::cmp_expensive_nodes(Node* n1, Node* n2) {
4227   if (n1-&gt;Opcode() &lt; n2-&gt;Opcode())      return -1;
4228   else if (n1-&gt;Opcode() &gt; n2-&gt;Opcode()) return 1;
4229 
4230   assert(n1-&gt;req() == n2-&gt;req(), "can't compare %s nodes: n1-&gt;req() = %d, n2-&gt;req() = %d", NodeClassNames[n1-&gt;Opcode()], n1-&gt;req(), n2-&gt;req());
4231   for (uint i = 1; i &lt; n1-&gt;req(); i++) {
4232     if (n1-&gt;in(i) &lt; n2-&gt;in(i))      return -1;
4233     else if (n1-&gt;in(i) &gt; n2-&gt;in(i)) return 1;
4234   }
4235 
4236   return 0;
4237 }
4238 
4239 int Compile::cmp_expensive_nodes(Node** n1p, Node** n2p) {
4240   Node* n1 = *n1p;
4241   Node* n2 = *n2p;
4242 
4243   return cmp_expensive_nodes(n1, n2);
4244 }
4245 
4246 void Compile::sort_expensive_nodes() {
4247   if (!expensive_nodes_sorted()) {
4248     _expensive_nodes-&gt;sort(cmp_expensive_nodes);
4249   }
4250 }
4251 
4252 bool Compile::expensive_nodes_sorted() const {
4253   for (int i = 1; i &lt; _expensive_nodes-&gt;length(); i++) {
4254     if (cmp_expensive_nodes(_expensive_nodes-&gt;adr_at(i), _expensive_nodes-&gt;adr_at(i-1)) &lt; 0) {
4255       return false;
4256     }
4257   }
4258   return true;
4259 }
4260 
4261 bool Compile::should_optimize_expensive_nodes(PhaseIterGVN &amp;igvn) {
4262   if (_expensive_nodes-&gt;length() == 0) {
4263     return false;
4264   }
4265 
4266   assert(OptimizeExpensiveOps, "optimization off?");
4267 
4268   // Take this opportunity to remove dead nodes from the list
4269   int j = 0;
4270   for (int i = 0; i &lt; _expensive_nodes-&gt;length(); i++) {
4271     Node* n = _expensive_nodes-&gt;at(i);
4272     if (!n-&gt;is_unreachable(igvn)) {
4273       assert(n-&gt;is_expensive(), "should be expensive");
4274       _expensive_nodes-&gt;at_put(j, n);
4275       j++;
4276     }
4277   }
4278   _expensive_nodes-&gt;trunc_to(j);
4279 
4280   // Then sort the list so that similar nodes are next to each other
4281   // and check for at least two nodes of identical kind with same data
4282   // inputs.
4283   sort_expensive_nodes();
4284 
4285   for (int i = 0; i &lt; _expensive_nodes-&gt;length()-1; i++) {
4286     if (cmp_expensive_nodes(_expensive_nodes-&gt;adr_at(i), _expensive_nodes-&gt;adr_at(i+1)) == 0) {
4287       return true;
4288     }
4289   }
4290 
4291   return false;
4292 }
4293 
4294 void Compile::cleanup_expensive_nodes(PhaseIterGVN &amp;igvn) {
4295   if (_expensive_nodes-&gt;length() == 0) {
4296     return;
4297   }
4298 
4299   assert(OptimizeExpensiveOps, "optimization off?");
4300 
4301   // Sort to bring similar nodes next to each other and clear the
4302   // control input of nodes for which there's only a single copy.
4303   sort_expensive_nodes();
4304 
4305   int j = 0;
4306   int identical = 0;
4307   int i = 0;
4308   bool modified = false;
4309   for (; i &lt; _expensive_nodes-&gt;length()-1; i++) {
4310     assert(j &lt;= i, "can't write beyond current index");
4311     if (_expensive_nodes-&gt;at(i)-&gt;Opcode() == _expensive_nodes-&gt;at(i+1)-&gt;Opcode()) {
4312       identical++;
4313       _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4314       continue;
4315     }
4316     if (identical &gt; 0) {
4317       _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4318       identical = 0;
4319     } else {
4320       Node* n = _expensive_nodes-&gt;at(i);
4321       igvn.replace_input_of(n, 0, NULL);
4322       igvn.hash_insert(n);
4323       modified = true;
4324     }
4325   }
4326   if (identical &gt; 0) {
4327     _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4328   } else if (_expensive_nodes-&gt;length() &gt;= 1) {
4329     Node* n = _expensive_nodes-&gt;at(i);
4330     igvn.replace_input_of(n, 0, NULL);
4331     igvn.hash_insert(n);
4332     modified = true;
4333   }
4334   _expensive_nodes-&gt;trunc_to(j);
4335   if (modified) {
4336     igvn.optimize();
4337   }
4338 }
4339 
4340 void Compile::add_expensive_node(Node * n) {
4341   assert(!_expensive_nodes-&gt;contains(n), "duplicate entry in expensive list");
4342   assert(n-&gt;is_expensive(), "expensive nodes with non-null control here only");
4343   assert(!n-&gt;is_CFG() &amp;&amp; !n-&gt;is_Mem(), "no cfg or memory nodes here");
4344   if (OptimizeExpensiveOps) {
4345     _expensive_nodes-&gt;append(n);
4346   } else {
4347     // Clear control input and let IGVN optimize expensive nodes if
4348     // OptimizeExpensiveOps is off.
4349     n-&gt;set_req(0, NULL);
4350   }
4351 }
4352 
4353 /**
4354  * Remove the speculative part of types and clean up the graph
4355  */
4356 void Compile::remove_speculative_types(PhaseIterGVN &amp;igvn) {
4357   if (UseTypeSpeculation) {
4358     Unique_Node_List worklist;
4359     worklist.push(root());
4360     int modified = 0;
4361     // Go over all type nodes that carry a speculative type, drop the
4362     // speculative part of the type and enqueue the node for an igvn
4363     // which may optimize it out.
4364     for (uint next = 0; next &lt; worklist.size(); ++next) {
4365       Node *n  = worklist.at(next);
4366       if (n-&gt;is_Type()) {
4367         TypeNode* tn = n-&gt;as_Type();
4368         const Type* t = tn-&gt;type();
4369         const Type* t_no_spec = t-&gt;remove_speculative();
4370         if (t_no_spec != t) {
4371           bool in_hash = igvn.hash_delete(n);
4372           assert(in_hash, "node should be in igvn hash table");
4373           tn-&gt;set_type(t_no_spec);
4374           igvn.hash_insert(n);
4375           igvn._worklist.push(n); // give it a chance to go away
4376           modified++;
4377         }
4378       }
4379       uint max = n-&gt;len();
4380       for( uint i = 0; i &lt; max; ++i ) {
4381         Node *m = n-&gt;in(i);
4382         if (not_a_node(m))  continue;
4383         worklist.push(m);
4384       }
4385     }
4386     // Drop the speculative part of all types in the igvn's type table
4387     igvn.remove_speculative_types();
4388     if (modified &gt; 0) {
4389       igvn.optimize();
4390     }
4391 #ifdef ASSERT
4392     // Verify that after the IGVN is over no speculative type has resurfaced
4393     worklist.clear();
4394     worklist.push(root());
4395     for (uint next = 0; next &lt; worklist.size(); ++next) {
4396       Node *n  = worklist.at(next);
4397       const Type* t = igvn.type_or_null(n);
4398       assert((t == NULL) || (t == t-&gt;remove_speculative()), "no more speculative types");
4399       if (n-&gt;is_Type()) {
4400         t = n-&gt;as_Type()-&gt;type();
4401         assert(t == t-&gt;remove_speculative(), "no more speculative types");
4402       }
4403       uint max = n-&gt;len();
4404       for( uint i = 0; i &lt; max; ++i ) {
4405         Node *m = n-&gt;in(i);
4406         if (not_a_node(m))  continue;
4407         worklist.push(m);
4408       }
4409     }
4410     igvn.check_no_speculative_types();
4411 #endif
4412   }
4413 }
4414 
4415 // Auxiliary method to support randomized stressing/fuzzing.
4416 //
4417 // This method can be called the arbitrary number of times, with current count
4418 // as the argument. The logic allows selecting a single candidate from the
4419 // running list of candidates as follows:
4420 //    int count = 0;
4421 //    Cand* selected = null;
4422 //    while(cand = cand-&gt;next()) {
4423 //      if (randomized_select(++count)) {
4424 //        selected = cand;
4425 //      }
4426 //    }
4427 //
4428 // Including count equalizes the chances any candidate is "selected".
4429 // This is useful when we don't have the complete list of candidates to choose
4430 // from uniformly. In this case, we need to adjust the randomicity of the
4431 // selection, or else we will end up biasing the selection towards the latter
4432 // candidates.
4433 //
4434 // Quick back-envelope calculation shows that for the list of n candidates
4435 // the equal probability for the candidate to persist as "best" can be
4436 // achieved by replacing it with "next" k-th candidate with the probability
4437 // of 1/k. It can be easily shown that by the end of the run, the
4438 // probability for any candidate is converged to 1/n, thus giving the
4439 // uniform distribution among all the candidates.
4440 //
4441 // We don't care about the domain size as long as (RANDOMIZED_DOMAIN / count) is large.
4442 #define RANDOMIZED_DOMAIN_POW 29
4443 #define RANDOMIZED_DOMAIN (1 &lt;&lt; RANDOMIZED_DOMAIN_POW)
4444 #define RANDOMIZED_DOMAIN_MASK ((1 &lt;&lt; (RANDOMIZED_DOMAIN_POW + 1)) - 1)
4445 bool Compile::randomized_select(int count) {
4446   assert(count &gt; 0, "only positive");
4447   return (os::random() &amp; RANDOMIZED_DOMAIN_MASK) &lt; (RANDOMIZED_DOMAIN / count);
4448 }
4449 
4450 CloneMap&amp;     Compile::clone_map()                 { return _clone_map; }
4451 void          Compile::set_clone_map(Dict* d)      { _clone_map._dict = d; }
4452 
4453 void NodeCloneInfo::dump() const {
4454   tty-&gt;print(" {%d:%d} ", idx(), gen());
4455 }
4456 
4457 void CloneMap::clone(Node* old, Node* nnn, int gen) {
4458   uint64_t val = value(old-&gt;_idx);
4459   NodeCloneInfo cio(val);
4460   assert(val != 0, "old node should be in the map");
4461   NodeCloneInfo cin(cio.idx(), gen + cio.gen());
4462   insert(nnn-&gt;_idx, cin.get());
4463 #ifndef PRODUCT
4464   if (is_debug()) {
4465     tty-&gt;print_cr("CloneMap::clone inserted node %d info {%d:%d} into CloneMap", nnn-&gt;_idx, cin.idx(), cin.gen());
4466   }
4467 #endif
4468 }
4469 
4470 void CloneMap::verify_insert_and_clone(Node* old, Node* nnn, int gen) {
4471   NodeCloneInfo cio(value(old-&gt;_idx));
4472   if (cio.get() == 0) {
4473     cio.set(old-&gt;_idx, 0);
4474     insert(old-&gt;_idx, cio.get());
4475 #ifndef PRODUCT
4476     if (is_debug()) {
4477       tty-&gt;print_cr("CloneMap::verify_insert_and_clone inserted node %d info {%d:%d} into CloneMap", old-&gt;_idx, cio.idx(), cio.gen());
4478     }
4479 #endif
4480   }
4481   clone(old, nnn, gen);
4482 }
4483 
4484 int CloneMap::max_gen() const {
4485   int g = 0;
4486   DictI di(_dict);
4487   for(; di.test(); ++di) {
4488     int t = gen(di._key);
4489     if (g &lt; t) {
4490       g = t;
4491 #ifndef PRODUCT
4492       if (is_debug()) {
4493         tty-&gt;print_cr("CloneMap::max_gen() update max=%d from %d", g, _2_node_idx_t(di._key));
4494       }
4495 #endif
4496     }
4497   }
4498   return g;
4499 }
4500 
4501 void CloneMap::dump(node_idx_t key) const {
4502   uint64_t val = value(key);
4503   if (val != 0) {
4504     NodeCloneInfo ni(val);
4505     ni.dump();
4506   }
4507 }
</pre></body></html>
