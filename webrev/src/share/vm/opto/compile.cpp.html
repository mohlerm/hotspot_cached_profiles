<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/opto/compile.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/macroAssembler.hpp"
  27 #include "asm/macroAssembler.inline.hpp"
  28 #include "ci/ciReplay.hpp"
  29 #include "ci/ciCacheReplay.hpp"
  30 #include "ci/ciCacheProfiles.hpp"
  31 #include "classfile/systemDictionary.hpp"
  32 #include "code/exceptionHandlerTable.hpp"
  33 #include "code/nmethod.hpp"
  34 #include "compiler/compileBroker.hpp"
  35 #include "compiler/compileLog.hpp"
  36 #include "compiler/disassembler.hpp"
  37 #include "compiler/oopMap.hpp"
  38 #include "opto/addnode.hpp"
  39 #include "opto/block.hpp"
  40 #include "opto/c2compiler.hpp"
  41 #include "opto/callGenerator.hpp"
  42 #include "opto/callnode.hpp"
  43 #include "opto/castnode.hpp"
  44 #include "opto/cfgnode.hpp"
  45 #include "opto/chaitin.hpp"
  46 #include "opto/compile.hpp"
  47 #include "opto/connode.hpp"
  48 #include "opto/convertnode.hpp"
  49 #include "opto/divnode.hpp"
  50 #include "opto/escape.hpp"
  51 #include "opto/idealGraphPrinter.hpp"
  52 #include "opto/loopnode.hpp"
  53 #include "opto/machnode.hpp"
  54 #include "opto/macro.hpp"
  55 #include "opto/matcher.hpp"
  56 #include "opto/mathexactnode.hpp"
  57 #include "opto/memnode.hpp"
  58 #include "opto/mulnode.hpp"
  59 #include "opto/narrowptrnode.hpp"
  60 #include "opto/node.hpp"
  61 #include "opto/opcodes.hpp"
  62 #include "opto/output.hpp"
  63 #include "opto/parse.hpp"
  64 #include "opto/phaseX.hpp"
  65 #include "opto/rootnode.hpp"
  66 #include "opto/runtime.hpp"
  67 #include "opto/stringopts.hpp"
  68 #include "opto/type.hpp"
  69 #include "opto/vectornode.hpp"
  70 #include "runtime/arguments.hpp"
  71 #include "runtime/sharedRuntime.hpp"
  72 #include "runtime/signature.hpp"
  73 #include "runtime/stubRoutines.hpp"
  74 #include "runtime/timer.hpp"
  75 #include "utilities/copy.hpp"
  76 
  77 
  78 // -------------------- Compile::mach_constant_base_node -----------------------
  79 // Constant table base node singleton.
  80 MachConstantBaseNode* Compile::mach_constant_base_node() {
  81   if (_mach_constant_base_node == NULL) {
  82     _mach_constant_base_node = new MachConstantBaseNode();
  83     _mach_constant_base_node-&gt;add_req(C-&gt;root());
  84   }
  85   return _mach_constant_base_node;
  86 }
  87 
  88 
  89 /// Support for intrinsics.
  90 
  91 // Return the index at which m must be inserted (or already exists).
  92 // The sort order is by the address of the ciMethod, with is_virtual as minor key.
  93 class IntrinsicDescPair {
  94  private:
  95   ciMethod* _m;
  96   bool _is_virtual;
  97  public:
  98   IntrinsicDescPair(ciMethod* m, bool is_virtual) : _m(m), _is_virtual(is_virtual) {}
  99   static int compare(IntrinsicDescPair* const&amp; key, CallGenerator* const&amp; elt) {
 100     ciMethod* m= elt-&gt;method();
 101     ciMethod* key_m = key-&gt;_m;
 102     if (key_m &lt; m)      return -1;
 103     else if (key_m &gt; m) return 1;
 104     else {
 105       bool is_virtual = elt-&gt;is_virtual();
 106       bool key_virtual = key-&gt;_is_virtual;
 107       if (key_virtual &lt; is_virtual)      return -1;
 108       else if (key_virtual &gt; is_virtual) return 1;
 109       else                               return 0;
 110     }
 111   }
 112 };
 113 int Compile::intrinsic_insertion_index(ciMethod* m, bool is_virtual, bool&amp; found) {
 114 #ifdef ASSERT
 115   for (int i = 1; i &lt; _intrinsics-&gt;length(); i++) {
 116     CallGenerator* cg1 = _intrinsics-&gt;at(i-1);
 117     CallGenerator* cg2 = _intrinsics-&gt;at(i);
 118     assert(cg1-&gt;method() != cg2-&gt;method()
 119            ? cg1-&gt;method()     &lt; cg2-&gt;method()
 120            : cg1-&gt;is_virtual() &lt; cg2-&gt;is_virtual(),
 121            "compiler intrinsics list must stay sorted");
 122   }
 123 #endif
 124   IntrinsicDescPair pair(m, is_virtual);
 125   return _intrinsics-&gt;find_sorted&lt;IntrinsicDescPair*, IntrinsicDescPair::compare&gt;(&amp;pair, found);
 126 }
 127 
 128 void Compile::register_intrinsic(CallGenerator* cg) {
 129   if (_intrinsics == NULL) {
 130     _intrinsics = new (comp_arena())GrowableArray&lt;CallGenerator*&gt;(comp_arena(), 60, 0, NULL);
 131   }
 132   int len = _intrinsics-&gt;length();
 133   bool found = false;
 134   int index = intrinsic_insertion_index(cg-&gt;method(), cg-&gt;is_virtual(), found);
 135   assert(!found, "registering twice");
 136   _intrinsics-&gt;insert_before(index, cg);
 137   assert(find_intrinsic(cg-&gt;method(), cg-&gt;is_virtual()) == cg, "registration worked");
 138 }
 139 
 140 CallGenerator* Compile::find_intrinsic(ciMethod* m, bool is_virtual) {
 141   assert(m-&gt;is_loaded(), "don't try this on unloaded methods");
 142   if (_intrinsics != NULL) {
 143     bool found = false;
 144     int index = intrinsic_insertion_index(m, is_virtual, found);
 145      if (found) {
 146       return _intrinsics-&gt;at(index);
 147     }
 148   }
 149   // Lazily create intrinsics for intrinsic IDs well-known in the runtime.
 150   if (m-&gt;intrinsic_id() != vmIntrinsics::_none &amp;&amp;
 151       m-&gt;intrinsic_id() &lt;= vmIntrinsics::LAST_COMPILER_INLINE) {
 152     CallGenerator* cg = make_vm_intrinsic(m, is_virtual);
 153     if (cg != NULL) {
 154       // Save it for next time:
 155       register_intrinsic(cg);
 156       return cg;
 157     } else {
 158       gather_intrinsic_statistics(m-&gt;intrinsic_id(), is_virtual, _intrinsic_disabled);
 159     }
 160   }
 161   return NULL;
 162 }
 163 
 164 // Compile:: register_library_intrinsics and make_vm_intrinsic are defined
 165 // in library_call.cpp.
 166 
 167 
 168 #ifndef PRODUCT
 169 // statistics gathering...
 170 
 171 juint  Compile::_intrinsic_hist_count[vmIntrinsics::ID_LIMIT] = {0};
 172 jubyte Compile::_intrinsic_hist_flags[vmIntrinsics::ID_LIMIT] = {0};
 173 
 174 bool Compile::gather_intrinsic_statistics(vmIntrinsics::ID id, bool is_virtual, int flags) {
 175   assert(id &gt; vmIntrinsics::_none &amp;&amp; id &lt; vmIntrinsics::ID_LIMIT, "oob");
 176   int oflags = _intrinsic_hist_flags[id];
 177   assert(flags != 0, "what happened?");
 178   if (is_virtual) {
 179     flags |= _intrinsic_virtual;
 180   }
 181   bool changed = (flags != oflags);
 182   if ((flags &amp; _intrinsic_worked) != 0) {
 183     juint count = (_intrinsic_hist_count[id] += 1);
 184     if (count == 1) {
 185       changed = true;           // first time
 186     }
 187     // increment the overall count also:
 188     _intrinsic_hist_count[vmIntrinsics::_none] += 1;
 189   }
 190   if (changed) {
 191     if (((oflags ^ flags) &amp; _intrinsic_virtual) != 0) {
 192       // Something changed about the intrinsic's virtuality.
 193       if ((flags &amp; _intrinsic_virtual) != 0) {
 194         // This is the first use of this intrinsic as a virtual call.
 195         if (oflags != 0) {
 196           // We already saw it as a non-virtual, so note both cases.
 197           flags |= _intrinsic_both;
 198         }
 199       } else if ((oflags &amp; _intrinsic_both) == 0) {
 200         // This is the first use of this intrinsic as a non-virtual
 201         flags |= _intrinsic_both;
 202       }
 203     }
 204     _intrinsic_hist_flags[id] = (jubyte) (oflags | flags);
 205   }
 206   // update the overall flags also:
 207   _intrinsic_hist_flags[vmIntrinsics::_none] |= (jubyte) flags;
 208   return changed;
 209 }
 210 
 211 static char* format_flags(int flags, char* buf) {
 212   buf[0] = 0;
 213   if ((flags &amp; Compile::_intrinsic_worked) != 0)    strcat(buf, ",worked");
 214   if ((flags &amp; Compile::_intrinsic_failed) != 0)    strcat(buf, ",failed");
 215   if ((flags &amp; Compile::_intrinsic_disabled) != 0)  strcat(buf, ",disabled");
 216   if ((flags &amp; Compile::_intrinsic_virtual) != 0)   strcat(buf, ",virtual");
 217   if ((flags &amp; Compile::_intrinsic_both) != 0)      strcat(buf, ",nonvirtual");
 218   if (buf[0] == 0)  strcat(buf, ",");
 219   assert(buf[0] == ',', "must be");
 220   return &amp;buf[1];
 221 }
 222 
 223 void Compile::print_intrinsic_statistics() {
 224   char flagsbuf[100];
 225   ttyLocker ttyl;
 226   if (xtty != NULL)  xtty-&gt;head("statistics type='intrinsic'");
 227   tty-&gt;print_cr("Compiler intrinsic usage:");
 228   juint total = _intrinsic_hist_count[vmIntrinsics::_none];
 229   if (total == 0)  total = 1;  // avoid div0 in case of no successes
 230   #define PRINT_STAT_LINE(name, c, f) \
 231     tty-&gt;print_cr("  %4d (%4.1f%%) %s (%s)", (int)(c), ((c) * 100.0) / total, name, f);
 232   for (int index = 1 + (int)vmIntrinsics::_none; index &lt; (int)vmIntrinsics::ID_LIMIT; index++) {
 233     vmIntrinsics::ID id = (vmIntrinsics::ID) index;
 234     int   flags = _intrinsic_hist_flags[id];
 235     juint count = _intrinsic_hist_count[id];
 236     if ((flags | count) != 0) {
 237       PRINT_STAT_LINE(vmIntrinsics::name_at(id), count, format_flags(flags, flagsbuf));
 238     }
 239   }
 240   PRINT_STAT_LINE("total", total, format_flags(_intrinsic_hist_flags[vmIntrinsics::_none], flagsbuf));
 241   if (xtty != NULL)  xtty-&gt;tail("statistics");
 242 }
 243 
 244 void Compile::print_statistics() {
 245   { ttyLocker ttyl;
 246     if (xtty != NULL)  xtty-&gt;head("statistics type='opto'");
 247     Parse::print_statistics();
 248     PhaseCCP::print_statistics();
 249     PhaseRegAlloc::print_statistics();
 250     Scheduling::print_statistics();
 251     PhasePeephole::print_statistics();
 252     PhaseIdealLoop::print_statistics();
 253     if (xtty != NULL)  xtty-&gt;tail("statistics");
 254   }
 255   if (_intrinsic_hist_flags[vmIntrinsics::_none] != 0) {
 256     // put this under its own &lt;statistics&gt; element.
 257     print_intrinsic_statistics();
 258   }
 259 }
 260 #endif //PRODUCT
 261 
 262 // Support for bundling info
 263 Bundle* Compile::node_bundling(const Node *n) {
 264   assert(valid_bundle_info(n), "oob");
 265   return &amp;_node_bundling_base[n-&gt;_idx];
 266 }
 267 
 268 bool Compile::valid_bundle_info(const Node *n) {
 269   return (_node_bundling_limit &gt; n-&gt;_idx);
 270 }
 271 
 272 
 273 void Compile::gvn_replace_by(Node* n, Node* nn) {
 274   for (DUIterator_Last imin, i = n-&gt;last_outs(imin); i &gt;= imin; ) {
 275     Node* use = n-&gt;last_out(i);
 276     bool is_in_table = initial_gvn()-&gt;hash_delete(use);
 277     uint uses_found = 0;
 278     for (uint j = 0; j &lt; use-&gt;len(); j++) {
 279       if (use-&gt;in(j) == n) {
 280         if (j &lt; use-&gt;req())
 281           use-&gt;set_req(j, nn);
 282         else
 283           use-&gt;set_prec(j, nn);
 284         uses_found++;
 285       }
 286     }
 287     if (is_in_table) {
 288       // reinsert into table
 289       initial_gvn()-&gt;hash_find_insert(use);
 290     }
 291     record_for_igvn(use);
 292     i -= uses_found;    // we deleted 1 or more copies of this edge
 293   }
 294 }
 295 
 296 
 297 static inline bool not_a_node(const Node* n) {
 298   if (n == NULL)                   return true;
 299   if (((intptr_t)n &amp; 1) != 0)      return true;  // uninitialized, etc.
 300   if (*(address*)n == badAddress)  return true;  // kill by Node::destruct
 301   return false;
 302 }
 303 
 304 // Identify all nodes that are reachable from below, useful.
 305 // Use breadth-first pass that records state in a Unique_Node_List,
 306 // recursive traversal is slower.
 307 void Compile::identify_useful_nodes(Unique_Node_List &amp;useful) {
 308   int estimated_worklist_size = live_nodes();
 309   useful.map( estimated_worklist_size, NULL );  // preallocate space
 310 
 311   // Initialize worklist
 312   if (root() != NULL)     { useful.push(root()); }
 313   // If 'top' is cached, declare it useful to preserve cached node
 314   if( cached_top_node() ) { useful.push(cached_top_node()); }
 315 
 316   // Push all useful nodes onto the list, breadthfirst
 317   for( uint next = 0; next &lt; useful.size(); ++next ) {
 318     assert( next &lt; unique(), "Unique useful nodes &lt; total nodes");
 319     Node *n  = useful.at(next);
 320     uint max = n-&gt;len();
 321     for( uint i = 0; i &lt; max; ++i ) {
 322       Node *m = n-&gt;in(i);
 323       if (not_a_node(m))  continue;
 324       useful.push(m);
 325     }
 326   }
 327 }
 328 
 329 // Update dead_node_list with any missing dead nodes using useful
 330 // list. Consider all non-useful nodes to be useless i.e., dead nodes.
 331 void Compile::update_dead_node_list(Unique_Node_List &amp;useful) {
 332   uint max_idx = unique();
 333   VectorSet&amp; useful_node_set = useful.member_set();
 334 
 335   for (uint node_idx = 0; node_idx &lt; max_idx; node_idx++) {
 336     // If node with index node_idx is not in useful set,
 337     // mark it as dead in dead node list.
 338     if (! useful_node_set.test(node_idx) ) {
 339       record_dead_node(node_idx);
 340     }
 341   }
 342 }
 343 
 344 void Compile::remove_useless_late_inlines(GrowableArray&lt;CallGenerator*&gt;* inlines, Unique_Node_List &amp;useful) {
 345   int shift = 0;
 346   for (int i = 0; i &lt; inlines-&gt;length(); i++) {
 347     CallGenerator* cg = inlines-&gt;at(i);
 348     CallNode* call = cg-&gt;call_node();
 349     if (shift &gt; 0) {
 350       inlines-&gt;at_put(i-shift, cg);
 351     }
 352     if (!useful.member(call)) {
 353       shift++;
 354     }
 355   }
 356   inlines-&gt;trunc_to(inlines-&gt;length()-shift);
 357 }
 358 
 359 // Disconnect all useless nodes by disconnecting those at the boundary.
 360 void Compile::remove_useless_nodes(Unique_Node_List &amp;useful) {
 361   uint next = 0;
 362   while (next &lt; useful.size()) {
 363     Node *n = useful.at(next++);
 364     if (n-&gt;is_SafePoint()) {
 365       // We're done with a parsing phase. Replaced nodes are not valid
 366       // beyond that point.
 367       n-&gt;as_SafePoint()-&gt;delete_replaced_nodes();
 368     }
 369     // Use raw traversal of out edges since this code removes out edges
 370     int max = n-&gt;outcnt();
 371     for (int j = 0; j &lt; max; ++j) {
 372       Node* child = n-&gt;raw_out(j);
 373       if (! useful.member(child)) {
 374         assert(!child-&gt;is_top() || child != top(),
 375                "If top is cached in Compile object it is in useful list");
 376         // Only need to remove this out-edge to the useless node
 377         n-&gt;raw_del_out(j);
 378         --j;
 379         --max;
 380       }
 381     }
 382     if (n-&gt;outcnt() == 1 &amp;&amp; n-&gt;has_special_unique_user()) {
 383       record_for_igvn(n-&gt;unique_out());
 384     }
 385   }
 386   // Remove useless macro and predicate opaq nodes
 387   for (int i = C-&gt;macro_count()-1; i &gt;= 0; i--) {
 388     Node* n = C-&gt;macro_node(i);
 389     if (!useful.member(n)) {
 390       remove_macro_node(n);
 391     }
 392   }
 393   // Remove useless CastII nodes with range check dependency
 394   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 395     Node* cast = range_check_cast_node(i);
 396     if (!useful.member(cast)) {
 397       remove_range_check_cast(cast);
 398     }
 399   }
 400   // Remove useless expensive node
 401   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 402     Node* n = C-&gt;expensive_node(i);
 403     if (!useful.member(n)) {
 404       remove_expensive_node(n);
 405     }
 406   }
 407   // clean up the late inline lists
 408   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 409   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 410   remove_useless_late_inlines(&amp;_late_inlines, useful);
 411   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 412 }
 413 
 414 //------------------------------frame_size_in_words-----------------------------
 415 // frame_slots in units of words
 416 int Compile::frame_size_in_words() const {
 417   // shift is 0 in LP32 and 1 in LP64
 418   const int shift = (LogBytesPerWord - LogBytesPerInt);
 419   int words = _frame_slots &gt;&gt; shift;
 420   assert( words &lt;&lt; shift == _frame_slots, "frame size must be properly aligned in LP64" );
 421   return words;
 422 }
 423 
 424 // To bang the stack of this compiled method we use the stack size
 425 // that the interpreter would need in case of a deoptimization. This
 426 // removes the need to bang the stack in the deoptimization blob which
 427 // in turn simplifies stack overflow handling.
 428 int Compile::bang_size_in_bytes() const {
 429   return MAX2(frame_size_in_bytes() + os::extra_bang_size_in_bytes(), _interpreter_frame_size);
 430 }
 431 
 432 // ============================================================================
 433 //------------------------------CompileWrapper---------------------------------
 434 class CompileWrapper : public StackObj {
 435   Compile *const _compile;
 436  public:
 437   CompileWrapper(Compile* compile);
 438 
 439   ~CompileWrapper();
 440 };
 441 
 442 CompileWrapper::CompileWrapper(Compile* compile) : _compile(compile) {
 443   // the Compile* pointer is stored in the current ciEnv:
 444   ciEnv* env = compile-&gt;env();
 445   assert(env == ciEnv::current(), "must already be a ciEnv active");
 446   assert(env-&gt;compiler_data() == NULL, "compile already active?");
 447   env-&gt;set_compiler_data(compile);
 448   assert(compile == Compile::current(), "sanity");
 449 
 450   compile-&gt;set_type_dict(NULL);
 451   compile-&gt;set_clone_map(new Dict(cmpkey, hashkey, _compile-&gt;comp_arena()));
 452   compile-&gt;clone_map().set_clone_idx(0);
 453   compile-&gt;set_type_hwm(NULL);
 454   compile-&gt;set_type_last_size(0);
 455   compile-&gt;set_last_tf(NULL, NULL);
 456   compile-&gt;set_indexSet_arena(NULL);
 457   compile-&gt;set_indexSet_free_block_list(NULL);
 458   compile-&gt;init_type_arena();
 459   Type::Initialize(compile);
 460   _compile-&gt;set_scratch_buffer_blob(NULL);
 461   _compile-&gt;begin_method();
 462   _compile-&gt;clone_map().set_debug(_compile-&gt;has_method() &amp;&amp; _compile-&gt;directive()-&gt;CloneMapDebugOption);
 463 }
 464 CompileWrapper::~CompileWrapper() {
 465   _compile-&gt;end_method();
 466   if (_compile-&gt;scratch_buffer_blob() != NULL)
 467     BufferBlob::free(_compile-&gt;scratch_buffer_blob());
 468   _compile-&gt;env()-&gt;set_compiler_data(NULL);
 469 }
 470 
 471 
 472 //----------------------------print_compile_messages---------------------------
 473 void Compile::print_compile_messages() {
 474 #ifndef PRODUCT
 475   // Check if recompiling
 476   if (_subsume_loads == false &amp;&amp; PrintOpto) {
 477     // Recompiling without allowing machine instructions to subsume loads
 478     tty-&gt;print_cr("*********************************************************");
 479     tty-&gt;print_cr("** Bailout: Recompile without subsuming loads          **");
 480     tty-&gt;print_cr("*********************************************************");
 481   }
 482   if (_do_escape_analysis != DoEscapeAnalysis &amp;&amp; PrintOpto) {
 483     // Recompiling without escape analysis
 484     tty-&gt;print_cr("*********************************************************");
 485     tty-&gt;print_cr("** Bailout: Recompile without escape analysis          **");
 486     tty-&gt;print_cr("*********************************************************");
 487   }
 488   if (_eliminate_boxing != EliminateAutoBox &amp;&amp; PrintOpto) {
 489     // Recompiling without boxing elimination
 490     tty-&gt;print_cr("*********************************************************");
 491     tty-&gt;print_cr("** Bailout: Recompile without boxing elimination       **");
 492     tty-&gt;print_cr("*********************************************************");
 493   }
 494   if (C-&gt;directive()-&gt;BreakAtCompileOption) {
 495     // Open the debugger when compiling this method.
 496     tty-&gt;print("### Breaking when compiling: ");
 497     method()-&gt;print_short_name();
 498     tty-&gt;cr();
 499     BREAKPOINT;
 500   }
 501 
 502   if( PrintOpto ) {
 503     if (is_osr_compilation()) {
 504       tty-&gt;print("[OSR]%3d", _compile_id);
 505     } else {
 506       tty-&gt;print("%3d", _compile_id);
 507     }
 508   }
 509 #endif
 510 }
 511 
 512 
 513 //-----------------------init_scratch_buffer_blob------------------------------
 514 // Construct a temporary BufferBlob and cache it for this compile.
 515 void Compile::init_scratch_buffer_blob(int const_size) {
 516   // If there is already a scratch buffer blob allocated and the
 517   // constant section is big enough, use it.  Otherwise free the
 518   // current and allocate a new one.
 519   BufferBlob* blob = scratch_buffer_blob();
 520   if ((blob != NULL) &amp;&amp; (const_size &lt;= _scratch_const_size)) {
 521     // Use the current blob.
 522   } else {
 523     if (blob != NULL) {
 524       BufferBlob::free(blob);
 525     }
 526 
 527     ResourceMark rm;
 528     _scratch_const_size = const_size;
 529     int size = (MAX_inst_size + MAX_stubs_size + _scratch_const_size);
 530     blob = BufferBlob::create("Compile::scratch_buffer", size);
 531     // Record the buffer blob for next time.
 532     set_scratch_buffer_blob(blob);
 533     // Have we run out of code space?
 534     if (scratch_buffer_blob() == NULL) {
 535       // Let CompilerBroker disable further compilations.
 536       record_failure("Not enough space for scratch buffer in CodeCache");
 537       return;
 538     }
 539   }
 540 
 541   // Initialize the relocation buffers
 542   relocInfo* locs_buf = (relocInfo*) blob-&gt;content_end() - MAX_locs_size;
 543   set_scratch_locs_memory(locs_buf);
 544 }
 545 
 546 
 547 //-----------------------scratch_emit_size-------------------------------------
 548 // Helper function that computes size by emitting code
 549 uint Compile::scratch_emit_size(const Node* n) {
 550   // Start scratch_emit_size section.
 551   set_in_scratch_emit_size(true);
 552 
 553   // Emit into a trash buffer and count bytes emitted.
 554   // This is a pretty expensive way to compute a size,
 555   // but it works well enough if seldom used.
 556   // All common fixed-size instructions are given a size
 557   // method by the AD file.
 558   // Note that the scratch buffer blob and locs memory are
 559   // allocated at the beginning of the compile task, and
 560   // may be shared by several calls to scratch_emit_size.
 561   // The allocation of the scratch buffer blob is particularly
 562   // expensive, since it has to grab the code cache lock.
 563   BufferBlob* blob = this-&gt;scratch_buffer_blob();
 564   assert(blob != NULL, "Initialize BufferBlob at start");
 565   assert(blob-&gt;size() &gt; MAX_inst_size, "sanity");
 566   relocInfo* locs_buf = scratch_locs_memory();
 567   address blob_begin = blob-&gt;content_begin();
 568   address blob_end   = (address)locs_buf;
 569   assert(blob-&gt;content_contains(blob_end), "sanity");
 570   CodeBuffer buf(blob_begin, blob_end - blob_begin);
 571   buf.initialize_consts_size(_scratch_const_size);
 572   buf.initialize_stubs_size(MAX_stubs_size);
 573   assert(locs_buf != NULL, "sanity");
 574   int lsize = MAX_locs_size / 3;
 575   buf.consts()-&gt;initialize_shared_locs(&amp;locs_buf[lsize * 0], lsize);
 576   buf.insts()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 1], lsize);
 577   buf.stubs()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 2], lsize);
 578 
 579   // Do the emission.
 580 
 581   Label fakeL; // Fake label for branch instructions.
 582   Label*   saveL = NULL;
 583   uint save_bnum = 0;
 584   bool is_branch = n-&gt;is_MachBranch();
 585   if (is_branch) {
 586     MacroAssembler masm(&amp;buf);
 587     masm.bind(fakeL);
 588     n-&gt;as_MachBranch()-&gt;save_label(&amp;saveL, &amp;save_bnum);
 589     n-&gt;as_MachBranch()-&gt;label_set(&amp;fakeL, 0);
 590   }
 591   n-&gt;emit(buf, this-&gt;regalloc());
 592 
 593   // Emitting into the scratch buffer should not fail
 594   assert (!failing(), "Must not have pending failure. Reason is: %s", failure_reason());
 595 
 596   if (is_branch) // Restore label.
 597     n-&gt;as_MachBranch()-&gt;label_set(saveL, save_bnum);
 598 
 599   // End scratch_emit_size section.
 600   set_in_scratch_emit_size(false);
 601 
 602   return buf.insts_size();
 603 }
 604 
 605 
 606 // ============================================================================
 607 //------------------------------Compile standard-------------------------------
 608 debug_only( int Compile::_debug_idx = 100000; )
 609 
 610 // Compile a method.  entry_bci is -1 for normal compilations and indicates
 611 // the continuation bci for on stack replacement.
 612 
 613 
 614 Compile::Compile( ciEnv* ci_env, C2Compiler* compiler, ciMethod* target, int osr_bci,
 615                   bool subsume_loads, bool do_escape_analysis, bool eliminate_boxing, DirectiveSet* directive)
 616                 : Phase(Compiler),
 617                   _env(ci_env),
 618                   _directive(directive),
 619                   _log(ci_env-&gt;log()),
 620                   _compile_id(ci_env-&gt;compile_id()),
 621                   _save_argument_registers(false),
 622                   _stub_name(NULL),
 623                   _stub_function(NULL),
 624                   _stub_entry_point(NULL),
 625                   _method(target),
 626                   _entry_bci(osr_bci),
 627                   _initial_gvn(NULL),
 628                   _for_igvn(NULL),
 629                   _warm_calls(NULL),
 630                   _subsume_loads(subsume_loads),
 631                   _do_escape_analysis(do_escape_analysis),
 632                   _eliminate_boxing(eliminate_boxing),
 633                   _failure_reason(NULL),
 634                   _code_buffer("Compile::Fill_buffer"),
 635                   _orig_pc_slot(0),
 636                   _orig_pc_slot_offset_in_bytes(0),
 637                   _has_method_handle_invokes(false),
 638                   _mach_constant_base_node(NULL),
 639                   _node_bundling_limit(0),
 640                   _node_bundling_base(NULL),
 641                   _java_calls(0),
 642                   _inner_loops(0),
 643                   _scratch_const_size(-1),
 644                   _in_scratch_emit_size(false),
 645                   _dead_node_list(comp_arena()),
 646                   _dead_node_count(0),
 647 #ifndef PRODUCT
 648                   _trace_opto_output(directive-&gt;TraceOptoOutputOption),
 649                   _in_dump_cnt(0),
 650                   _printer(IdealGraphPrinter::printer()),
 651 #endif
 652                   _congraph(NULL),
 653                   _comp_arena(mtCompiler),
 654                   _node_arena(mtCompiler),
 655                   _old_arena(mtCompiler),
 656                   _Compile_types(mtCompiler),
 657                   _replay_inline_data(NULL),
 658                   _late_inlines(comp_arena(), 2, 0, NULL),
 659                   _string_late_inlines(comp_arena(), 2, 0, NULL),
 660                   _boxing_late_inlines(comp_arena(), 2, 0, NULL),
 661                   _late_inlines_pos(0),
 662                   _number_of_mh_late_inlines(0),
 663                   _inlining_progress(false),
 664                   _inlining_incrementally(false),
 665                   _print_inlining_list(NULL),
 666                   _print_inlining_stream(NULL),
 667                   _print_inlining_idx(0),
 668                   _print_inlining_output(NULL),
 669                   _interpreter_frame_size(0),
 670                   _max_node_limit(MaxNodeLimit),
 671                   _has_reserved_stack_access(target-&gt;has_reserved_stack_access()) {
 672   C = this;
 673 #ifndef PRODUCT
 674   if (_printer != NULL) {
 675     _printer-&gt;set_compile(this);
 676   }
 677 #endif
 678   CompileWrapper cw(this);
 679 
 680   if (CITimeVerbose) {
 681     tty-&gt;print(" ");
 682     target-&gt;holder()-&gt;name()-&gt;print();
 683     tty-&gt;print(".");
 684     target-&gt;print_short_name();
 685     tty-&gt;print("  ");
 686   }
 687   TraceTime t1("Total compilation time", &amp;_t_totalCompilation, CITime, CITimeVerbose);
 688   TraceTime t2(NULL, &amp;_t_methodCompilation, CITime, false);
 689 
 690 #ifndef PRODUCT
 691   bool print_opto_assembly = directive-&gt;PrintOptoAssemblyOption;
 692   if (!print_opto_assembly) {
 693     bool print_assembly = directive-&gt;PrintAssemblyOption;
 694     if (print_assembly &amp;&amp; !Disassembler::can_decode()) {
 695       tty-&gt;print_cr("PrintAssembly request changed to PrintOptoAssembly");
 696       print_opto_assembly = true;
 697     }
 698   }
 699   set_print_assembly(print_opto_assembly);
 700   set_parsed_irreducible_loop(false);
 701 
 702   if (directive-&gt;ReplayInlineOption) {
 703     _replay_inline_data = ciReplay::load_inline_data(method(), entry_bci(), ci_env-&gt;comp_level());
 704   }
 705 #endif
 706   set_print_inlining(directive-&gt;PrintInliningOption || PrintOptoInlining);
 707   set_print_intrinsics(directive-&gt;PrintIntrinsicsOption);
 708   set_has_irreducible_loop(true); // conservative until build_loop_tree() reset it
 709 
 710   if (ProfileTraps RTM_OPT_ONLY( || UseRTMLocking )) {
 711     // Make sure the method being compiled gets its own MDO,
 712     // so we can at least track the decompile_count().
 713     // Need MDO to record RTM code generation state.
 714     method()-&gt;ensure_method_data();
 715   }
 716 
 717   Init(::AliasLevel);
 718 
 719 
 720   print_compile_messages();
 721 
 722   _ilt = InlineTree::build_inline_tree_root();
 723 
 724   // Even if NO memory addresses are used, MergeMem nodes must have at least 1 slice
 725   assert(num_alias_types() &gt;= AliasIdxRaw, "");
 726 
 727 #define MINIMUM_NODE_HASH  1023
 728   // Node list that Iterative GVN will start with
 729   Unique_Node_List for_igvn(comp_arena());
 730   set_for_igvn(&amp;for_igvn);
 731 
 732   // GVN that will be run immediately on new nodes
 733   uint estimated_size = method()-&gt;code_size()*4+64;
 734   estimated_size = (estimated_size &lt; MINIMUM_NODE_HASH ? MINIMUM_NODE_HASH : estimated_size);
 735   PhaseGVN gvn(node_arena(), estimated_size);
 736   set_initial_gvn(&amp;gvn);
 737 
 738   print_inlining_init();
 739   { // Scope for timing the parser
 740     TracePhase tp("parse", &amp;timers[_t_parser]);
 741 
 742     // Put top into the hash table ASAP.
 743     initial_gvn()-&gt;transform_no_reclaim(top());
 744 
 745     // Set up tf(), start(), and find a CallGenerator.
 746     CallGenerator* cg = NULL;
 747     if (is_osr_compilation()) {
 748       const TypeTuple *domain = StartOSRNode::osr_domain();
 749       const TypeTuple *range = TypeTuple::make_range(method()-&gt;signature());
 750       init_tf(TypeFunc::make(domain, range));
 751       StartNode* s = new StartOSRNode(root(), domain);
 752       initial_gvn()-&gt;set_type_bottom(s);
 753       init_start(s);
 754       cg = CallGenerator::for_osr(method(), entry_bci());
 755     } else {
 756       // Normal case.
 757       init_tf(TypeFunc::make(method()));
 758       StartNode* s = new StartNode(root(), tf()-&gt;domain());
 759       initial_gvn()-&gt;set_type_bottom(s);
 760       init_start(s);
 761       if (method()-&gt;intrinsic_id() == vmIntrinsics::_Reference_get &amp;&amp; UseG1GC) {
 762         // With java.lang.ref.reference.get() we must go through the
 763         // intrinsic when G1 is enabled - even when get() is the root
 764         // method of the compile - so that, if necessary, the value in
 765         // the referent field of the reference object gets recorded by
 766         // the pre-barrier code.
 767         // Specifically, if G1 is enabled, the value in the referent
 768         // field is recorded by the G1 SATB pre barrier. This will
 769         // result in the referent being marked live and the reference
 770         // object removed from the list of discovered references during
 771         // reference processing.
 772         cg = find_intrinsic(method(), false);
 773       }
 774       if (cg == NULL) {
 775         float past_uses = method()-&gt;interpreter_invocation_count();
 776         float expected_uses = past_uses;
 777         cg = CallGenerator::for_inline(method(), expected_uses);
 778       }
 779     }
 780     if (failing())  return;
 781     if (cg == NULL) {
 782       record_method_not_compilable_all_tiers("cannot parse method");
 783       return;
 784     }
 785     JVMState* jvms = build_start_state(start(), tf());
 786     if ((jvms = cg-&gt;generate(jvms)) == NULL) {
 787       if (!failure_reason_is(C2Compiler::retry_class_loading_during_parsing())) {
 788         record_method_not_compilable("method parse failed");
 789       }
 790       return;
 791     }
 792     GraphKit kit(jvms);
 793 
 794     if (!kit.stopped()) {
 795       // Accept return values, and transfer control we know not where.
 796       // This is done by a special, unique ReturnNode bound to root.
 797       return_values(kit.jvms());
 798     }
 799 
 800     if (kit.has_exceptions()) {
 801       // Any exceptions that escape from this call must be rethrown
 802       // to whatever caller is dynamically above us on the stack.
 803       // This is done by a special, unique RethrowNode bound to root.
 804       rethrow_exceptions(kit.transfer_exceptions_into_jvms());
 805     }
 806 
 807     assert(IncrementalInline || (_late_inlines.length() == 0 &amp;&amp; !has_mh_late_inlines()), "incremental inlining is off");
 808 
 809     if (_late_inlines.length() == 0 &amp;&amp; !has_mh_late_inlines() &amp;&amp; !failing() &amp;&amp; has_stringbuilder()) {
 810       inline_string_calls(true);
 811     }
 812 
 813     if (failing())  return;
 814 
 815     print_method(PHASE_BEFORE_REMOVEUSELESS, 3);
 816 
 817     // Remove clutter produced by parsing.
 818     if (!failing()) {
 819       ResourceMark rm;
 820       PhaseRemoveUseless pru(initial_gvn(), &amp;for_igvn);
 821     }
 822   }
 823 
 824   // Note:  Large methods are capped off in do_one_bytecode().
 825   if (failing())  return;
 826 
 827   // After parsing, node notes are no longer automagic.
 828   // They must be propagated by register_new_node_with_optimizer(),
 829   // clone(), or the like.
 830   set_default_node_notes(NULL);
 831 
 832   for (;;) {
 833     int successes = Inline_Warm();
 834     if (failing())  return;
 835     if (successes == 0)  break;
 836   }
 837 
 838   // Drain the list.
 839   Finish_Warm();
 840 #ifndef PRODUCT
 841   if (_printer &amp;&amp; _printer-&gt;should_print(1)) {
 842     _printer-&gt;print_inlining();
 843   }
 844 #endif
 845 
 846   if (failing())  return;
 847   NOT_PRODUCT( verify_graph_edges(); )
 848 
 849   // Now optimize
 850   Optimize();
 851   if (failing())  return;
 852   NOT_PRODUCT( verify_graph_edges(); )
 853 
 854 #ifndef PRODUCT
 855   if (PrintIdeal) {
 856     ttyLocker ttyl;  // keep the following output all in one block
 857     // This output goes directly to the tty, not the compiler log.
 858     // To enable tools to match it up with the compilation activity,
 859     // be sure to tag this tty output with the compile ID.
 860     if (xtty != NULL) {
 861       xtty-&gt;head("ideal compile_id='%d'%s", compile_id(),
 862                  is_osr_compilation()    ? " compile_kind='osr'" :
 863                  "");
 864     }
 865     root()-&gt;dump(9999);
 866     if (xtty != NULL) {
 867       xtty-&gt;tail("ideal");
 868     }
 869   }
 870 #endif
 871 
 872   NOT_PRODUCT( verify_barriers(); )
 873 
 874   // Dump compilation data to replay it.
 875   if (directive-&gt;DumpReplayOption) {
 876     env()-&gt;dump_replay_data(_compile_id);
 877   }
 878   if (directive-&gt;DumpInlineOption &amp;&amp; (ilt() != NULL)) {
 879     env()-&gt;dump_inline_data(_compile_id);
 880   }
 881   // Dump profile to allow profile caching
 882   if(env()-&gt;comp_level()&gt;CompLevel_limited_profile &amp;&amp; env()-&gt;comp_level() &gt;= DumpProfilesMinTier) {
 883     if ((DumpProfiles || method()-&gt;has_option("DumpProfile")) &amp;&amp; (!method()-&gt;has_option("IgnoreDumpProfile"))) {
 884                 const char* klassmethod = _method-&gt;holder()-&gt;name()-&gt;as_utf8();
 885                 int length = strlen(klassmethod);
 886                 char* subbuff = NEW_RESOURCE_ARRAY(char,length+1);
 887                 memcpy( subbuff, klassmethod, length );
 888                 subbuff[length] = '\0';
 889                 if(strncmp(subbuff,"jdk/nashorn/internal/scripts/Script$Recompilation", 49)==0 || strcmp(subbuff,"java/lang/invoke/LambdaForm$MH")==0 || strcmp(subbuff,"java/lang/invoke/LambdaForm$BMH")==0 || strcmp(subbuff,"java/lang/invoke/LambdaForm$DMH")==0 || strcmp(subbuff,"jdk/nashorn/internal/runtime/ScriptObject")==0 || strcmp(subbuff,"jdk/internal/org/objectweb/asm/ClassWriter")==0 || strncmp(subbuff,"java/util/stream/ReferencePipeline", 34)==0 || strcmp(subbuff,"jdk/internal/loader/BuiltinClassLoader")==0 || strcmp(subbuff,"jdk/nashorn/internal/codegen/types/Type")==0 || strcmp(subbuff,"java/util/concurrent/ConcurrentHashMap")==0 || strcmp(subbuff,"jdk/nashorn/internal/codegen/CompilerConstants")==0 || strcmp(subbuff,"jdk/nashorn/internal/codegen/MethodEmitter")==0 || strcmp(subbuff,"jdk/nashorn/internal/codegen/TypeEvaluator")==0 || strcmp(subbuff, "jdk/dynalink/TypeConverterFactory")==0 || strcmp(subbuff, "java/lang/invoke/BoundMethodHandle")==0 || strcmp(subbuff, "java/lang/invoke/MethodHandleImpl")==0 || strcmp(subbuff, "jdk/nashorn/internal/codegen/types/ArrayType")==0 || strcmp(subbuff,"jdk/nashorn/internal/codegen/ClassEmitter")==0 || strcmp(subbuff,"jdk/dynalink/LinkerServicesImpl")==0 || strcmp(subbuff,"jdk/nashorn/internal/runtime/linker/NashornLinker")==0 || strcmp(subbuff,"jdk/nashorn/internal/runtime/AccessorProperty")==0 || strcmp(subbuff,"jdk/nashorn/internal/codegen/CodeGenerator")==0 || strcmp(subbuff,"java/security/Permissions")==0 || strcmp(subbuff," jdk/nashorn/internal/runtime/linker/NashornGuards")==0) {
 890                         //tty-&gt;print("###Avoided: %s\n",method()-&gt;holder()-&gt;name()-&gt;as_utf8());
 891                 } else {
 892                         //tty-&gt;print("###Dump: %s\n",method()-&gt;holder()-&gt;name()-&gt;as_utf8());
 893                         _env-&gt;dump_cache_profiles(0, method()-&gt;name()-&gt;as_utf8());
 894                 }
 895     }
 896   }
 897 
 898   // Now that we know the size of all the monitors we can add a fixed slot
 899   // for the original deopt pc.
 900 
 901   _orig_pc_slot =  fixed_slots();
 902   int next_slot = _orig_pc_slot + (sizeof(address) / VMRegImpl::stack_slot_size);
 903   set_fixed_slots(next_slot);
 904 
 905   // Compute when to use implicit null checks. Used by matching trap based
 906   // nodes and NullCheck optimization.
 907   set_allowed_deopt_reasons();
 908 
 909   // Now generate code
 910   Code_Gen();
 911   if (failing())  return;
 912 
 913   // Check if we want to skip execution of all compiled code.
 914   {
 915 #ifndef PRODUCT
 916     if (OptoNoExecute) {
 917       record_method_not_compilable("+OptoNoExecute");  // Flag as failed
 918       return;
 919     }
 920 #endif
 921     TracePhase tp("install_code", &amp;timers[_t_registerMethod]);
 922 
 923     if (is_osr_compilation()) {
 924       _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);
 925       _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);
 926     } else {
 927       _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);
 928       _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);
 929     }
 930 
 931     env()-&gt;register_method(_method, _entry_bci,
 932                            &amp;_code_offsets,
 933                            _orig_pc_slot_offset_in_bytes,
 934                            code_buffer(),
 935                            frame_size_in_words(), _oop_map_set,
 936                            &amp;_handler_table, &amp;_inc_table,
 937                            compiler,
 938                            has_unsafe_access(),
 939                            SharedRuntime::is_wide_vector(max_vector_size()),
 940                            rtm_state()
 941                            );
 942 
 943     if (log() != NULL) // Print code cache state into compiler log
 944       log()-&gt;code_cache_state();
 945   }
 946 }
 947 
 948 //------------------------------Compile----------------------------------------
 949 // Compile a runtime stub
 950 Compile::Compile( ciEnv* ci_env,
 951                   TypeFunc_generator generator,
 952                   address stub_function,
 953                   const char *stub_name,
 954                   int is_fancy_jump,
 955                   bool pass_tls,
 956                   bool save_arg_registers,
 957                   bool return_pc,
 958                   DirectiveSet* directive)
 959   : Phase(Compiler),
 960     _env(ci_env),
 961     _directive(directive),
 962     _log(ci_env-&gt;log()),
 963     _compile_id(0),
 964     _save_argument_registers(save_arg_registers),
 965     _method(NULL),
 966     _stub_name(stub_name),
 967     _stub_function(stub_function),
 968     _stub_entry_point(NULL),
 969     _entry_bci(InvocationEntryBci),
 970     _initial_gvn(NULL),
 971     _for_igvn(NULL),
 972     _warm_calls(NULL),
 973     _orig_pc_slot(0),
 974     _orig_pc_slot_offset_in_bytes(0),
 975     _subsume_loads(true),
 976     _do_escape_analysis(false),
 977     _eliminate_boxing(false),
 978     _failure_reason(NULL),
 979     _code_buffer("Compile::Fill_buffer"),
 980     _has_method_handle_invokes(false),
 981     _mach_constant_base_node(NULL),
 982     _node_bundling_limit(0),
 983     _node_bundling_base(NULL),
 984     _java_calls(0),
 985     _inner_loops(0),
 986 #ifndef PRODUCT
 987     _trace_opto_output(TraceOptoOutput),
 988     _in_dump_cnt(0),
 989     _printer(NULL),
 990 #endif
 991     _comp_arena(mtCompiler),
 992     _node_arena(mtCompiler),
 993     _old_arena(mtCompiler),
 994     _Compile_types(mtCompiler),
 995     _dead_node_list(comp_arena()),
 996     _dead_node_count(0),
 997     _congraph(NULL),
 998     _replay_inline_data(NULL),
 999     _number_of_mh_late_inlines(0),
1000     _inlining_progress(false),
1001     _inlining_incrementally(false),
1002     _print_inlining_list(NULL),
1003     _print_inlining_stream(NULL),
1004     _print_inlining_idx(0),
1005     _print_inlining_output(NULL),
1006     _allowed_reasons(0),
1007     _interpreter_frame_size(0),
1008     _max_node_limit(MaxNodeLimit) {
1009   C = this;
1010 
1011   TraceTime t1(NULL, &amp;_t_totalCompilation, CITime, false);
1012   TraceTime t2(NULL, &amp;_t_stubCompilation, CITime, false);
1013 
1014 #ifndef PRODUCT
1015   set_print_assembly(PrintFrameConverterAssembly);
1016   set_parsed_irreducible_loop(false);
1017 #endif
1018   set_has_irreducible_loop(false); // no loops
1019 
1020   CompileWrapper cw(this);
1021   Init(/*AliasLevel=*/ 0);
1022   init_tf((*generator)());
1023 
1024   {
1025     // The following is a dummy for the sake of GraphKit::gen_stub
1026     Unique_Node_List for_igvn(comp_arena());
1027     set_for_igvn(&amp;for_igvn);  // not used, but some GraphKit guys push on this
1028     PhaseGVN gvn(Thread::current()-&gt;resource_area(),255);
1029     set_initial_gvn(&amp;gvn);    // not significant, but GraphKit guys use it pervasively
1030     gvn.transform_no_reclaim(top());
1031 
1032     GraphKit kit;
1033     kit.gen_stub(stub_function, stub_name, is_fancy_jump, pass_tls, return_pc);
1034   }
1035 
1036   NOT_PRODUCT( verify_graph_edges(); )
1037   Code_Gen();
1038   if (failing())  return;
1039 
1040 
1041   // Entry point will be accessed using compile-&gt;stub_entry_point();
1042   if (code_buffer() == NULL) {
1043     Matcher::soft_match_failure();
1044   } else {
1045     if (PrintAssembly &amp;&amp; (WizardMode || Verbose))
1046       tty-&gt;print_cr("### Stub::%s", stub_name);
1047 
1048     if (!failing()) {
1049       assert(_fixed_slots == 0, "no fixed slots used for runtime stubs");
1050 
1051       // Make the NMethod
1052       // For now we mark the frame as never safe for profile stackwalking
1053       RuntimeStub *rs = RuntimeStub::new_runtime_stub(stub_name,
1054                                                       code_buffer(),
1055                                                       CodeOffsets::frame_never_safe,
1056                                                       // _code_offsets.value(CodeOffsets::Frame_Complete),
1057                                                       frame_size_in_words(),
1058                                                       _oop_map_set,
1059                                                       save_arg_registers);
1060       assert(rs != NULL &amp;&amp; rs-&gt;is_runtime_stub(), "sanity check");
1061 
1062       _stub_entry_point = rs-&gt;entry_point();
1063     }
1064   }
1065 }
1066 
1067 //------------------------------Init-------------------------------------------
1068 // Prepare for a single compilation
1069 void Compile::Init(int aliaslevel) {
1070   _unique  = 0;
1071   _regalloc = NULL;
1072 
1073   _tf      = NULL;  // filled in later
1074   _top     = NULL;  // cached later
1075   _matcher = NULL;  // filled in later
1076   _cfg     = NULL;  // filled in later
1077 
1078   set_24_bit_selection_and_mode(Use24BitFP, false);
1079 
1080   _node_note_array = NULL;
1081   _default_node_notes = NULL;
1082   DEBUG_ONLY( _modified_nodes = NULL; ) // Used in Optimize()
1083 
1084   _immutable_memory = NULL; // filled in at first inquiry
1085 
1086   // Globally visible Nodes
1087   // First set TOP to NULL to give safe behavior during creation of RootNode
1088   set_cached_top_node(NULL);
1089   set_root(new RootNode());
1090   // Now that you have a Root to point to, create the real TOP
1091   set_cached_top_node( new ConNode(Type::TOP) );
1092   set_recent_alloc(NULL, NULL);
1093 
1094   // Create Debug Information Recorder to record scopes, oopmaps, etc.
1095   env()-&gt;set_oop_recorder(new OopRecorder(env()-&gt;arena()));
1096   env()-&gt;set_debug_info(new DebugInformationRecorder(env()-&gt;oop_recorder()));
1097   env()-&gt;set_dependencies(new Dependencies(env()));
1098 
1099   _fixed_slots = 0;
1100   set_has_split_ifs(false);
1101   set_has_loops(has_method() &amp;&amp; method()-&gt;has_loops()); // first approximation
1102   set_has_stringbuilder(false);
1103   set_has_boxed_value(false);
1104   _trap_can_recompile = false;  // no traps emitted yet
1105   _major_progress = true; // start out assuming good things will happen
1106   set_has_unsafe_access(false);
1107   set_max_vector_size(0);
1108   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
1109   set_decompile_count(0);
1110 
1111   set_do_freq_based_layout(_directive-&gt;BlockLayoutByFrequencyOption);
1112   set_num_loop_opts(LoopOptsCount);
1113   set_do_inlining(Inline);
1114   set_max_inline_size(MaxInlineSize);
1115   set_freq_inline_size(FreqInlineSize);
1116   set_do_scheduling(OptoScheduling);
1117   set_do_count_invocations(false);
1118   set_do_method_data_update(false);
1119 
1120   set_do_vector_loop(false);
1121 
1122   if (AllowVectorizeOnDemand) {
1123     if (has_method() &amp;&amp; (_directive-&gt;VectorizeOption || _directive-&gt;VectorizeDebugOption)) {
1124       set_do_vector_loop(true);
1125       NOT_PRODUCT(if (do_vector_loop() &amp;&amp; Verbose) {tty-&gt;print("Compile::Init: do vectorized loops (SIMD like) for method %s\n",  method()-&gt;name()-&gt;as_quoted_ascii());})
1126     } else if (has_method() &amp;&amp; method()-&gt;name() != 0 &amp;&amp;
1127                method()-&gt;intrinsic_id() == vmIntrinsics::_forEachRemaining) {
1128       set_do_vector_loop(true);
1129     }
1130   }
1131   set_use_cmove(UseCMoveUnconditionally /* || do_vector_loop()*/); //TODO: consider do_vector_loop() mandate use_cmove unconditionally
1132   NOT_PRODUCT(if (use_cmove() &amp;&amp; Verbose &amp;&amp; has_method()) {tty-&gt;print("Compile::Init: use CMove without profitability tests for method %s\n",  method()-&gt;name()-&gt;as_quoted_ascii());})
1133 
1134   set_age_code(has_method() &amp;&amp; method()-&gt;profile_aging());
1135   set_rtm_state(NoRTM); // No RTM lock eliding by default
1136   _max_node_limit = _directive-&gt;MaxNodeLimitOption;
1137 
1138 #if INCLUDE_RTM_OPT
1139   if (UseRTMLocking &amp;&amp; has_method() &amp;&amp; (method()-&gt;method_data_or_null() != NULL)) {
1140     int rtm_state = method()-&gt;method_data()-&gt;rtm_state();
1141     if (method_has_option("NoRTMLockEliding") || ((rtm_state &amp; NoRTM) != 0)) {
1142       // Don't generate RTM lock eliding code.
1143       set_rtm_state(NoRTM);
1144     } else if (method_has_option("UseRTMLockEliding") || ((rtm_state &amp; UseRTM) != 0) || !UseRTMDeopt) {
1145       // Generate RTM lock eliding code without abort ratio calculation code.
1146       set_rtm_state(UseRTM);
1147     } else if (UseRTMDeopt) {
1148       // Generate RTM lock eliding code and include abort ratio calculation
1149       // code if UseRTMDeopt is on.
1150       set_rtm_state(ProfileRTM);
1151     }
1152   }
1153 #endif
1154   if (debug_info()-&gt;recording_non_safepoints()) {
1155     set_node_note_array(new(comp_arena()) GrowableArray&lt;Node_Notes*&gt;
1156                         (comp_arena(), 8, 0, NULL));
1157     set_default_node_notes(Node_Notes::make(this));
1158   }
1159 
1160   // // -- Initialize types before each compile --
1161   // // Update cached type information
1162   // if( _method &amp;&amp; _method-&gt;constants() )
1163   //   Type::update_loaded_types(_method, _method-&gt;constants());
1164 
1165   // Init alias_type map.
1166   if (!_do_escape_analysis &amp;&amp; aliaslevel == 3)
1167     aliaslevel = 2;  // No unique types without escape analysis
1168   _AliasLevel = aliaslevel;
1169   const int grow_ats = 16;
1170   _max_alias_types = grow_ats;
1171   _alias_types   = NEW_ARENA_ARRAY(comp_arena(), AliasType*, grow_ats);
1172   AliasType* ats = NEW_ARENA_ARRAY(comp_arena(), AliasType,  grow_ats);
1173   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
1174   {
1175     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
1176   }
1177   // Initialize the first few types.
1178   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
1179   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1180   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1181   _num_alias_types = AliasIdxRaw+1;
1182   // Zero out the alias type cache.
1183   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1184   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1185   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1186 
1187   _intrinsics = NULL;
1188   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1189   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1190   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1191   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1192   register_library_intrinsics();
1193 }
1194 
1195 //---------------------------init_start----------------------------------------
1196 // Install the StartNode on this compile object.
1197 void Compile::init_start(StartNode* s) {
1198   if (failing())
1199     return; // already failing
1200   assert(s == start(), "");
1201 }
1202 
1203 /**
1204  * Return the 'StartNode'. We must not have a pending failure, since the ideal graph
1205  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1206  * the ideal graph.
1207  */
1208 StartNode* Compile::start() const {
1209   assert (!failing(), "Must not have pending failure. Reason is: %s", failure_reason());
1210   for (DUIterator_Fast imax, i = root()-&gt;fast_outs(imax); i &lt; imax; i++) {
1211     Node* start = root()-&gt;fast_out(i);
1212     if (start-&gt;is_Start()) {
1213       return start-&gt;as_Start();
1214     }
1215   }
1216   fatal("Did not find Start node!");
1217   return NULL;
1218 }
1219 
1220 //-------------------------------immutable_memory-------------------------------------
1221 // Access immutable memory
1222 Node* Compile::immutable_memory() {
1223   if (_immutable_memory != NULL) {
1224     return _immutable_memory;
1225   }
1226   StartNode* s = start();
1227   for (DUIterator_Fast imax, i = s-&gt;fast_outs(imax); true; i++) {
1228     Node *p = s-&gt;fast_out(i);
1229     if (p != s &amp;&amp; p-&gt;as_Proj()-&gt;_con == TypeFunc::Memory) {
1230       _immutable_memory = p;
1231       return _immutable_memory;
1232     }
1233   }
1234   ShouldNotReachHere();
1235   return NULL;
1236 }
1237 
1238 //----------------------set_cached_top_node------------------------------------
1239 // Install the cached top node, and make sure Node::is_top works correctly.
1240 void Compile::set_cached_top_node(Node* tn) {
1241   if (tn != NULL)  verify_top(tn);
1242   Node* old_top = _top;
1243   _top = tn;
1244   // Calling Node::setup_is_top allows the nodes the chance to adjust
1245   // their _out arrays.
1246   if (_top != NULL)     _top-&gt;setup_is_top();
1247   if (old_top != NULL)  old_top-&gt;setup_is_top();
1248   assert(_top == NULL || top()-&gt;is_top(), "");
1249 }
1250 
1251 #ifdef ASSERT
1252 uint Compile::count_live_nodes_by_graph_walk() {
1253   Unique_Node_List useful(comp_arena());
1254   // Get useful node list by walking the graph.
1255   identify_useful_nodes(useful);
1256   return useful.size();
1257 }
1258 
1259 void Compile::print_missing_nodes() {
1260 
1261   // Return if CompileLog is NULL and PrintIdealNodeCount is false.
1262   if ((_log == NULL) &amp;&amp; (! PrintIdealNodeCount)) {
1263     return;
1264   }
1265 
1266   // This is an expensive function. It is executed only when the user
1267   // specifies VerifyIdealNodeCount option or otherwise knows the
1268   // additional work that needs to be done to identify reachable nodes
1269   // by walking the flow graph and find the missing ones using
1270   // _dead_node_list.
1271 
1272   Unique_Node_List useful(comp_arena());
1273   // Get useful node list by walking the graph.
1274   identify_useful_nodes(useful);
1275 
1276   uint l_nodes = C-&gt;live_nodes();
1277   uint l_nodes_by_walk = useful.size();
1278 
1279   if (l_nodes != l_nodes_by_walk) {
1280     if (_log != NULL) {
1281       _log-&gt;begin_head("mismatched_nodes count='%d'", abs((int) (l_nodes - l_nodes_by_walk)));
1282       _log-&gt;stamp();
1283       _log-&gt;end_head();
1284     }
1285     VectorSet&amp; useful_member_set = useful.member_set();
1286     int last_idx = l_nodes_by_walk;
1287     for (int i = 0; i &lt; last_idx; i++) {
1288       if (useful_member_set.test(i)) {
1289         if (_dead_node_list.test(i)) {
1290           if (_log != NULL) {
1291             _log-&gt;elem("mismatched_node_info node_idx='%d' type='both live and dead'", i);
1292           }
1293           if (PrintIdealNodeCount) {
1294             // Print the log message to tty
1295               tty-&gt;print_cr("mismatched_node idx='%d' both live and dead'", i);
1296               useful.at(i)-&gt;dump();
1297           }
1298         }
1299       }
1300       else if (! _dead_node_list.test(i)) {
1301         if (_log != NULL) {
1302           _log-&gt;elem("mismatched_node_info node_idx='%d' type='neither live nor dead'", i);
1303         }
1304         if (PrintIdealNodeCount) {
1305           // Print the log message to tty
1306           tty-&gt;print_cr("mismatched_node idx='%d' type='neither live nor dead'", i);
1307         }
1308       }
1309     }
1310     if (_log != NULL) {
1311       _log-&gt;tail("mismatched_nodes");
1312     }
1313   }
1314 }
1315 void Compile::record_modified_node(Node* n) {
1316   if (_modified_nodes != NULL &amp;&amp; !_inlining_incrementally &amp;&amp;
1317       n-&gt;outcnt() != 0 &amp;&amp; !n-&gt;is_Con()) {
1318     _modified_nodes-&gt;push(n);
1319   }
1320 }
1321 
1322 void Compile::remove_modified_node(Node* n) {
1323   if (_modified_nodes != NULL) {
1324     _modified_nodes-&gt;remove(n);
1325   }
1326 }
1327 #endif
1328 
1329 #ifndef PRODUCT
1330 void Compile::verify_top(Node* tn) const {
1331   if (tn != NULL) {
1332     assert(tn-&gt;is_Con(), "top node must be a constant");
1333     assert(((ConNode*)tn)-&gt;type() == Type::TOP, "top node must have correct type");
1334     assert(tn-&gt;in(0) != NULL, "must have live top node");
1335   }
1336 }
1337 #endif
1338 
1339 
1340 ///-------------------Managing Per-Node Debug &amp; Profile Info-------------------
1341 
1342 void Compile::grow_node_notes(GrowableArray&lt;Node_Notes*&gt;* arr, int grow_by) {
1343   guarantee(arr != NULL, "");
1344   int num_blocks = arr-&gt;length();
1345   if (grow_by &lt; num_blocks)  grow_by = num_blocks;
1346   int num_notes = grow_by * _node_notes_block_size;
1347   Node_Notes* notes = NEW_ARENA_ARRAY(node_arena(), Node_Notes, num_notes);
1348   Copy::zero_to_bytes(notes, num_notes * sizeof(Node_Notes));
1349   while (num_notes &gt; 0) {
1350     arr-&gt;append(notes);
1351     notes     += _node_notes_block_size;
1352     num_notes -= _node_notes_block_size;
1353   }
1354   assert(num_notes == 0, "exact multiple, please");
1355 }
1356 
1357 bool Compile::copy_node_notes_to(Node* dest, Node* source) {
1358   if (source == NULL || dest == NULL)  return false;
1359 
1360   if (dest-&gt;is_Con())
1361     return false;               // Do not push debug info onto constants.
1362 
1363 #ifdef ASSERT
1364   // Leave a bread crumb trail pointing to the original node:
1365   if (dest != NULL &amp;&amp; dest != source &amp;&amp; dest-&gt;debug_orig() == NULL) {
1366     dest-&gt;set_debug_orig(source);
1367   }
1368 #endif
1369 
1370   if (node_note_array() == NULL)
1371     return false;               // Not collecting any notes now.
1372 
1373   // This is a copy onto a pre-existing node, which may already have notes.
1374   // If both nodes have notes, do not overwrite any pre-existing notes.
1375   Node_Notes* source_notes = node_notes_at(source-&gt;_idx);
1376   if (source_notes == NULL || source_notes-&gt;is_clear())  return false;
1377   Node_Notes* dest_notes   = node_notes_at(dest-&gt;_idx);
1378   if (dest_notes == NULL || dest_notes-&gt;is_clear()) {
1379     return set_node_notes_at(dest-&gt;_idx, source_notes);
1380   }
1381 
1382   Node_Notes merged_notes = (*source_notes);
1383   // The order of operations here ensures that dest notes will win...
1384   merged_notes.update_from(dest_notes);
1385   return set_node_notes_at(dest-&gt;_idx, &amp;merged_notes);
1386 }
1387 
1388 
1389 //--------------------------allow_range_check_smearing-------------------------
1390 // Gating condition for coalescing similar range checks.
1391 // Sometimes we try 'speculatively' replacing a series of a range checks by a
1392 // single covering check that is at least as strong as any of them.
1393 // If the optimization succeeds, the simplified (strengthened) range check
1394 // will always succeed.  If it fails, we will deopt, and then give up
1395 // on the optimization.
1396 bool Compile::allow_range_check_smearing() const {
1397   // If this method has already thrown a range-check,
1398   // assume it was because we already tried range smearing
1399   // and it failed.
1400   uint already_trapped = trap_count(Deoptimization::Reason_range_check);
1401   return !already_trapped;
1402 }
1403 
1404 
1405 //------------------------------flatten_alias_type-----------------------------
1406 const TypePtr *Compile::flatten_alias_type( const TypePtr *tj ) const {
1407   int offset = tj-&gt;offset();
1408   TypePtr::PTR ptr = tj-&gt;ptr();
1409 
1410   // Known instance (scalarizable allocation) alias only with itself.
1411   bool is_known_inst = tj-&gt;isa_oopptr() != NULL &amp;&amp;
1412                        tj-&gt;is_oopptr()-&gt;is_known_instance();
1413 
1414   // Process weird unsafe references.
1415   if (offset == Type::OffsetBot &amp;&amp; (tj-&gt;isa_instptr() /*|| tj-&gt;isa_klassptr()*/)) {
1416     assert(InlineUnsafeOps, "indeterminate pointers come only from unsafe ops");
1417     assert(!is_known_inst, "scalarizable allocation should not have unsafe references");
1418     tj = TypeOopPtr::BOTTOM;
1419     ptr = tj-&gt;ptr();
1420     offset = tj-&gt;offset();
1421   }
1422 
1423   // Array pointers need some flattening
1424   const TypeAryPtr *ta = tj-&gt;isa_aryptr();
1425   if (ta &amp;&amp; ta-&gt;is_stable()) {
1426     // Erase stability property for alias analysis.
1427     tj = ta = ta-&gt;cast_to_stable(false);
1428   }
1429   if( ta &amp;&amp; is_known_inst ) {
1430     if ( offset != Type::OffsetBot &amp;&amp;
1431          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1432       offset = Type::OffsetBot; // Flatten constant access into array body only
1433       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, offset, ta-&gt;instance_id());
1434     }
1435   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1436     // For arrays indexed by constant indices, we flatten the alias
1437     // space to include all of the array body.  Only the header, klass
1438     // and array length can be accessed un-aliased.
1439     if( offset != Type::OffsetBot ) {
1440       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1441         offset = Type::OffsetBot;   // Flatten constant access into array body
1442         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,offset);
1443       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1444         // range is OK as-is.
1445         tj = ta = TypeAryPtr::RANGE;
1446       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1447         tj = TypeInstPtr::KLASS; // all klass loads look alike
1448         ta = TypeAryPtr::RANGE; // generic ignored junk
1449         ptr = TypePtr::BotPTR;
1450       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1451         tj = TypeInstPtr::MARK;
1452         ta = TypeAryPtr::RANGE; // generic ignored junk
1453         ptr = TypePtr::BotPTR;
1454       } else {                  // Random constant offset into array body
1455         offset = Type::OffsetBot;   // Flatten constant access into array body
1456         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,offset);
1457       }
1458     }
1459     // Arrays of fixed size alias with arrays of unknown size.
1460     if (ta-&gt;size() != TypeInt::POS) {
1461       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
1462       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,offset);
1463     }
1464     // Arrays of known objects become arrays of unknown objects.
1465     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1466       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
1467       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);
1468     }
1469     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1470       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
1471       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);
1472     }
1473     // Arrays of bytes and of booleans both use 'bastore' and 'baload' so
1474     // cannot be distinguished by bytecode alone.
1475     if (ta-&gt;elem() == TypeInt::BOOL) {
1476       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1477       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
1478       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,offset);
1479     }
1480     // During the 2nd round of IterGVN, NotNull castings are removed.
1481     // Make sure the Bottom and NotNull variants alias the same.
1482     // Also, make sure exact and non-exact variants alias the same.
1483     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
1484       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,offset);
1485     }
1486   }
1487 
1488   // Oop pointers need some flattening
1489   const TypeInstPtr *to = tj-&gt;isa_instptr();
1490   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1491     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1492     if( ptr == TypePtr::Constant ) {
1493       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1494           offset &lt; k-&gt;size_helper() * wordSize) {
1495         // No constant oop pointers (such as Strings); they alias with
1496         // unknown strings.
1497         assert(!is_known_inst, "not scalarizable allocation");
1498         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);
1499       }
1500     } else if( is_known_inst ) {
1501       tj = to; // Keep NotNull and klass_is_exact for instance type
1502     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1503       // During the 2nd round of IterGVN, NotNull castings are removed.
1504       // Make sure the Bottom and NotNull variants alias the same.
1505       // Also, make sure exact and non-exact variants alias the same.
1506       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);
1507     }
1508     if (to-&gt;speculative() != NULL) {
1509       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),to-&gt;offset(), to-&gt;instance_id());
1510     }
1511     // Canonicalize the holder of this field
1512     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1513       // First handle header references such as a LoadKlassNode, even if the
1514       // object's klass is unloaded at compile time (4965979).
1515       if (!is_known_inst) { // Do it only for non-instance types
1516         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, offset);
1517       }
1518     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1519       // Static fields are in the space above the normal instance
1520       // fields in the java.lang.Class instance.
1521       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1522         to = NULL;
1523         tj = TypeOopPtr::BOTTOM;
1524         offset = tj-&gt;offset();
1525       }
1526     } else {
1527       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1528       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1529         if( is_known_inst ) {
1530           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, offset, to-&gt;instance_id());
1531         } else {
1532           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, offset);
1533         }
1534       }
1535     }
1536   }
1537 
1538   // Klass pointers to object array klasses need some flattening
1539   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1540   if( tk ) {
1541     // If we are referencing a field within a Klass, we need
1542     // to assume the worst case of an Object.  Both exact and
1543     // inexact types must flatten to the same alias class so
1544     // use NotNull as the PTR.
1545     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1546 
1547       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1548                                    TypeKlassPtr::OBJECT-&gt;klass(),
1549                                    offset);
1550     }
1551 
1552     ciKlass* klass = tk-&gt;klass();
1553     if( klass-&gt;is_obj_array_klass() ) {
1554       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1555       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1556         k = TypeInstPtr::BOTTOM-&gt;klass();
1557       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );
1558     }
1559 
1560     // Check for precise loads from the primary supertype array and force them
1561     // to the supertype cache alias index.  Check for generic array loads from
1562     // the primary supertype array and also force them to the supertype cache
1563     // alias index.  Since the same load can reach both, we need to merge
1564     // these 2 disparate memories into the same alias class.  Since the
1565     // primary supertype array is read-only, there's no chance of confusion
1566     // where we bypass an array load and an array store.
1567     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1568     if (offset == Type::OffsetBot ||
1569         (offset &gt;= primary_supers_offset &amp;&amp;
1570          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1571         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1572       offset = in_bytes(Klass::secondary_super_cache_offset());
1573       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk-&gt;klass(), offset );
1574     }
1575   }
1576 
1577   // Flatten all Raw pointers together.
1578   if (tj-&gt;base() == Type::RawPtr)
1579     tj = TypeRawPtr::BOTTOM;
1580 
1581   if (tj-&gt;base() == Type::AnyPtr)
1582     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1583 
1584   // Flatten all to bottom for now
1585   switch( _AliasLevel ) {
1586   case 0:
1587     tj = TypePtr::BOTTOM;
1588     break;
1589   case 1:                       // Flatten to: oop, static, field or array
1590     switch (tj-&gt;base()) {
1591     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1592     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1593     case Type::AryPtr:   // do not distinguish arrays at all
1594     case Type::InstPtr:  tj = TypeInstPtr::BOTTOM;  break;
1595     case Type::KlassPtr: tj = TypeKlassPtr::OBJECT; break;
1596     case Type::AnyPtr:   tj = TypePtr::BOTTOM;      break;  // caller checks it
1597     default: ShouldNotReachHere();
1598     }
1599     break;
1600   case 2:                       // No collapsing at level 2; keep all splits
1601   case 3:                       // No collapsing at level 3; keep all splits
1602     break;
1603   default:
1604     Unimplemented();
1605   }
1606 
1607   offset = tj-&gt;offset();
1608   assert( offset != Type::OffsetTop, "Offset has fallen from constant" );
1609 
1610   assert( (offset != Type::OffsetBot &amp;&amp; tj-&gt;base() != Type::AryPtr) ||
1611           (offset == Type::OffsetBot &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1612           (offset == Type::OffsetBot &amp;&amp; tj == TypeOopPtr::BOTTOM) ||
1613           (offset == Type::OffsetBot &amp;&amp; tj == TypePtr::BOTTOM) ||
1614           (offset == oopDesc::mark_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1615           (offset == oopDesc::klass_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1616           (offset == arrayOopDesc::length_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr)  ,
1617           "For oops, klasses, raw offset must be constant; for arrays the offset is never known" );
1618   assert( tj-&gt;ptr() != TypePtr::TopPTR &amp;&amp;
1619           tj-&gt;ptr() != TypePtr::AnyNull &amp;&amp;
1620           tj-&gt;ptr() != TypePtr::Null, "No imprecise addresses" );
1621 //    assert( tj-&gt;ptr() != TypePtr::Constant ||
1622 //            tj-&gt;base() == Type::RawPtr ||
1623 //            tj-&gt;base() == Type::KlassPtr, "No constant oop addresses" );
1624 
1625   return tj;
1626 }
1627 
1628 void Compile::AliasType::Init(int i, const TypePtr* at) {
1629   _index = i;
1630   _adr_type = at;
1631   _field = NULL;
1632   _element = NULL;
1633   _is_rewritable = true; // default
1634   const TypeOopPtr *atoop = (at != NULL) ? at-&gt;isa_oopptr() : NULL;
1635   if (atoop != NULL &amp;&amp; atoop-&gt;is_known_instance()) {
1636     const TypeOopPtr *gt = atoop-&gt;cast_to_instance_id(TypeOopPtr::InstanceBot);
1637     _general_index = Compile::current()-&gt;get_alias_index(gt);
1638   } else {
1639     _general_index = 0;
1640   }
1641 }
1642 
1643 //---------------------------------print_on------------------------------------
1644 #ifndef PRODUCT
1645 void Compile::AliasType::print_on(outputStream* st) {
1646   if (index() &lt; 10)
1647         st-&gt;print("@ &lt;%d&gt; ", index());
1648   else  st-&gt;print("@ &lt;%d&gt;",  index());
1649   st-&gt;print(is_rewritable() ? "   " : " RO");
1650   int offset = adr_type()-&gt;offset();
1651   if (offset == Type::OffsetBot)
1652         st-&gt;print(" +any");
1653   else  st-&gt;print(" +%-3d", offset);
1654   st-&gt;print(" in ");
1655   adr_type()-&gt;dump_on(st);
1656   const TypeOopPtr* tjp = adr_type()-&gt;isa_oopptr();
1657   if (field() != NULL &amp;&amp; tjp) {
1658     if (tjp-&gt;klass()  != field()-&gt;holder() ||
1659         tjp-&gt;offset() != field()-&gt;offset_in_bytes()) {
1660       st-&gt;print(" != ");
1661       field()-&gt;print();
1662       st-&gt;print(" ***");
1663     }
1664   }
1665 }
1666 
1667 void print_alias_types() {
1668   Compile* C = Compile::current();
1669   tty-&gt;print_cr("--- Alias types, AliasIdxBot .. %d", C-&gt;num_alias_types()-1);
1670   for (int idx = Compile::AliasIdxBot; idx &lt; C-&gt;num_alias_types(); idx++) {
1671     C-&gt;alias_type(idx)-&gt;print_on(tty);
1672     tty-&gt;cr();
1673   }
1674 }
1675 #endif
1676 
1677 
1678 //----------------------------probe_alias_cache--------------------------------
1679 Compile::AliasCacheEntry* Compile::probe_alias_cache(const TypePtr* adr_type) {
1680   intptr_t key = (intptr_t) adr_type;
1681   key ^= key &gt;&gt; logAliasCacheSize;
1682   return &amp;_alias_cache[key &amp; right_n_bits(logAliasCacheSize)];
1683 }
1684 
1685 
1686 //-----------------------------grow_alias_types--------------------------------
1687 void Compile::grow_alias_types() {
1688   const int old_ats  = _max_alias_types; // how many before?
1689   const int new_ats  = old_ats;          // how many more?
1690   const int grow_ats = old_ats+new_ats;  // how many now?
1691   _max_alias_types = grow_ats;
1692   _alias_types =  REALLOC_ARENA_ARRAY(comp_arena(), AliasType*, _alias_types, old_ats, grow_ats);
1693   AliasType* ats =    NEW_ARENA_ARRAY(comp_arena(), AliasType, new_ats);
1694   Copy::zero_to_bytes(ats, sizeof(AliasType)*new_ats);
1695   for (int i = 0; i &lt; new_ats; i++)  _alias_types[old_ats+i] = &amp;ats[i];
1696 }
1697 
1698 
1699 //--------------------------------find_alias_type------------------------------
1700 Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {
1701   if (_AliasLevel == 0)
1702     return alias_type(AliasIdxBot);
1703 
1704   AliasCacheEntry* ace = probe_alias_cache(adr_type);
1705   if (ace-&gt;_adr_type == adr_type) {
1706     return alias_type(ace-&gt;_index);
1707   }
1708 
1709   // Handle special cases.
1710   if (adr_type == NULL)             return alias_type(AliasIdxTop);
1711   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
1712 
1713   // Do it the slow way.
1714   const TypePtr* flat = flatten_alias_type(adr_type);
1715 
1716 #ifdef ASSERT
1717   assert(flat == flatten_alias_type(flat), "idempotent");
1718   assert(flat != TypePtr::BOTTOM,     "cannot alias-analyze an untyped ptr");
1719   if (flat-&gt;isa_oopptr() &amp;&amp; !flat-&gt;isa_klassptr()) {
1720     const TypeOopPtr* foop = flat-&gt;is_oopptr();
1721     // Scalarizable allocations have exact klass always.
1722     bool exact = !foop-&gt;klass_is_exact() || foop-&gt;is_known_instance();
1723     const TypePtr* xoop = foop-&gt;cast_to_exactness(exact)-&gt;is_ptr();
1724     assert(foop == flatten_alias_type(xoop), "exactness must not affect alias type");
1725   }
1726   assert(flat == flatten_alias_type(flat), "exact bit doesn't matter");
1727 #endif
1728 
1729   int idx = AliasIdxTop;
1730   for (int i = 0; i &lt; num_alias_types(); i++) {
1731     if (alias_type(i)-&gt;adr_type() == flat) {
1732       idx = i;
1733       break;
1734     }
1735   }
1736 
1737   if (idx == AliasIdxTop) {
1738     if (no_create)  return NULL;
1739     // Grow the array if necessary.
1740     if (_num_alias_types == _max_alias_types)  grow_alias_types();
1741     // Add a new alias type.
1742     idx = _num_alias_types++;
1743     _alias_types[idx]-&gt;Init(idx, flat);
1744     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1745     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1746     if (flat-&gt;isa_instptr()) {
1747       if (flat-&gt;offset() == java_lang_Class::klass_offset_in_bytes()
1748           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1749         alias_type(idx)-&gt;set_rewritable(false);
1750     }
1751     if (flat-&gt;isa_aryptr()) {
1752 #ifdef ASSERT
1753       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1754       // (T_BYTE has the weakest alignment and size restrictions...)
1755       assert(flat-&gt;offset() &lt; header_size_min, "array body reference must be OffsetBot");
1756 #endif
1757       if (flat-&gt;offset() == TypePtr::OffsetBot) {
1758         alias_type(idx)-&gt;set_element(flat-&gt;is_aryptr()-&gt;elem());
1759       }
1760     }
1761     if (flat-&gt;isa_klassptr()) {
1762       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1763         alias_type(idx)-&gt;set_rewritable(false);
1764       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1765         alias_type(idx)-&gt;set_rewritable(false);
1766       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1767         alias_type(idx)-&gt;set_rewritable(false);
1768       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1769         alias_type(idx)-&gt;set_rewritable(false);
1770     }
1771     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1772     // but the base pointer type is not distinctive enough to identify
1773     // references into JavaThread.)
1774 
1775     // Check for final fields.
1776     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1777     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {
1778       ciField* field;
1779       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1780           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1781           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1782         // static field
1783         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1784         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
1785       } else {
1786         ciInstanceKlass *k = tinst-&gt;klass()-&gt;as_instance_klass();
1787         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1788       }
1789       assert(field == NULL ||
1790              original_field == NULL ||
1791              (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;
1792               field-&gt;offset() == original_field-&gt;offset() &amp;&amp;
1793               field-&gt;is_static() == original_field-&gt;is_static()), "wrong field?");
1794       // Set field() and is_rewritable() attributes.
1795       if (field != NULL)  alias_type(idx)-&gt;set_field(field);
1796     }
1797   }
1798 
1799   // Fill the cache for next time.
1800   ace-&gt;_adr_type = adr_type;
1801   ace-&gt;_index    = idx;
1802   assert(alias_type(adr_type) == alias_type(idx),  "type must be installed");
1803 
1804   // Might as well try to fill the cache for the flattened version, too.
1805   AliasCacheEntry* face = probe_alias_cache(flat);
1806   if (face-&gt;_adr_type == NULL) {
1807     face-&gt;_adr_type = flat;
1808     face-&gt;_index    = idx;
1809     assert(alias_type(flat) == alias_type(idx), "flat type must work too");
1810   }
1811 
1812   return alias_type(idx);
1813 }
1814 
1815 
1816 Compile::AliasType* Compile::alias_type(ciField* field) {
1817   const TypeOopPtr* t;
1818   if (field-&gt;is_static())
1819     t = TypeInstPtr::make(field-&gt;holder()-&gt;java_mirror());
1820   else
1821     t = TypeOopPtr::make_from_klass_raw(field-&gt;holder());
1822   AliasType* atp = alias_type(t-&gt;add_offset(field-&gt;offset_in_bytes()), field);
1823   assert((field-&gt;is_final() || field-&gt;is_stable()) == !atp-&gt;is_rewritable(), "must get the rewritable bits correct");
1824   return atp;
1825 }
1826 
1827 
1828 //------------------------------have_alias_type--------------------------------
1829 bool Compile::have_alias_type(const TypePtr* adr_type) {
1830   AliasCacheEntry* ace = probe_alias_cache(adr_type);
1831   if (ace-&gt;_adr_type == adr_type) {
1832     return true;
1833   }
1834 
1835   // Handle special cases.
1836   if (adr_type == NULL)             return true;
1837   if (adr_type == TypePtr::BOTTOM)  return true;
1838 
1839   return find_alias_type(adr_type, true, NULL) != NULL;
1840 }
1841 
1842 //-----------------------------must_alias--------------------------------------
1843 // True if all values of the given address type are in the given alias category.
1844 bool Compile::must_alias(const TypePtr* adr_type, int alias_idx) {
1845   if (alias_idx == AliasIdxBot)         return true;  // the universal category
1846   if (adr_type == NULL)                 return true;  // NULL serves as TypePtr::TOP
1847   if (alias_idx == AliasIdxTop)         return false; // the empty category
1848   if (adr_type-&gt;base() == Type::AnyPtr) return false; // TypePtr::BOTTOM or its twins
1849 
1850   // the only remaining possible overlap is identity
1851   int adr_idx = get_alias_index(adr_type);
1852   assert(adr_idx != AliasIdxBot &amp;&amp; adr_idx != AliasIdxTop, "");
1853   assert(adr_idx == alias_idx ||
1854          (alias_type(alias_idx)-&gt;adr_type() != TypeOopPtr::BOTTOM
1855           &amp;&amp; adr_type                       != TypeOopPtr::BOTTOM),
1856          "should not be testing for overlap with an unsafe pointer");
1857   return adr_idx == alias_idx;
1858 }
1859 
1860 //------------------------------can_alias--------------------------------------
1861 // True if any values of the given address type are in the given alias category.
1862 bool Compile::can_alias(const TypePtr* adr_type, int alias_idx) {
1863   if (alias_idx == AliasIdxTop)         return false; // the empty category
1864   if (adr_type == NULL)                 return false; // NULL serves as TypePtr::TOP
1865   if (alias_idx == AliasIdxBot)         return true;  // the universal category
1866   if (adr_type-&gt;base() == Type::AnyPtr) return true;  // TypePtr::BOTTOM or its twins
1867 
1868   // the only remaining possible overlap is identity
1869   int adr_idx = get_alias_index(adr_type);
1870   assert(adr_idx != AliasIdxBot &amp;&amp; adr_idx != AliasIdxTop, "");
1871   return adr_idx == alias_idx;
1872 }
1873 
1874 
1875 
1876 //---------------------------pop_warm_call-------------------------------------
1877 WarmCallInfo* Compile::pop_warm_call() {
1878   WarmCallInfo* wci = _warm_calls;
1879   if (wci != NULL)  _warm_calls = wci-&gt;remove_from(wci);
1880   return wci;
1881 }
1882 
1883 //----------------------------Inline_Warm--------------------------------------
1884 int Compile::Inline_Warm() {
1885   // If there is room, try to inline some more warm call sites.
1886   // %%% Do a graph index compaction pass when we think we're out of space?
1887   if (!InlineWarmCalls)  return 0;
1888 
1889   int calls_made_hot = 0;
1890   int room_to_grow   = NodeCountInliningCutoff - unique();
1891   int amount_to_grow = MIN2(room_to_grow, (int)NodeCountInliningStep);
1892   int amount_grown   = 0;
1893   WarmCallInfo* call;
1894   while (amount_to_grow &gt; 0 &amp;&amp; (call = pop_warm_call()) != NULL) {
1895     int est_size = (int)call-&gt;size();
1896     if (est_size &gt; (room_to_grow - amount_grown)) {
1897       // This one won't fit anyway.  Get rid of it.
1898       call-&gt;make_cold();
1899       continue;
1900     }
1901     call-&gt;make_hot();
1902     calls_made_hot++;
1903     amount_grown   += est_size;
1904     amount_to_grow -= est_size;
1905   }
1906 
1907   if (calls_made_hot &gt; 0)  set_major_progress();
1908   return calls_made_hot;
1909 }
1910 
1911 
1912 //----------------------------Finish_Warm--------------------------------------
1913 void Compile::Finish_Warm() {
1914   if (!InlineWarmCalls)  return;
1915   if (failing())  return;
1916   if (warm_calls() == NULL)  return;
1917 
1918   // Clean up loose ends, if we are out of space for inlining.
1919   WarmCallInfo* call;
1920   while ((call = pop_warm_call()) != NULL) {
1921     call-&gt;make_cold();
1922   }
1923 }
1924 
1925 //---------------------cleanup_loop_predicates-----------------------
1926 // Remove the opaque nodes that protect the predicates so that all unused
1927 // checks and uncommon_traps will be eliminated from the ideal graph
1928 void Compile::cleanup_loop_predicates(PhaseIterGVN &amp;igvn) {
1929   if (predicate_count()==0) return;
1930   for (int i = predicate_count(); i &gt; 0; i--) {
1931     Node * n = predicate_opaque1_node(i-1);
1932     assert(n-&gt;Opcode() == Op_Opaque1, "must be");
1933     igvn.replace_node(n, n-&gt;in(1));
1934   }
1935   assert(predicate_count()==0, "should be clean!");
1936 }
1937 
1938 void Compile::add_range_check_cast(Node* n) {
1939   assert(n-&gt;isa_CastII()-&gt;has_range_check(), "CastII should have range check dependency");
1940   assert(!_range_check_casts-&gt;contains(n), "duplicate entry in range check casts");
1941   _range_check_casts-&gt;append(n);
1942 }
1943 
1944 // Remove all range check dependent CastIINodes.
1945 void Compile::remove_range_check_casts(PhaseIterGVN &amp;igvn) {
1946   for (int i = range_check_cast_count(); i &gt; 0; i--) {
1947     Node* cast = range_check_cast_node(i-1);
1948     assert(cast-&gt;isa_CastII()-&gt;has_range_check(), "CastII should have range check dependency");
1949     igvn.replace_node(cast, cast-&gt;in(1));
1950   }
1951   assert(range_check_cast_count() == 0, "should be empty");
1952 }
1953 
1954 // StringOpts and late inlining of string methods
1955 void Compile::inline_string_calls(bool parse_time) {
1956   {
1957     // remove useless nodes to make the usage analysis simpler
1958     ResourceMark rm;
1959     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
1960   }
1961 
1962   {
1963     ResourceMark rm;
1964     print_method(PHASE_BEFORE_STRINGOPTS, 3);
1965     PhaseStringOpts pso(initial_gvn(), for_igvn());
1966     print_method(PHASE_AFTER_STRINGOPTS, 3);
1967   }
1968 
1969   // now inline anything that we skipped the first time around
1970   if (!parse_time) {
1971     _late_inlines_pos = _late_inlines.length();
1972   }
1973 
1974   while (_string_late_inlines.length() &gt; 0) {
1975     CallGenerator* cg = _string_late_inlines.pop();
1976     cg-&gt;do_late_inline();
1977     if (failing())  return;
1978   }
1979   _string_late_inlines.trunc_to(0);
1980 }
1981 
1982 // Late inlining of boxing methods
1983 void Compile::inline_boxing_calls(PhaseIterGVN&amp; igvn) {
1984   if (_boxing_late_inlines.length() &gt; 0) {
1985     assert(has_boxed_value(), "inconsistent");
1986 
1987     PhaseGVN* gvn = initial_gvn();
1988     set_inlining_incrementally(true);
1989 
1990     assert( igvn._worklist.size() == 0, "should be done with igvn" );
1991     for_igvn()-&gt;clear();
1992     gvn-&gt;replace_with(&amp;igvn);
1993 
1994     _late_inlines_pos = _late_inlines.length();
1995 
1996     while (_boxing_late_inlines.length() &gt; 0) {
1997       CallGenerator* cg = _boxing_late_inlines.pop();
1998       cg-&gt;do_late_inline();
1999       if (failing())  return;
2000     }
2001     _boxing_late_inlines.trunc_to(0);
2002 
2003     {
2004       ResourceMark rm;
2005       PhaseRemoveUseless pru(gvn, for_igvn());
2006     }
2007 
2008     igvn = PhaseIterGVN(gvn);
2009     igvn.optimize();
2010 
2011     set_inlining_progress(false);
2012     set_inlining_incrementally(false);
2013   }
2014 }
2015 
2016 void Compile::inline_incrementally_one(PhaseIterGVN&amp; igvn) {
2017   assert(IncrementalInline, "incremental inlining should be on");
2018   PhaseGVN* gvn = initial_gvn();
2019 
2020   set_inlining_progress(false);
2021   for_igvn()-&gt;clear();
2022   gvn-&gt;replace_with(&amp;igvn);
2023 
2024   {
2025     TracePhase tp("incrementalInline_inline", &amp;timers[_t_incrInline_inline]);
2026     int i = 0;
2027     for (; i &lt;_late_inlines.length() &amp;&amp; !inlining_progress(); i++) {
2028       CallGenerator* cg = _late_inlines.at(i);
2029       _late_inlines_pos = i+1;
2030       cg-&gt;do_late_inline();
2031       if (failing())  return;
2032     }
2033     int j = 0;
2034     for (; i &lt; _late_inlines.length(); i++, j++) {
2035       _late_inlines.at_put(j, _late_inlines.at(i));
2036     }
2037     _late_inlines.trunc_to(j);
2038   }
2039 
2040   {
2041     TracePhase tp("incrementalInline_pru", &amp;timers[_t_incrInline_pru]);
2042     ResourceMark rm;
2043     PhaseRemoveUseless pru(gvn, for_igvn());
2044   }
2045 
2046   {
2047     TracePhase tp("incrementalInline_igvn", &amp;timers[_t_incrInline_igvn]);
2048     igvn = PhaseIterGVN(gvn);
2049   }
2050 }
2051 
2052 // Perform incremental inlining until bound on number of live nodes is reached
2053 void Compile::inline_incrementally(PhaseIterGVN&amp; igvn) {
2054   TracePhase tp("incrementalInline", &amp;timers[_t_incrInline]);
2055 
2056   PhaseGVN* gvn = initial_gvn();
2057 
2058   set_inlining_incrementally(true);
2059   set_inlining_progress(true);
2060   uint low_live_nodes = 0;
2061 
2062   while(inlining_progress() &amp;&amp; _late_inlines.length() &gt; 0) {
2063 
2064     if (live_nodes() &gt; (uint)LiveNodeCountInliningCutoff) {
2065       if (low_live_nodes &lt; (uint)LiveNodeCountInliningCutoff * 8 / 10) {
2066         TracePhase tp("incrementalInline_ideal", &amp;timers[_t_incrInline_ideal]);
2067         // PhaseIdealLoop is expensive so we only try it once we are
2068         // out of live nodes and we only try it again if the previous
2069         // helped got the number of nodes down significantly
2070         PhaseIdealLoop ideal_loop( igvn, false, true );
2071         if (failing())  return;
2072         low_live_nodes = live_nodes();
2073         _major_progress = true;
2074       }
2075 
2076       if (live_nodes() &gt; (uint)LiveNodeCountInliningCutoff) {
2077         break;
2078       }
2079     }
2080 
2081     inline_incrementally_one(igvn);
2082 
2083     if (failing())  return;
2084 
2085     {
2086       TracePhase tp("incrementalInline_igvn", &amp;timers[_t_incrInline_igvn]);
2087       igvn.optimize();
2088     }
2089 
2090     if (failing())  return;
2091   }
2092 
2093   assert( igvn._worklist.size() == 0, "should be done with igvn" );
2094 
2095   if (_string_late_inlines.length() &gt; 0) {
2096     assert(has_stringbuilder(), "inconsistent");
2097     for_igvn()-&gt;clear();
2098     initial_gvn()-&gt;replace_with(&amp;igvn);
2099 
2100     inline_string_calls(false);
2101 
2102     if (failing())  return;
2103 
2104     {
2105       TracePhase tp("incrementalInline_pru", &amp;timers[_t_incrInline_pru]);
2106       ResourceMark rm;
2107       PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2108     }
2109 
2110     {
2111       TracePhase tp("incrementalInline_igvn", &amp;timers[_t_incrInline_igvn]);
2112       igvn = PhaseIterGVN(gvn);
2113       igvn.optimize();
2114     }
2115   }
2116 
2117   set_inlining_incrementally(false);
2118 }
2119 
2120 
2121 //------------------------------Optimize---------------------------------------
2122 // Given a graph, optimize it.
2123 void Compile::Optimize() {
2124   TracePhase tp("optimizer", &amp;timers[_t_optimizer]);
2125 
2126 #ifndef PRODUCT
2127   if (_directive-&gt;BreakAtCompileOption) {
2128     BREAKPOINT;
2129   }
2130 
2131 #endif
2132 
2133   ResourceMark rm;
2134   int          loop_opts_cnt;
2135 
2136   print_inlining_reinit();
2137 
2138   NOT_PRODUCT( verify_graph_edges(); )
2139 
2140   print_method(PHASE_AFTER_PARSING);
2141 
2142  {
2143   // Iterative Global Value Numbering, including ideal transforms
2144   // Initialize IterGVN with types and values from parse-time GVN
2145   PhaseIterGVN igvn(initial_gvn());
2146 #ifdef ASSERT
2147   _modified_nodes = new (comp_arena()) Unique_Node_List(comp_arena());
2148 #endif
2149   {
2150     TracePhase tp("iterGVN", &amp;timers[_t_iterGVN]);
2151     igvn.optimize();
2152   }
2153 
2154   print_method(PHASE_ITER_GVN1, 2);
2155 
2156   if (failing())  return;
2157 
2158   inline_incrementally(igvn);
2159 
2160   print_method(PHASE_INCREMENTAL_INLINE, 2);
2161 
2162   if (failing())  return;
2163 
2164   if (eliminate_boxing()) {
2165     // Inline valueOf() methods now.
2166     inline_boxing_calls(igvn);
2167 
2168     if (AlwaysIncrementalInline) {
2169       inline_incrementally(igvn);
2170     }
2171 
2172     print_method(PHASE_INCREMENTAL_BOXING_INLINE, 2);
2173 
2174     if (failing())  return;
2175   }
2176 
2177   // Remove the speculative part of types and clean up the graph from
2178   // the extra CastPP nodes whose only purpose is to carry them. Do
2179   // that early so that optimizations are not disrupted by the extra
2180   // CastPP nodes.
2181   remove_speculative_types(igvn);
2182 
2183   // No more new expensive nodes will be added to the list from here
2184   // so keep only the actual candidates for optimizations.
2185   cleanup_expensive_nodes(igvn);
2186 
2187   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2188     Compile::TracePhase tp("", &amp;timers[_t_renumberLive]);
2189     initial_gvn()-&gt;replace_with(&amp;igvn);
2190     for_igvn()-&gt;clear();
2191     Unique_Node_List new_worklist(C-&gt;comp_arena());
2192     {
2193       ResourceMark rm;
2194       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2195     }
2196     set_for_igvn(&amp;new_worklist);
2197     igvn = PhaseIterGVN(initial_gvn());
2198     igvn.optimize();
2199   }
2200 
2201   // Perform escape analysis
2202   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2203     if (has_loops()) {
2204       // Cleanup graph (remove dead nodes).
2205       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2206       PhaseIdealLoop ideal_loop( igvn, false, true );
2207       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2208       if (failing())  return;
2209     }
2210     ConnectionGraph::do_analysis(this, &amp;igvn);
2211 
2212     if (failing())  return;
2213 
2214     // Optimize out fields loads from scalar replaceable allocations.
2215     igvn.optimize();
2216     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2217 
2218     if (failing())  return;
2219 
2220     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
2221       TracePhase tp("macroEliminate", &amp;timers[_t_macroEliminate]);
2222       PhaseMacroExpand mexp(igvn);
2223       mexp.eliminate_macro_nodes();
2224       igvn.set_delay_transform(false);
2225 
2226       igvn.optimize();
2227       print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);
2228 
2229       if (failing())  return;
2230     }
2231   }
2232 
2233   // Loop transforms on the ideal graph.  Range Check Elimination,
2234   // peeling, unrolling, etc.
2235 
2236   // Set loop opts counter
2237   loop_opts_cnt = num_loop_opts();
2238   if((loop_opts_cnt &gt; 0) &amp;&amp; (has_loops() || has_split_ifs())) {
2239     {
2240       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2241       PhaseIdealLoop ideal_loop( igvn, true );
2242       loop_opts_cnt--;
2243       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP1, 2);
2244       if (failing())  return;
2245     }
2246     // Loop opts pass if partial peeling occurred in previous pass
2247     if(PartialPeelLoop &amp;&amp; major_progress() &amp;&amp; (loop_opts_cnt &gt; 0)) {
2248       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2249       PhaseIdealLoop ideal_loop( igvn, false );
2250       loop_opts_cnt--;
2251       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP2, 2);
2252       if (failing())  return;
2253     }
2254     // Loop opts pass for loop-unrolling before CCP
2255     if(major_progress() &amp;&amp; (loop_opts_cnt &gt; 0)) {
2256       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2257       PhaseIdealLoop ideal_loop( igvn, false );
2258       loop_opts_cnt--;
2259       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP3, 2);
2260     }
2261     if (!failing()) {
2262       // Verify that last round of loop opts produced a valid graph
2263       TracePhase tp("idealLoopVerify", &amp;timers[_t_idealLoopVerify]);
2264       PhaseIdealLoop::verify(igvn);
2265     }
2266   }
2267   if (failing())  return;
2268 
2269   // Conditional Constant Propagation;
2270   PhaseCCP ccp( &amp;igvn );
2271   assert( true, "Break here to ccp.dump_nodes_and_types(_root,999,1)");
2272   {
2273     TracePhase tp("ccp", &amp;timers[_t_ccp]);
2274     ccp.do_transform();
2275   }
2276   print_method(PHASE_CPP1, 2);
2277 
2278   assert( true, "Break here to ccp.dump_old2new_map()");
2279 
2280   // Iterative Global Value Numbering, including ideal transforms
2281   {
2282     TracePhase tp("iterGVN2", &amp;timers[_t_iterGVN2]);
2283     igvn = ccp;
2284     igvn.optimize();
2285   }
2286 
2287   print_method(PHASE_ITER_GVN2, 2);
2288 
2289   if (failing())  return;
2290 
2291   // Loop transforms on the ideal graph.  Range Check Elimination,
2292   // peeling, unrolling, etc.
2293   if(loop_opts_cnt &gt; 0) {
2294     debug_only( int cnt = 0; );
2295     while(major_progress() &amp;&amp; (loop_opts_cnt &gt; 0)) {
2296       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2297       assert( cnt++ &lt; 40, "infinite cycle in loop optimization" );
2298       PhaseIdealLoop ideal_loop( igvn, true);
2299       loop_opts_cnt--;
2300       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP_ITERATIONS, 2);
2301       if (failing())  return;
2302     }
2303   }
2304   // Ensure that major progress is now clear
2305   C-&gt;clear_major_progress();
2306 
2307   {
2308     // Verify that all previous optimizations produced a valid graph
2309     // at least to this point, even if no loop optimizations were done.
2310     TracePhase tp("idealLoopVerify", &amp;timers[_t_idealLoopVerify]);
2311     PhaseIdealLoop::verify(igvn);
2312   }
2313 
2314   if (range_check_cast_count() &gt; 0) {
2315     // No more loop optimizations. Remove all range check dependent CastIINodes.
2316     C-&gt;remove_range_check_casts(igvn);
2317     igvn.optimize();
2318   }
2319 
2320   {
2321     TracePhase tp("macroExpand", &amp;timers[_t_macroExpand]);
2322     PhaseMacroExpand  mex(igvn);
2323     if (mex.expand_macro_nodes()) {
2324       assert(failing(), "must bail out w/ explicit message");
2325       return;
2326     }
2327   }
2328 
2329   DEBUG_ONLY( _modified_nodes = NULL; )
2330  } // (End scope of igvn; run destructor if necessary for asserts.)
2331 
2332  process_print_inlining();
2333  // A method with only infinite loops has no edges entering loops from root
2334  {
2335    TracePhase tp("graphReshape", &amp;timers[_t_graphReshaping]);
2336    if (final_graph_reshaping()) {
2337      assert(failing(), "must bail out w/ explicit message");
2338      return;
2339    }
2340  }
2341 
2342  print_method(PHASE_OPTIMIZE_FINISHED, 2);
2343 }
2344 
2345 
2346 //------------------------------Code_Gen---------------------------------------
2347 // Given a graph, generate code for it
2348 void Compile::Code_Gen() {
2349   if (failing()) {
2350     return;
2351   }
2352 
2353   // Perform instruction selection.  You might think we could reclaim Matcher
2354   // memory PDQ, but actually the Matcher is used in generating spill code.
2355   // Internals of the Matcher (including some VectorSets) must remain live
2356   // for awhile - thus I cannot reclaim Matcher memory lest a VectorSet usage
2357   // set a bit in reclaimed memory.
2358 
2359   // In debug mode can dump m._nodes.dump() for mapping of ideal to machine
2360   // nodes.  Mapping is only valid at the root of each matched subtree.
2361   NOT_PRODUCT( verify_graph_edges(); )
2362 
2363   Matcher matcher;
2364   _matcher = &amp;matcher;
2365   {
2366     TracePhase tp("matcher", &amp;timers[_t_matcher]);
2367     matcher.match();
2368   }
2369   // In debug mode can dump m._nodes.dump() for mapping of ideal to machine
2370   // nodes.  Mapping is only valid at the root of each matched subtree.
2371   NOT_PRODUCT( verify_graph_edges(); )
2372 
2373   // If you have too many nodes, or if matching has failed, bail out
2374   check_node_count(0, "out of nodes matching instructions");
2375   if (failing()) {
2376     return;
2377   }
2378 
2379   // Build a proper-looking CFG
2380   PhaseCFG cfg(node_arena(), root(), matcher);
2381   _cfg = &amp;cfg;
2382   {
2383     TracePhase tp("scheduler", &amp;timers[_t_scheduler]);
2384     bool success = cfg.do_global_code_motion();
2385     if (!success) {
2386       return;
2387     }
2388 
2389     print_method(PHASE_GLOBAL_CODE_MOTION, 2);
2390     NOT_PRODUCT( verify_graph_edges(); )
2391     debug_only( cfg.verify(); )
2392   }
2393 
2394   PhaseChaitin regalloc(unique(), cfg, matcher, false);
2395   _regalloc = &amp;regalloc;
2396   {
2397     TracePhase tp("regalloc", &amp;timers[_t_registerAllocation]);
2398     // Perform register allocation.  After Chaitin, use-def chains are
2399     // no longer accurate (at spill code) and so must be ignored.
2400     // Node-&gt;LRG-&gt;reg mappings are still accurate.
2401     _regalloc-&gt;Register_Allocate();
2402 
2403     // Bail out if the allocator builds too many nodes
2404     if (failing()) {
2405       return;
2406     }
2407   }
2408 
2409   // Prior to register allocation we kept empty basic blocks in case the
2410   // the allocator needed a place to spill.  After register allocation we
2411   // are not adding any new instructions.  If any basic block is empty, we
2412   // can now safely remove it.
2413   {
2414     TracePhase tp("blockOrdering", &amp;timers[_t_blockOrdering]);
2415     cfg.remove_empty_blocks();
2416     if (do_freq_based_layout()) {
2417       PhaseBlockLayout layout(cfg);
2418     } else {
2419       cfg.set_loop_alignment();
2420     }
2421     cfg.fixup_flow();
2422   }
2423 
2424   // Apply peephole optimizations
2425   if( OptoPeephole ) {
2426     TracePhase tp("peephole", &amp;timers[_t_peephole]);
2427     PhasePeephole peep( _regalloc, cfg);
2428     peep.do_transform();
2429   }
2430 
2431   // Do late expand if CPU requires this.
2432   if (Matcher::require_postalloc_expand) {
2433     TracePhase tp("postalloc_expand", &amp;timers[_t_postalloc_expand]);
2434     cfg.postalloc_expand(_regalloc);
2435   }
2436 
2437   // Convert Nodes to instruction bits in a buffer
2438   {
2439     TraceTime tp("output", &amp;timers[_t_output], CITime);
2440     Output();
2441   }
2442 
2443   print_method(PHASE_FINAL_CODE);
2444 
2445   // He's dead, Jim.
2446   _cfg     = (PhaseCFG*)0xdeadbeef;
2447   _regalloc = (PhaseChaitin*)0xdeadbeef;
2448 }
2449 
2450 
2451 //------------------------------dump_asm---------------------------------------
2452 // Dump formatted assembly
2453 #ifndef PRODUCT
2454 void Compile::dump_asm(int *pcs, uint pc_limit) {
2455   bool cut_short = false;
2456   tty-&gt;print_cr("#");
2457   tty-&gt;print("#  ");  _tf-&gt;dump();  tty-&gt;cr();
2458   tty-&gt;print_cr("#");
2459 
2460   // For all blocks
2461   int pc = 0x0;                 // Program counter
2462   char starts_bundle = ' ';
2463   _regalloc-&gt;dump_frame();
2464 
2465   Node *n = NULL;
2466   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
2467     if (VMThread::should_terminate()) {
2468       cut_short = true;
2469       break;
2470     }
2471     Block* block = _cfg-&gt;get_block(i);
2472     if (block-&gt;is_connector() &amp;&amp; !Verbose) {
2473       continue;
2474     }
2475     n = block-&gt;head();
2476     if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit) {
2477       tty-&gt;print("%3.3x   ", pcs[n-&gt;_idx]);
2478     } else {
2479       tty-&gt;print("      ");
2480     }
2481     block-&gt;dump_head(_cfg);
2482     if (block-&gt;is_connector()) {
2483       tty-&gt;print_cr("        # Empty connector block");
2484     } else if (block-&gt;num_preds() == 2 &amp;&amp; block-&gt;pred(1)-&gt;is_CatchProj() &amp;&amp; block-&gt;pred(1)-&gt;as_CatchProj()-&gt;_con == CatchProjNode::fall_through_index) {
2485       tty-&gt;print_cr("        # Block is sole successor of call");
2486     }
2487 
2488     // For all instructions
2489     Node *delay = NULL;
2490     for (uint j = 0; j &lt; block-&gt;number_of_nodes(); j++) {
2491       if (VMThread::should_terminate()) {
2492         cut_short = true;
2493         break;
2494       }
2495       n = block-&gt;get_node(j);
2496       if (valid_bundle_info(n)) {
2497         Bundle* bundle = node_bundling(n);
2498         if (bundle-&gt;used_in_unconditional_delay()) {
2499           delay = n;
2500           continue;
2501         }
2502         if (bundle-&gt;starts_bundle()) {
2503           starts_bundle = '+';
2504         }
2505       }
2506 
2507       if (WizardMode) {
2508         n-&gt;dump();
2509       }
2510 
2511       if( !n-&gt;is_Region() &amp;&amp;    // Dont print in the Assembly
2512           !n-&gt;is_Phi() &amp;&amp;       // a few noisely useless nodes
2513           !n-&gt;is_Proj() &amp;&amp;
2514           !n-&gt;is_MachTemp() &amp;&amp;
2515           !n-&gt;is_SafePointScalarObject() &amp;&amp;
2516           !n-&gt;is_Catch() &amp;&amp;     // Would be nice to print exception table targets
2517           !n-&gt;is_MergeMem() &amp;&amp;  // Not very interesting
2518           !n-&gt;is_top() &amp;&amp;       // Debug info table constants
2519           !(n-&gt;is_Con() &amp;&amp; !n-&gt;is_Mach())// Debug info table constants
2520           ) {
2521         if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit)
2522           tty-&gt;print("%3.3x", pcs[n-&gt;_idx]);
2523         else
2524           tty-&gt;print("   ");
2525         tty-&gt;print(" %c ", starts_bundle);
2526         starts_bundle = ' ';
2527         tty-&gt;print("\t");
2528         n-&gt;format(_regalloc, tty);
2529         tty-&gt;cr();
2530       }
2531 
2532       // If we have an instruction with a delay slot, and have seen a delay,
2533       // then back up and print it
2534       if (valid_bundle_info(n) &amp;&amp; node_bundling(n)-&gt;use_unconditional_delay()) {
2535         assert(delay != NULL, "no unconditional delay instruction");
2536         if (WizardMode) delay-&gt;dump();
2537 
2538         if (node_bundling(delay)-&gt;starts_bundle())
2539           starts_bundle = '+';
2540         if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit)
2541           tty-&gt;print("%3.3x", pcs[n-&gt;_idx]);
2542         else
2543           tty-&gt;print("   ");
2544         tty-&gt;print(" %c ", starts_bundle);
2545         starts_bundle = ' ';
2546         tty-&gt;print("\t");
2547         delay-&gt;format(_regalloc, tty);
2548         tty-&gt;cr();
2549         delay = NULL;
2550       }
2551 
2552       // Dump the exception table as well
2553       if( n-&gt;is_Catch() &amp;&amp; (Verbose || WizardMode) ) {
2554         // Print the exception table for this offset
2555         _handler_table.print_subtable_for(pc);
2556       }
2557     }
2558 
2559     if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit)
2560       tty-&gt;print_cr("%3.3x", pcs[n-&gt;_idx]);
2561     else
2562       tty-&gt;cr();
2563 
2564     assert(cut_short || delay == NULL, "no unconditional delay branch");
2565 
2566   } // End of per-block dump
2567   tty-&gt;cr();
2568 
2569   if (cut_short)  tty-&gt;print_cr("*** disassembly is cut short ***");
2570 }
2571 #endif
2572 
2573 //------------------------------Final_Reshape_Counts---------------------------
2574 // This class defines counters to help identify when a method
2575 // may/must be executed using hardware with only 24-bit precision.
2576 struct Final_Reshape_Counts : public StackObj {
2577   int  _call_count;             // count non-inlined 'common' calls
2578   int  _float_count;            // count float ops requiring 24-bit precision
2579   int  _double_count;           // count double ops requiring more precision
2580   int  _java_call_count;        // count non-inlined 'java' calls
2581   int  _inner_loop_count;       // count loops which need alignment
2582   VectorSet _visited;           // Visitation flags
2583   Node_List _tests;             // Set of IfNodes &amp; PCTableNodes
2584 
2585   Final_Reshape_Counts() :
2586     _call_count(0), _float_count(0), _double_count(0),
2587     _java_call_count(0), _inner_loop_count(0),
2588     _visited( Thread::current()-&gt;resource_area() ) { }
2589 
2590   void inc_call_count  () { _call_count  ++; }
2591   void inc_float_count () { _float_count ++; }
2592   void inc_double_count() { _double_count++; }
2593   void inc_java_call_count() { _java_call_count++; }
2594   void inc_inner_loop_count() { _inner_loop_count++; }
2595 
2596   int  get_call_count  () const { return _call_count  ; }
2597   int  get_float_count () const { return _float_count ; }
2598   int  get_double_count() const { return _double_count; }
2599   int  get_java_call_count() const { return _java_call_count; }
2600   int  get_inner_loop_count() const { return _inner_loop_count; }
2601 };
2602 
2603 #ifdef ASSERT
2604 static bool oop_offset_is_sane(const TypeInstPtr* tp) {
2605   ciInstanceKlass *k = tp-&gt;klass()-&gt;as_instance_klass();
2606   // Make sure the offset goes inside the instance layout.
2607   return k-&gt;contains_field_offset(tp-&gt;offset());
2608   // Note that OffsetBot and OffsetTop are very negative.
2609 }
2610 #endif
2611 
2612 // Eliminate trivially redundant StoreCMs and accumulate their
2613 // precedence edges.
2614 void Compile::eliminate_redundant_card_marks(Node* n) {
2615   assert(n-&gt;Opcode() == Op_StoreCM, "expected StoreCM");
2616   if (n-&gt;in(MemNode::Address)-&gt;outcnt() &gt; 1) {
2617     // There are multiple users of the same address so it might be
2618     // possible to eliminate some of the StoreCMs
2619     Node* mem = n-&gt;in(MemNode::Memory);
2620     Node* adr = n-&gt;in(MemNode::Address);
2621     Node* val = n-&gt;in(MemNode::ValueIn);
2622     Node* prev = n;
2623     bool done = false;
2624     // Walk the chain of StoreCMs eliminating ones that match.  As
2625     // long as it's a chain of single users then the optimization is
2626     // safe.  Eliminating partially redundant StoreCMs would require
2627     // cloning copies down the other paths.
2628     while (mem-&gt;Opcode() == Op_StoreCM &amp;&amp; mem-&gt;outcnt() == 1 &amp;&amp; !done) {
2629       if (adr == mem-&gt;in(MemNode::Address) &amp;&amp;
2630           val == mem-&gt;in(MemNode::ValueIn)) {
2631         // redundant StoreCM
2632         if (mem-&gt;req() &gt; MemNode::OopStore) {
2633           // Hasn't been processed by this code yet.
2634           n-&gt;add_prec(mem-&gt;in(MemNode::OopStore));
2635         } else {
2636           // Already converted to precedence edge
2637           for (uint i = mem-&gt;req(); i &lt; mem-&gt;len(); i++) {
2638             // Accumulate any precedence edges
2639             if (mem-&gt;in(i) != NULL) {
2640               n-&gt;add_prec(mem-&gt;in(i));
2641             }
2642           }
2643           // Everything above this point has been processed.
2644           done = true;
2645         }
2646         // Eliminate the previous StoreCM
2647         prev-&gt;set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2648         assert(mem-&gt;outcnt() == 0, "should be dead");
2649         mem-&gt;disconnect_inputs(NULL, this);
2650       } else {
2651         prev = mem;
2652       }
2653       mem = prev-&gt;in(MemNode::Memory);
2654     }
2655   }
2656 }
2657 
2658 //------------------------------final_graph_reshaping_impl----------------------
2659 // Implement items 1-5 from final_graph_reshaping below.
2660 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &amp;frc) {
2661 
2662   if ( n-&gt;outcnt() == 0 ) return; // dead node
2663   uint nop = n-&gt;Opcode();
2664 
2665   // Check for 2-input instruction with "last use" on right input.
2666   // Swap to left input.  Implements item (2).
2667   if( n-&gt;req() == 3 &amp;&amp;          // two-input instruction
2668       n-&gt;in(1)-&gt;outcnt() &gt; 1 &amp;&amp; // left use is NOT a last use
2669       (!n-&gt;in(1)-&gt;is_Phi() || n-&gt;in(1)-&gt;in(2) != n) &amp;&amp; // it is not data loop
2670       n-&gt;in(2)-&gt;outcnt() == 1 &amp;&amp;// right use IS a last use
2671       !n-&gt;in(2)-&gt;is_Con() ) {   // right use is not a constant
2672     // Check for commutative opcode
2673     switch( nop ) {
2674     case Op_AddI:  case Op_AddF:  case Op_AddD:  case Op_AddL:
2675     case Op_MaxI:  case Op_MinI:
2676     case Op_MulI:  case Op_MulF:  case Op_MulD:  case Op_MulL:
2677     case Op_AndL:  case Op_XorL:  case Op_OrL:
2678     case Op_AndI:  case Op_XorI:  case Op_OrI: {
2679       // Move "last use" input to left by swapping inputs
2680       n-&gt;swap_edges(1, 2);
2681       break;
2682     }
2683     default:
2684       break;
2685     }
2686   }
2687 
2688 #ifdef ASSERT
2689   if( n-&gt;is_Mem() ) {
2690     int alias_idx = get_alias_index(n-&gt;as_Mem()-&gt;adr_type());
2691     assert( n-&gt;in(0) != NULL || alias_idx != Compile::AliasIdxRaw ||
2692             // oop will be recorded in oop map if load crosses safepoint
2693             n-&gt;is_Load() &amp;&amp; (n-&gt;as_Load()-&gt;bottom_type()-&gt;isa_oopptr() ||
2694                              LoadNode::is_immutable_value(n-&gt;in(MemNode::Address))),
2695             "raw memory operations should have control edge");
2696   }
2697 #endif
2698   // Count FPU ops and common calls, implements item (3)
2699   switch( nop ) {
2700   // Count all float operations that may use FPU
2701   case Op_AddF:
2702   case Op_SubF:
2703   case Op_MulF:
2704   case Op_DivF:
2705   case Op_NegF:
2706   case Op_ModF:
2707   case Op_ConvI2F:
2708   case Op_ConF:
2709   case Op_CmpF:
2710   case Op_CmpF3:
2711   // case Op_ConvL2F: // longs are split into 32-bit halves
2712     frc.inc_float_count();
2713     break;
2714 
2715   case Op_ConvF2D:
2716   case Op_ConvD2F:
2717     frc.inc_float_count();
2718     frc.inc_double_count();
2719     break;
2720 
2721   // Count all double operations that may use FPU
2722   case Op_AddD:
2723   case Op_SubD:
2724   case Op_MulD:
2725   case Op_DivD:
2726   case Op_NegD:
2727   case Op_ModD:
2728   case Op_ConvI2D:
2729   case Op_ConvD2I:
2730   // case Op_ConvL2D: // handled by leaf call
2731   // case Op_ConvD2L: // handled by leaf call
2732   case Op_ConD:
2733   case Op_CmpD:
2734   case Op_CmpD3:
2735     frc.inc_double_count();
2736     break;
2737   case Op_Opaque1:              // Remove Opaque Nodes before matching
2738   case Op_Opaque2:              // Remove Opaque Nodes before matching
2739   case Op_Opaque3:
2740     n-&gt;subsume_by(n-&gt;in(1), this);
2741     break;
2742   case Op_CallStaticJava:
2743   case Op_CallJava:
2744   case Op_CallDynamicJava:
2745     frc.inc_java_call_count(); // Count java call site;
2746   case Op_CallRuntime:
2747   case Op_CallLeaf:
2748   case Op_CallLeafNoFP: {
2749     assert( n-&gt;is_Call(), "" );
2750     CallNode *call = n-&gt;as_Call();
2751     // Count call sites where the FP mode bit would have to be flipped.
2752     // Do not count uncommon runtime calls:
2753     // uncommon_trap, _complete_monitor_locking, _complete_monitor_unlocking,
2754     // _new_Java, _new_typeArray, _new_objArray, _rethrow_Java, ...
2755     if( !call-&gt;is_CallStaticJava() || !call-&gt;as_CallStaticJava()-&gt;_name ) {
2756       frc.inc_call_count();   // Count the call site
2757     } else {                  // See if uncommon argument is shared
2758       Node *n = call-&gt;in(TypeFunc::Parms);
2759       int nop = n-&gt;Opcode();
2760       // Clone shared simple arguments to uncommon calls, item (1).
2761       if( n-&gt;outcnt() &gt; 1 &amp;&amp;
2762           !n-&gt;is_Proj() &amp;&amp;
2763           nop != Op_CreateEx &amp;&amp;
2764           nop != Op_CheckCastPP &amp;&amp;
2765           nop != Op_DecodeN &amp;&amp;
2766           nop != Op_DecodeNKlass &amp;&amp;
2767           !n-&gt;is_Mem() ) {
2768         Node *x = n-&gt;clone();
2769         call-&gt;set_req( TypeFunc::Parms, x );
2770       }
2771     }
2772     break;
2773   }
2774 
2775   case Op_StoreD:
2776   case Op_LoadD:
2777   case Op_LoadD_unaligned:
2778     frc.inc_double_count();
2779     goto handle_mem;
2780   case Op_StoreF:
2781   case Op_LoadF:
2782     frc.inc_float_count();
2783     goto handle_mem;
2784 
2785   case Op_StoreCM:
2786     {
2787       // Convert OopStore dependence into precedence edge
2788       Node* prec = n-&gt;in(MemNode::OopStore);
2789       n-&gt;del_req(MemNode::OopStore);
2790       n-&gt;add_prec(prec);
2791       eliminate_redundant_card_marks(n);
2792     }
2793 
2794     // fall through
2795 
2796   case Op_StoreB:
2797   case Op_StoreC:
2798   case Op_StorePConditional:
2799   case Op_StoreI:
2800   case Op_StoreL:
2801   case Op_StoreIConditional:
2802   case Op_StoreLConditional:
2803   case Op_CompareAndSwapI:
2804   case Op_CompareAndSwapL:
2805   case Op_CompareAndSwapP:
2806   case Op_CompareAndSwapN:
2807   case Op_WeakCompareAndSwapI:
2808   case Op_WeakCompareAndSwapL:
2809   case Op_WeakCompareAndSwapP:
2810   case Op_WeakCompareAndSwapN:
2811   case Op_CompareAndExchangeI:
2812   case Op_CompareAndExchangeL:
2813   case Op_CompareAndExchangeP:
2814   case Op_CompareAndExchangeN:
2815   case Op_GetAndAddI:
2816   case Op_GetAndAddL:
2817   case Op_GetAndSetI:
2818   case Op_GetAndSetL:
2819   case Op_GetAndSetP:
2820   case Op_GetAndSetN:
2821   case Op_StoreP:
2822   case Op_StoreN:
2823   case Op_StoreNKlass:
2824   case Op_LoadB:
2825   case Op_LoadUB:
2826   case Op_LoadUS:
2827   case Op_LoadI:
2828   case Op_LoadKlass:
2829   case Op_LoadNKlass:
2830   case Op_LoadL:
2831   case Op_LoadL_unaligned:
2832   case Op_LoadPLocked:
2833   case Op_LoadP:
2834   case Op_LoadN:
2835   case Op_LoadRange:
2836   case Op_LoadS: {
2837   handle_mem:
2838 #ifdef ASSERT
2839     if( VerifyOptoOopOffsets ) {
2840       assert( n-&gt;is_Mem(), "" );
2841       MemNode *mem  = (MemNode*)n;
2842       // Check to see if address types have grounded out somehow.
2843       const TypeInstPtr *tp = mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_instptr();
2844       assert( !tp || oop_offset_is_sane(tp), "" );
2845     }
2846 #endif
2847     break;
2848   }
2849 
2850   case Op_AddP: {               // Assert sane base pointers
2851     Node *addp = n-&gt;in(AddPNode::Address);
2852     assert( !addp-&gt;is_AddP() ||
2853             addp-&gt;in(AddPNode::Base)-&gt;is_top() || // Top OK for allocation
2854             addp-&gt;in(AddPNode::Base) == n-&gt;in(AddPNode::Base),
2855             "Base pointers must match" );
2856 #ifdef _LP64
2857     if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp;
2858         addp-&gt;Opcode() == Op_ConP &amp;&amp;
2859         addp == n-&gt;in(AddPNode::Base) &amp;&amp;
2860         n-&gt;in(AddPNode::Offset)-&gt;is_Con()) {
2861       // Use addressing with narrow klass to load with offset on x86.
2862       // On sparc loading 32-bits constant and decoding it have less
2863       // instructions (4) then load 64-bits constant (7).
2864       // Do this transformation here since IGVN will convert ConN back to ConP.
2865       const Type* t = addp-&gt;bottom_type();
2866       if (t-&gt;isa_oopptr() || t-&gt;isa_klassptr()) {
2867         Node* nn = NULL;
2868 
2869         int op = t-&gt;isa_oopptr() ? Op_ConN : Op_ConNKlass;
2870 
2871         // Look for existing ConN node of the same exact type.
2872         Node* r  = root();
2873         uint cnt = r-&gt;outcnt();
2874         for (uint i = 0; i &lt; cnt; i++) {
2875           Node* m = r-&gt;raw_out(i);
2876           if (m!= NULL &amp;&amp; m-&gt;Opcode() == op &amp;&amp;
2877               m-&gt;bottom_type()-&gt;make_ptr() == t) {
2878             nn = m;
2879             break;
2880           }
2881         }
2882         if (nn != NULL) {
2883           // Decode a narrow oop to match address
2884           // [R12 + narrow_oop_reg&lt;&lt;3 + offset]
2885           if (t-&gt;isa_oopptr()) {
2886             nn = new DecodeNNode(nn, t);
2887           } else {
2888             nn = new DecodeNKlassNode(nn, t);
2889           }
2890           n-&gt;set_req(AddPNode::Base, nn);
2891           n-&gt;set_req(AddPNode::Address, nn);
2892           if (addp-&gt;outcnt() == 0) {
2893             addp-&gt;disconnect_inputs(NULL, this);
2894           }
2895         }
2896       }
2897     }
2898 #endif
2899     break;
2900   }
2901 
2902   case Op_CastPP: {
2903     // Remove CastPP nodes to gain more freedom during scheduling but
2904     // keep the dependency they encode as control or precedence edges
2905     // (if control is set already) on memory operations. Some CastPP
2906     // nodes don't have a control (don't carry a dependency): skip
2907     // those.
2908     if (n-&gt;in(0) != NULL) {
2909       ResourceMark rm;
2910       Unique_Node_List wq;
2911       wq.push(n);
2912       for (uint next = 0; next &lt; wq.size(); ++next) {
2913         Node *m = wq.at(next);
2914         for (DUIterator_Fast imax, i = m-&gt;fast_outs(imax); i &lt; imax; i++) {
2915           Node* use = m-&gt;fast_out(i);
2916           if (use-&gt;is_Mem() || use-&gt;is_EncodeNarrowPtr()) {
2917             use-&gt;ensure_control_or_add_prec(n-&gt;in(0));
2918           } else {
2919             switch(use-&gt;Opcode()) {
2920             case Op_AddP:
2921             case Op_DecodeN:
2922             case Op_DecodeNKlass:
2923             case Op_CheckCastPP:
2924             case Op_CastPP:
2925               wq.push(use);
2926               break;
2927             }
2928           }
2929         }
2930       }
2931     }
2932     const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);
2933     if (is_LP64 &amp;&amp; n-&gt;in(1)-&gt;is_DecodeN() &amp;&amp; Matcher::gen_narrow_oop_implicit_null_checks()) {
2934       Node* in1 = n-&gt;in(1);
2935       const Type* t = n-&gt;bottom_type();
2936       Node* new_in1 = in1-&gt;clone();
2937       new_in1-&gt;as_DecodeN()-&gt;set_type(t);
2938 
2939       if (!Matcher::narrow_oop_use_complex_address()) {
2940         //
2941         // x86, ARM and friends can handle 2 adds in addressing mode
2942         // and Matcher can fold a DecodeN node into address by using
2943         // a narrow oop directly and do implicit NULL check in address:
2944         //
2945         // [R12 + narrow_oop_reg&lt;&lt;3 + offset]
2946         // NullCheck narrow_oop_reg
2947         //
2948         // On other platforms (Sparc) we have to keep new DecodeN node and
2949         // use it to do implicit NULL check in address:
2950         //
2951         // decode_not_null narrow_oop_reg, base_reg
2952         // [base_reg + offset]
2953         // NullCheck base_reg
2954         //
2955         // Pin the new DecodeN node to non-null path on these platform (Sparc)
2956         // to keep the information to which NULL check the new DecodeN node
2957         // corresponds to use it as value in implicit_null_check().
2958         //
2959         new_in1-&gt;set_req(0, n-&gt;in(0));
2960       }
2961 
2962       n-&gt;subsume_by(new_in1, this);
2963       if (in1-&gt;outcnt() == 0) {
2964         in1-&gt;disconnect_inputs(NULL, this);
2965       }
2966     } else {
2967       n-&gt;subsume_by(n-&gt;in(1), this);
2968       if (n-&gt;outcnt() == 0) {
2969         n-&gt;disconnect_inputs(NULL, this);
2970       }
2971     }
2972     break;
2973   }
2974 #ifdef _LP64
2975   case Op_CmpP:
2976     // Do this transformation here to preserve CmpPNode::sub() and
2977     // other TypePtr related Ideal optimizations (for example, ptr nullness).
2978     if (n-&gt;in(1)-&gt;is_DecodeNarrowPtr() || n-&gt;in(2)-&gt;is_DecodeNarrowPtr()) {
2979       Node* in1 = n-&gt;in(1);
2980       Node* in2 = n-&gt;in(2);
2981       if (!in1-&gt;is_DecodeNarrowPtr()) {
2982         in2 = in1;
2983         in1 = n-&gt;in(2);
2984       }
2985       assert(in1-&gt;is_DecodeNarrowPtr(), "sanity");
2986 
2987       Node* new_in2 = NULL;
2988       if (in2-&gt;is_DecodeNarrowPtr()) {
2989         assert(in2-&gt;Opcode() == in1-&gt;Opcode(), "must be same node type");
2990         new_in2 = in2-&gt;in(1);
2991       } else if (in2-&gt;Opcode() == Op_ConP) {
2992         const Type* t = in2-&gt;bottom_type();
2993         if (t == TypePtr::NULL_PTR) {
2994           assert(in1-&gt;is_DecodeN(), "compare klass to null?");
2995           // Don't convert CmpP null check into CmpN if compressed
2996           // oops implicit null check is not generated.
2997           // This will allow to generate normal oop implicit null check.
2998           if (Matcher::gen_narrow_oop_implicit_null_checks())
2999             new_in2 = ConNode::make(TypeNarrowOop::NULL_PTR);
3000           //
3001           // This transformation together with CastPP transformation above
3002           // will generated code for implicit NULL checks for compressed oops.
3003           //
3004           // The original code after Optimize()
3005           //
3006           //    LoadN memory, narrow_oop_reg
3007           //    decode narrow_oop_reg, base_reg
3008           //    CmpP base_reg, NULL
3009           //    CastPP base_reg // NotNull
3010           //    Load [base_reg + offset], val_reg
3011           //
3012           // after these transformations will be
3013           //
3014           //    LoadN memory, narrow_oop_reg
3015           //    CmpN narrow_oop_reg, NULL
3016           //    decode_not_null narrow_oop_reg, base_reg
3017           //    Load [base_reg + offset], val_reg
3018           //
3019           // and the uncommon path (== NULL) will use narrow_oop_reg directly
3020           // since narrow oops can be used in debug info now (see the code in
3021           // final_graph_reshaping_walk()).
3022           //
3023           // At the end the code will be matched to
3024           // on x86:
3025           //
3026           //    Load_narrow_oop memory, narrow_oop_reg
3027           //    Load [R12 + narrow_oop_reg&lt;&lt;3 + offset], val_reg
3028           //    NullCheck narrow_oop_reg
3029           //
3030           // and on sparc:
3031           //
3032           //    Load_narrow_oop memory, narrow_oop_reg
3033           //    decode_not_null narrow_oop_reg, base_reg
3034           //    Load [base_reg + offset], val_reg
3035           //    NullCheck base_reg
3036           //
3037         } else if (t-&gt;isa_oopptr()) {
3038           new_in2 = ConNode::make(t-&gt;make_narrowoop());
3039         } else if (t-&gt;isa_klassptr()) {
3040           new_in2 = ConNode::make(t-&gt;make_narrowklass());
3041         }
3042       }
3043       if (new_in2 != NULL) {
3044         Node* cmpN = new CmpNNode(in1-&gt;in(1), new_in2);
3045         n-&gt;subsume_by(cmpN, this);
3046         if (in1-&gt;outcnt() == 0) {
3047           in1-&gt;disconnect_inputs(NULL, this);
3048         }
3049         if (in2-&gt;outcnt() == 0) {
3050           in2-&gt;disconnect_inputs(NULL, this);
3051         }
3052       }
3053     }
3054     break;
3055 
3056   case Op_DecodeN:
3057   case Op_DecodeNKlass:
3058     assert(!n-&gt;in(1)-&gt;is_EncodeNarrowPtr(), "should be optimized out");
3059     // DecodeN could be pinned when it can't be fold into
3060     // an address expression, see the code for Op_CastPP above.
3061     assert(n-&gt;in(0) == NULL || (UseCompressedOops &amp;&amp; !Matcher::narrow_oop_use_complex_address()), "no control");
3062     break;
3063 
3064   case Op_EncodeP:
3065   case Op_EncodePKlass: {
3066     Node* in1 = n-&gt;in(1);
3067     if (in1-&gt;is_DecodeNarrowPtr()) {
3068       n-&gt;subsume_by(in1-&gt;in(1), this);
3069     } else if (in1-&gt;Opcode() == Op_ConP) {
3070       const Type* t = in1-&gt;bottom_type();
3071       if (t == TypePtr::NULL_PTR) {
3072         assert(t-&gt;isa_oopptr(), "null klass?");
3073         n-&gt;subsume_by(ConNode::make(TypeNarrowOop::NULL_PTR), this);
3074       } else if (t-&gt;isa_oopptr()) {
3075         n-&gt;subsume_by(ConNode::make(t-&gt;make_narrowoop()), this);
3076       } else if (t-&gt;isa_klassptr()) {
3077         n-&gt;subsume_by(ConNode::make(t-&gt;make_narrowklass()), this);
3078       }
3079     }
3080     if (in1-&gt;outcnt() == 0) {
3081       in1-&gt;disconnect_inputs(NULL, this);
3082     }
3083     break;
3084   }
3085 
3086   case Op_Proj: {
3087     if (OptimizeStringConcat) {
3088       ProjNode* p = n-&gt;as_Proj();
3089       if (p-&gt;_is_io_use) {
3090         // Separate projections were used for the exception path which
3091         // are normally removed by a late inline.  If it wasn't inlined
3092         // then they will hang around and should just be replaced with
3093         // the original one.
3094         Node* proj = NULL;
3095         // Replace with just one
3096         for (SimpleDUIterator i(p-&gt;in(0)); i.has_next(); i.next()) {
3097           Node *use = i.get();
3098           if (use-&gt;is_Proj() &amp;&amp; p != use &amp;&amp; use-&gt;as_Proj()-&gt;_con == p-&gt;_con) {
3099             proj = use;
3100             break;
3101           }
3102         }
3103         assert(proj != NULL, "must be found");
3104         p-&gt;subsume_by(proj, this);
3105       }
3106     }
3107     break;
3108   }
3109 
3110   case Op_Phi:
3111     if (n-&gt;as_Phi()-&gt;bottom_type()-&gt;isa_narrowoop() || n-&gt;as_Phi()-&gt;bottom_type()-&gt;isa_narrowklass()) {
3112       // The EncodeP optimization may create Phi with the same edges
3113       // for all paths. It is not handled well by Register Allocator.
3114       Node* unique_in = n-&gt;in(1);
3115       assert(unique_in != NULL, "");
3116       uint cnt = n-&gt;req();
3117       for (uint i = 2; i &lt; cnt; i++) {
3118         Node* m = n-&gt;in(i);
3119         assert(m != NULL, "");
3120         if (unique_in != m)
3121           unique_in = NULL;
3122       }
3123       if (unique_in != NULL) {
3124         n-&gt;subsume_by(unique_in, this);
3125       }
3126     }
3127     break;
3128 
3129 #endif
3130 
3131 #ifdef ASSERT
3132   case Op_CastII:
3133     // Verify that all range check dependent CastII nodes were removed.
3134     if (n-&gt;isa_CastII()-&gt;has_range_check()) {
3135       n-&gt;dump(3);
3136       assert(false, "Range check dependent CastII node was not removed");
3137     }
3138     break;
3139 #endif
3140 
3141   case Op_ModI:
3142     if (UseDivMod) {
3143       // Check if a%b and a/b both exist
3144       Node* d = n-&gt;find_similar(Op_DivI);
3145       if (d) {
3146         // Replace them with a fused divmod if supported
3147         if (Matcher::has_match_rule(Op_DivModI)) {
3148           DivModINode* divmod = DivModINode::make(n);
3149           d-&gt;subsume_by(divmod-&gt;div_proj(), this);
3150           n-&gt;subsume_by(divmod-&gt;mod_proj(), this);
3151         } else {
3152           // replace a%b with a-((a/b)*b)
3153           Node* mult = new MulINode(d, d-&gt;in(2));
3154           Node* sub  = new SubINode(d-&gt;in(1), mult);
3155           n-&gt;subsume_by(sub, this);
3156         }
3157       }
3158     }
3159     break;
3160 
3161   case Op_ModL:
3162     if (UseDivMod) {
3163       // Check if a%b and a/b both exist
3164       Node* d = n-&gt;find_similar(Op_DivL);
3165       if (d) {
3166         // Replace them with a fused divmod if supported
3167         if (Matcher::has_match_rule(Op_DivModL)) {
3168           DivModLNode* divmod = DivModLNode::make(n);
3169           d-&gt;subsume_by(divmod-&gt;div_proj(), this);
3170           n-&gt;subsume_by(divmod-&gt;mod_proj(), this);
3171         } else {
3172           // replace a%b with a-((a/b)*b)
3173           Node* mult = new MulLNode(d, d-&gt;in(2));
3174           Node* sub  = new SubLNode(d-&gt;in(1), mult);
3175           n-&gt;subsume_by(sub, this);
3176         }
3177       }
3178     }
3179     break;
3180 
3181   case Op_LoadVector:
3182   case Op_StoreVector:
3183     break;
3184 
3185   case Op_AddReductionVI:
3186   case Op_AddReductionVL:
3187   case Op_AddReductionVF:
3188   case Op_AddReductionVD:
3189   case Op_MulReductionVI:
3190   case Op_MulReductionVL:
3191   case Op_MulReductionVF:
3192   case Op_MulReductionVD:
3193     break;
3194 
3195   case Op_PackB:
3196   case Op_PackS:
3197   case Op_PackI:
3198   case Op_PackF:
3199   case Op_PackL:
3200   case Op_PackD:
3201     if (n-&gt;req()-1 &gt; 2) {
3202       // Replace many operand PackNodes with a binary tree for matching
3203       PackNode* p = (PackNode*) n;
3204       Node* btp = p-&gt;binary_tree_pack(1, n-&gt;req());
3205       n-&gt;subsume_by(btp, this);
3206     }
3207     break;
3208   case Op_Loop:
3209   case Op_CountedLoop:
3210     if (n-&gt;as_Loop()-&gt;is_inner_loop()) {
3211       frc.inc_inner_loop_count();
3212     }
3213     break;
3214   case Op_LShiftI:
3215   case Op_RShiftI:
3216   case Op_URShiftI:
3217   case Op_LShiftL:
3218   case Op_RShiftL:
3219   case Op_URShiftL:
3220     if (Matcher::need_masked_shift_count) {
3221       // The cpu's shift instructions don't restrict the count to the
3222       // lower 5/6 bits. We need to do the masking ourselves.
3223       Node* in2 = n-&gt;in(2);
3224       juint mask = (n-&gt;bottom_type() == TypeInt::INT) ? (BitsPerInt - 1) : (BitsPerLong - 1);
3225       const TypeInt* t = in2-&gt;find_int_type();
3226       if (t != NULL &amp;&amp; t-&gt;is_con()) {
3227         juint shift = t-&gt;get_con();
3228         if (shift &gt; mask) { // Unsigned cmp
3229           n-&gt;set_req(2, ConNode::make(TypeInt::make(shift &amp; mask)));
3230         }
3231       } else {
3232         if (t == NULL || t-&gt;_lo &lt; 0 || t-&gt;_hi &gt; (int)mask) {
3233           Node* shift = new AndINode(in2, ConNode::make(TypeInt::make(mask)));
3234           n-&gt;set_req(2, shift);
3235         }
3236       }
3237       if (in2-&gt;outcnt() == 0) { // Remove dead node
3238         in2-&gt;disconnect_inputs(NULL, this);
3239       }
3240     }
3241     break;
3242   case Op_MemBarStoreStore:
3243   case Op_MemBarRelease:
3244     // Break the link with AllocateNode: it is no longer useful and
3245     // confuses register allocation.
3246     if (n-&gt;req() &gt; MemBarNode::Precedent) {
3247       n-&gt;set_req(MemBarNode::Precedent, top());
3248     }
3249     break;
3250   case Op_RangeCheck: {
3251     RangeCheckNode* rc = n-&gt;as_RangeCheck();
3252     Node* iff = new IfNode(rc-&gt;in(0), rc-&gt;in(1), rc-&gt;_prob, rc-&gt;_fcnt);
3253     n-&gt;subsume_by(iff, this);
3254     frc._tests.push(iff);
3255     break;
3256   }
3257   default:
3258     assert( !n-&gt;is_Call(), "" );
3259     assert( !n-&gt;is_Mem(), "" );
3260     assert( nop != Op_ProfileBoolean, "should be eliminated during IGVN");
3261     break;
3262   }
3263 
3264   // Collect CFG split points
3265   if (n-&gt;is_MultiBranch() &amp;&amp; !n-&gt;is_RangeCheck()) {
3266     frc._tests.push(n);
3267   }
3268 }
3269 
3270 //------------------------------final_graph_reshaping_walk---------------------
3271 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3272 // requires that the walk visits a node's inputs before visiting the node.
3273 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3274   ResourceArea *area = Thread::current()-&gt;resource_area();
3275   Unique_Node_List sfpt(area);
3276 
3277   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3278   uint cnt = root-&gt;req();
3279   Node *n = root;
3280   uint  i = 0;
3281   while (true) {
3282     if (i &lt; cnt) {
3283       // Place all non-visited non-null inputs onto stack
3284       Node* m = n-&gt;in(i);
3285       ++i;
3286       if (m != NULL &amp;&amp; !frc._visited.test_set(m-&gt;_idx)) {
3287         if (m-&gt;is_SafePoint() &amp;&amp; m-&gt;as_SafePoint()-&gt;jvms() != NULL) {
3288           // compute worst case interpreter size in case of a deoptimization
3289           update_interpreter_frame_size(m-&gt;as_SafePoint()-&gt;jvms()-&gt;interpreter_frame_size());
3290 
3291           sfpt.push(m);
3292         }
3293         cnt = m-&gt;req();
3294         nstack.push(n, i); // put on stack parent and next input's index
3295         n = m;
3296         i = 0;
3297       }
3298     } else {
3299       // Now do post-visit work
3300       final_graph_reshaping_impl( n, frc );
3301       if (nstack.is_empty())
3302         break;             // finished
3303       n = nstack.node();   // Get node from stack
3304       cnt = n-&gt;req();
3305       i = nstack.index();
3306       nstack.pop();        // Shift to the next node on stack
3307     }
3308   }
3309 
3310   // Skip next transformation if compressed oops are not used.
3311   if ((UseCompressedOops &amp;&amp; !Matcher::gen_narrow_oop_implicit_null_checks()) ||
3312       (!UseCompressedOops &amp;&amp; !UseCompressedClassPointers))
3313     return;
3314 
3315   // Go over safepoints nodes to skip DecodeN/DecodeNKlass nodes for debug edges.
3316   // It could be done for an uncommon traps or any safepoints/calls
3317   // if the DecodeN/DecodeNKlass node is referenced only in a debug info.
3318   while (sfpt.size() &gt; 0) {
3319     n = sfpt.pop();
3320     JVMState *jvms = n-&gt;as_SafePoint()-&gt;jvms();
3321     assert(jvms != NULL, "sanity");
3322     int start = jvms-&gt;debug_start();
3323     int end   = n-&gt;req();
3324     bool is_uncommon = (n-&gt;is_CallStaticJava() &amp;&amp;
3325                         n-&gt;as_CallStaticJava()-&gt;uncommon_trap_request() != 0);
3326     for (int j = start; j &lt; end; j++) {
3327       Node* in = n-&gt;in(j);
3328       if (in-&gt;is_DecodeNarrowPtr()) {
3329         bool safe_to_skip = true;
3330         if (!is_uncommon ) {
3331           // Is it safe to skip?
3332           for (uint i = 0; i &lt; in-&gt;outcnt(); i++) {
3333             Node* u = in-&gt;raw_out(i);
3334             if (!u-&gt;is_SafePoint() ||
3335                  u-&gt;is_Call() &amp;&amp; u-&gt;as_Call()-&gt;has_non_debug_use(n)) {
3336               safe_to_skip = false;
3337             }
3338           }
3339         }
3340         if (safe_to_skip) {
3341           n-&gt;set_req(j, in-&gt;in(1));
3342         }
3343         if (in-&gt;outcnt() == 0) {
3344           in-&gt;disconnect_inputs(NULL, this);
3345         }
3346       }
3347     }
3348   }
3349 }
3350 
3351 //------------------------------final_graph_reshaping--------------------------
3352 // Final Graph Reshaping.
3353 //
3354 // (1) Clone simple inputs to uncommon calls, so they can be scheduled late
3355 //     and not commoned up and forced early.  Must come after regular
3356 //     optimizations to avoid GVN undoing the cloning.  Clone constant
3357 //     inputs to Loop Phis; these will be split by the allocator anyways.
3358 //     Remove Opaque nodes.
3359 // (2) Move last-uses by commutative operations to the left input to encourage
3360 //     Intel update-in-place two-address operations and better register usage
3361 //     on RISCs.  Must come after regular optimizations to avoid GVN Ideal
3362 //     calls canonicalizing them back.
3363 // (3) Count the number of double-precision FP ops, single-precision FP ops
3364 //     and call sites.  On Intel, we can get correct rounding either by
3365 //     forcing singles to memory (requires extra stores and loads after each
3366 //     FP bytecode) or we can set a rounding mode bit (requires setting and
3367 //     clearing the mode bit around call sites).  The mode bit is only used
3368 //     if the relative frequency of single FP ops to calls is low enough.
3369 //     This is a key transform for SPEC mpeg_audio.
3370 // (4) Detect infinite loops; blobs of code reachable from above but not
3371 //     below.  Several of the Code_Gen algorithms fail on such code shapes,
3372 //     so we simply bail out.  Happens a lot in ZKM.jar, but also happens
3373 //     from time to time in other codes (such as -Xcomp finalizer loops, etc).
3374 //     Detection is by looking for IfNodes where only 1 projection is
3375 //     reachable from below or CatchNodes missing some targets.
3376 // (5) Assert for insane oop offsets in debug mode.
3377 
3378 bool Compile::final_graph_reshaping() {
3379   // an infinite loop may have been eliminated by the optimizer,
3380   // in which case the graph will be empty.
3381   if (root()-&gt;req() == 1) {
3382     record_method_not_compilable("trivial infinite loop");
3383     return true;
3384   }
3385 
3386   // Expensive nodes have their control input set to prevent the GVN
3387   // from freely commoning them. There's no GVN beyond this point so
3388   // no need to keep the control input. We want the expensive nodes to
3389   // be freely moved to the least frequent code path by gcm.
3390   assert(OptimizeExpensiveOps || expensive_count() == 0, "optimization off but list non empty?");
3391   for (int i = 0; i &lt; expensive_count(); i++) {
3392     _expensive_nodes-&gt;at(i)-&gt;set_req(0, NULL);
3393   }
3394 
3395   Final_Reshape_Counts frc;
3396 
3397   // Visit everybody reachable!
3398   // Allocate stack of size C-&gt;live_nodes()/2 to avoid frequent realloc
3399   Node_Stack nstack(live_nodes() &gt;&gt; 1);
3400   final_graph_reshaping_walk(nstack, root(), frc);
3401 
3402   // Check for unreachable (from below) code (i.e., infinite loops).
3403   for( uint i = 0; i &lt; frc._tests.size(); i++ ) {
3404     MultiBranchNode *n = frc._tests[i]-&gt;as_MultiBranch();
3405     // Get number of CFG targets.
3406     // Note that PCTables include exception targets after calls.
3407     uint required_outcnt = n-&gt;required_outcnt();
3408     if (n-&gt;outcnt() != required_outcnt) {
3409       // Check for a few special cases.  Rethrow Nodes never take the
3410       // 'fall-thru' path, so expected kids is 1 less.
3411       if (n-&gt;is_PCTable() &amp;&amp; n-&gt;in(0) &amp;&amp; n-&gt;in(0)-&gt;in(0)) {
3412         if (n-&gt;in(0)-&gt;in(0)-&gt;is_Call()) {
3413           CallNode *call = n-&gt;in(0)-&gt;in(0)-&gt;as_Call();
3414           if (call-&gt;entry_point() == OptoRuntime::rethrow_stub()) {
3415             required_outcnt--;      // Rethrow always has 1 less kid
3416           } else if (call-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
3417                      call-&gt;is_CallDynamicJava()) {
3418             // Check for null receiver. In such case, the optimizer has
3419             // detected that the virtual call will always result in a null
3420             // pointer exception. The fall-through projection of this CatchNode
3421             // will not be populated.
3422             Node *arg0 = call-&gt;in(TypeFunc::Parms);
3423             if (arg0-&gt;is_Type() &amp;&amp;
3424                 arg0-&gt;as_Type()-&gt;type()-&gt;higher_equal(TypePtr::NULL_PTR)) {
3425               required_outcnt--;
3426             }
3427           } else if (call-&gt;entry_point() == OptoRuntime::new_array_Java() &amp;&amp;
3428                      call-&gt;req() &gt; TypeFunc::Parms+1 &amp;&amp;
3429                      call-&gt;is_CallStaticJava()) {
3430             // Check for negative array length. In such case, the optimizer has
3431             // detected that the allocation attempt will always result in an
3432             // exception. There is no fall-through projection of this CatchNode .
3433             Node *arg1 = call-&gt;in(TypeFunc::Parms+1);
3434             if (arg1-&gt;is_Type() &amp;&amp;
3435                 arg1-&gt;as_Type()-&gt;type()-&gt;join(TypeInt::POS)-&gt;empty()) {
3436               required_outcnt--;
3437             }
3438           }
3439         }
3440       }
3441       // Recheck with a better notion of 'required_outcnt'
3442       if (n-&gt;outcnt() != required_outcnt) {
3443         record_method_not_compilable("malformed control flow");
3444         return true;            // Not all targets reachable!
3445       }
3446     }
3447     // Check that I actually visited all kids.  Unreached kids
3448     // must be infinite loops.
3449     for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++)
3450       if (!frc._visited.test(n-&gt;fast_out(j)-&gt;_idx)) {
3451         record_method_not_compilable("infinite loop");
3452         return true;            // Found unvisited kid; must be unreach
3453       }
3454   }
3455 
3456   // If original bytecodes contained a mixture of floats and doubles
3457   // check if the optimizer has made it homogenous, item (3).
3458   if( Use24BitFPMode &amp;&amp; Use24BitFP &amp;&amp; UseSSE == 0 &amp;&amp;
3459       frc.get_float_count() &gt; 32 &amp;&amp;
3460       frc.get_double_count() == 0 &amp;&amp;
3461       (10 * frc.get_call_count() &lt; frc.get_float_count()) ) {
3462     set_24_bit_selection_and_mode( false,  true );
3463   }
3464 
3465   set_java_calls(frc.get_java_call_count());
3466   set_inner_loops(frc.get_inner_loop_count());
3467 
3468   // No infinite loops, no reason to bail out.
3469   return false;
3470 }
3471 
3472 //-----------------------------too_many_traps----------------------------------
3473 // Report if there are too many traps at the current method and bci.
3474 // Return true if there was a trap, and/or PerMethodTrapLimit is exceeded.
3475 bool Compile::too_many_traps(ciMethod* method,
3476                              int bci,
3477                              Deoptimization::DeoptReason reason) {
3478   ciMethodData* md = method-&gt;method_data();
3479   if (md-&gt;is_empty()) {
3480     // Assume the trap has not occurred, or that it occurred only
3481     // because of a transient condition during start-up in the interpreter.
3482     return false;
3483   }
3484   ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this-&gt;method() : NULL;
3485   if (md-&gt;has_trap_at(bci, m, reason) != 0) {
3486     // Assume PerBytecodeTrapLimit==0, for a more conservative heuristic.
3487     // Also, if there are multiple reasons, or if there is no per-BCI record,
3488     // assume the worst.
3489     if (log())
3490       log()-&gt;elem("observe trap='%s' count='%d'",
3491                   Deoptimization::trap_reason_name(reason),
3492                   md-&gt;trap_count(reason));
3493     return true;
3494   } else {
3495     // Ignore method/bci and see if there have been too many globally.
3496     return too_many_traps(reason, md);
3497   }
3498 }
3499 
3500 // Less-accurate variant which does not require a method and bci.
3501 bool Compile::too_many_traps(Deoptimization::DeoptReason reason,
3502                              ciMethodData* logmd) {
3503   if (trap_count(reason) &gt;= Deoptimization::per_method_trap_limit(reason)) {
3504     // Too many traps globally.
3505     // Note that we use cumulative trap_count, not just md-&gt;trap_count.
3506     if (log()) {
3507       int mcount = (logmd == NULL)? -1: (int)logmd-&gt;trap_count(reason);
3508       log()-&gt;elem("observe trap='%s' count='0' mcount='%d' ccount='%d'",
3509                   Deoptimization::trap_reason_name(reason),
3510                   mcount, trap_count(reason));
3511     }
3512     return true;
3513   } else {
3514     // The coast is clear.
3515     return false;
3516   }
3517 }
3518 
3519 //--------------------------too_many_recompiles--------------------------------
3520 // Report if there are too many recompiles at the current method and bci.
3521 // Consults PerBytecodeRecompilationCutoff and PerMethodRecompilationCutoff.
3522 // Is not eager to return true, since this will cause the compiler to use
3523 // Action_none for a trap point, to avoid too many recompilations.
3524 bool Compile::too_many_recompiles(ciMethod* method,
3525                                   int bci,
3526                                   Deoptimization::DeoptReason reason) {
3527   ciMethodData* md = method-&gt;method_data();
3528   if (md-&gt;is_empty()) {
3529     // Assume the trap has not occurred, or that it occurred only
3530     // because of a transient condition during start-up in the interpreter.
3531     return false;
3532   }
3533   // Pick a cutoff point well within PerBytecodeRecompilationCutoff.
3534   uint bc_cutoff = (uint) PerBytecodeRecompilationCutoff / 8;
3535   uint m_cutoff  = (uint) PerMethodRecompilationCutoff / 2 + 1;  // not zero
3536   Deoptimization::DeoptReason per_bc_reason
3537     = Deoptimization::reason_recorded_per_bytecode_if_any(reason);
3538   ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this-&gt;method() : NULL;
3539   if ((per_bc_reason == Deoptimization::Reason_none
3540        || md-&gt;has_trap_at(bci, m, reason) != 0)
3541       // The trap frequency measure we care about is the recompile count:
3542       &amp;&amp; md-&gt;trap_recompiled_at(bci, m)
3543       &amp;&amp; md-&gt;overflow_recompile_count() &gt;= bc_cutoff) {
3544     // Do not emit a trap here if it has already caused recompilations.
3545     // Also, if there are multiple reasons, or if there is no per-BCI record,
3546     // assume the worst.
3547     if (log())
3548       log()-&gt;elem("observe trap='%s recompiled' count='%d' recompiles2='%d'",
3549                   Deoptimization::trap_reason_name(reason),
3550                   md-&gt;trap_count(reason),
3551                   md-&gt;overflow_recompile_count());
3552     return true;
3553   } else if (trap_count(reason) != 0
3554              &amp;&amp; decompile_count() &gt;= m_cutoff) {
3555     // Too many recompiles globally, and we have seen this sort of trap.
3556     // Use cumulative decompile_count, not just md-&gt;decompile_count.
3557     if (log())
3558       log()-&gt;elem("observe trap='%s' count='%d' mcount='%d' decompiles='%d' mdecompiles='%d'",
3559                   Deoptimization::trap_reason_name(reason),
3560                   md-&gt;trap_count(reason), trap_count(reason),
3561                   md-&gt;decompile_count(), decompile_count());
3562     return true;
3563   } else {
3564     // The coast is clear.
3565     return false;
3566   }
3567 }
3568 
3569 // Compute when not to trap. Used by matching trap based nodes and
3570 // NullCheck optimization.
3571 void Compile::set_allowed_deopt_reasons() {
3572   _allowed_reasons = 0;
3573   if (is_method_compilation()) {
3574     for (int rs = (int)Deoptimization::Reason_none+1; rs &lt; Compile::trapHistLength; rs++) {
3575       assert(rs &lt; BitsPerInt, "recode bit map");
3576       if (!too_many_traps((Deoptimization::DeoptReason) rs)) {
3577         _allowed_reasons |= nth_bit(rs);
3578       }
3579     }
3580   }
3581 }
3582 
3583 #ifndef PRODUCT
3584 //------------------------------verify_graph_edges---------------------------
3585 // Walk the Graph and verify that there is a one-to-one correspondence
3586 // between Use-Def edges and Def-Use edges in the graph.
3587 void Compile::verify_graph_edges(bool no_dead_code) {
3588   if (VerifyGraphEdges) {
3589     ResourceArea *area = Thread::current()-&gt;resource_area();
3590     Unique_Node_List visited(area);
3591     // Call recursive graph walk to check edges
3592     _root-&gt;verify_edges(visited);
3593     if (no_dead_code) {
3594       // Now make sure that no visited node is used by an unvisited node.
3595       bool dead_nodes = false;
3596       Unique_Node_List checked(area);
3597       while (visited.size() &gt; 0) {
3598         Node* n = visited.pop();
3599         checked.push(n);
3600         for (uint i = 0; i &lt; n-&gt;outcnt(); i++) {
3601           Node* use = n-&gt;raw_out(i);
3602           if (checked.member(use))  continue;  // already checked
3603           if (visited.member(use))  continue;  // already in the graph
3604           if (use-&gt;is_Con())        continue;  // a dead ConNode is OK
3605           // At this point, we have found a dead node which is DU-reachable.
3606           if (!dead_nodes) {
3607             tty-&gt;print_cr("*** Dead nodes reachable via DU edges:");
3608             dead_nodes = true;
3609           }
3610           use-&gt;dump(2);
3611           tty-&gt;print_cr("---");
3612           checked.push(use);  // No repeats; pretend it is now checked.
3613         }
3614       }
3615       assert(!dead_nodes, "using nodes must be reachable from root");
3616     }
3617   }
3618 }
3619 
3620 // Verify GC barriers consistency
3621 // Currently supported:
3622 // - G1 pre-barriers (see GraphKit::g1_write_barrier_pre())
3623 void Compile::verify_barriers() {
3624   if (UseG1GC) {
3625     // Verify G1 pre-barriers
3626     const int marking_offset = in_bytes(JavaThread::satb_mark_queue_offset() + SATBMarkQueue::byte_offset_of_active());
3627 
3628     ResourceArea *area = Thread::current()-&gt;resource_area();
3629     Unique_Node_List visited(area);
3630     Node_List worklist(area);
3631     // We're going to walk control flow backwards starting from the Root
3632     worklist.push(_root);
3633     while (worklist.size() &gt; 0) {
3634       Node* x = worklist.pop();
3635       if (x == NULL || x == top()) continue;
3636       if (visited.member(x)) {
3637         continue;
3638       } else {
3639         visited.push(x);
3640       }
3641 
3642       if (x-&gt;is_Region()) {
3643         for (uint i = 1; i &lt; x-&gt;req(); i++) {
3644           worklist.push(x-&gt;in(i));
3645         }
3646       } else {
3647         worklist.push(x-&gt;in(0));
3648         // We are looking for the pattern:
3649         //                            /-&gt;ThreadLocal
3650         // If-&gt;Bool-&gt;CmpI-&gt;LoadB-&gt;AddP-&gt;ConL(marking_offset)
3651         //              \-&gt;ConI(0)
3652         // We want to verify that the If and the LoadB have the same control
3653         // See GraphKit::g1_write_barrier_pre()
3654         if (x-&gt;is_If()) {
3655           IfNode *iff = x-&gt;as_If();
3656           if (iff-&gt;in(1)-&gt;is_Bool() &amp;&amp; iff-&gt;in(1)-&gt;in(1)-&gt;is_Cmp()) {
3657             CmpNode *cmp = iff-&gt;in(1)-&gt;in(1)-&gt;as_Cmp();
3658             if (cmp-&gt;Opcode() == Op_CmpI &amp;&amp; cmp-&gt;in(2)-&gt;is_Con() &amp;&amp; cmp-&gt;in(2)-&gt;bottom_type()-&gt;is_int()-&gt;get_con() == 0
3659                 &amp;&amp; cmp-&gt;in(1)-&gt;is_Load()) {
3660               LoadNode* load = cmp-&gt;in(1)-&gt;as_Load();
3661               if (load-&gt;Opcode() == Op_LoadB &amp;&amp; load-&gt;in(2)-&gt;is_AddP() &amp;&amp; load-&gt;in(2)-&gt;in(2)-&gt;Opcode() == Op_ThreadLocal
3662                   &amp;&amp; load-&gt;in(2)-&gt;in(3)-&gt;is_Con()
3663                   &amp;&amp; load-&gt;in(2)-&gt;in(3)-&gt;bottom_type()-&gt;is_intptr_t()-&gt;get_con() == marking_offset) {
3664 
3665                 Node* if_ctrl = iff-&gt;in(0);
3666                 Node* load_ctrl = load-&gt;in(0);
3667 
3668                 if (if_ctrl != load_ctrl) {
3669                   // Skip possible CProj-&gt;NeverBranch in infinite loops
3670                   if ((if_ctrl-&gt;is_Proj() &amp;&amp; if_ctrl-&gt;Opcode() == Op_CProj)
3671                       &amp;&amp; (if_ctrl-&gt;in(0)-&gt;is_MultiBranch() &amp;&amp; if_ctrl-&gt;in(0)-&gt;Opcode() == Op_NeverBranch)) {
3672                     if_ctrl = if_ctrl-&gt;in(0)-&gt;in(0);
3673                   }
3674                 }
3675                 assert(load_ctrl != NULL &amp;&amp; if_ctrl == load_ctrl, "controls must match");
3676               }
3677             }
3678           }
3679         }
3680       }
3681     }
3682   }
3683 }
3684 
3685 #endif
3686 
3687 // The Compile object keeps track of failure reasons separately from the ciEnv.
3688 // This is required because there is not quite a 1-1 relation between the
3689 // ciEnv and its compilation task and the Compile object.  Note that one
3690 // ciEnv might use two Compile objects, if C2Compiler::compile_method decides
3691 // to backtrack and retry without subsuming loads.  Other than this backtracking
3692 // behavior, the Compile's failure reason is quietly copied up to the ciEnv
3693 // by the logic in C2Compiler.
3694 void Compile::record_failure(const char* reason) {
3695   if (log() != NULL) {
3696     log()-&gt;elem("failure reason='%s' phase='compile'", reason);
3697   }
3698   if (_failure_reason == NULL) {
3699     // Record the first failure reason.
3700     _failure_reason = reason;
3701   }
3702 
3703   if (!C-&gt;failure_reason_is(C2Compiler::retry_no_subsuming_loads())) {
3704     C-&gt;print_method(PHASE_FAILURE);
3705   }
3706   _root = NULL;  // flush the graph, too
3707 }
3708 
3709 Compile::TracePhase::TracePhase(const char* name, elapsedTimer* accumulator)
3710   : TraceTime(name, accumulator, CITime, CITimeVerbose),
3711     _phase_name(name), _dolog(CITimeVerbose)
3712 {
3713   if (_dolog) {
3714     C = Compile::current();
3715     _log = C-&gt;log();
3716   } else {
3717     C = NULL;
3718     _log = NULL;
3719   }
3720   if (_log != NULL) {
3721     _log-&gt;begin_head("phase name='%s' nodes='%d' live='%d'", _phase_name, C-&gt;unique(), C-&gt;live_nodes());
3722     _log-&gt;stamp();
3723     _log-&gt;end_head();
3724   }
3725 }
3726 
3727 Compile::TracePhase::~TracePhase() {
3728 
3729   C = Compile::current();
3730   if (_dolog) {
3731     _log = C-&gt;log();
3732   } else {
3733     _log = NULL;
3734   }
3735 
3736 #ifdef ASSERT
3737   if (PrintIdealNodeCount) {
3738     tty-&gt;print_cr("phase name='%s' nodes='%d' live='%d' live_graph_walk='%d'",
3739                   _phase_name, C-&gt;unique(), C-&gt;live_nodes(), C-&gt;count_live_nodes_by_graph_walk());
3740   }
3741 
3742   if (VerifyIdealNodeCount) {
3743     Compile::current()-&gt;print_missing_nodes();
3744   }
3745 #endif
3746 
3747   if (_log != NULL) {
3748     _log-&gt;done("phase name='%s' nodes='%d' live='%d'", _phase_name, C-&gt;unique(), C-&gt;live_nodes());
3749   }
3750 }
3751 
3752 //=============================================================================
3753 // Two Constant's are equal when the type and the value are equal.
3754 bool Compile::Constant::operator==(const Constant&amp; other) {
3755   if (type()          != other.type()         )  return false;
3756   if (can_be_reused() != other.can_be_reused())  return false;
3757   // For floating point values we compare the bit pattern.
3758   switch (type()) {
3759   case T_FLOAT:   return (_v._value.i == other._v._value.i);
3760   case T_LONG:
3761   case T_DOUBLE:  return (_v._value.j == other._v._value.j);
3762   case T_OBJECT:
3763   case T_ADDRESS: return (_v._value.l == other._v._value.l);
3764   case T_VOID:    return (_v._value.l == other._v._value.l);  // jump-table entries
3765   case T_METADATA: return (_v._metadata == other._v._metadata);
3766   default: ShouldNotReachHere();
3767   }
3768   return false;
3769 }
3770 
3771 static int type_to_size_in_bytes(BasicType t) {
3772   switch (t) {
3773   case T_LONG:    return sizeof(jlong  );
3774   case T_FLOAT:   return sizeof(jfloat );
3775   case T_DOUBLE:  return sizeof(jdouble);
3776   case T_METADATA: return sizeof(Metadata*);
3777     // We use T_VOID as marker for jump-table entries (labels) which
3778     // need an internal word relocation.
3779   case T_VOID:
3780   case T_ADDRESS:
3781   case T_OBJECT:  return sizeof(jobject);
3782   }
3783 
3784   ShouldNotReachHere();
3785   return -1;
3786 }
3787 
3788 int Compile::ConstantTable::qsort_comparator(Constant* a, Constant* b) {
3789   // sort descending
3790   if (a-&gt;freq() &gt; b-&gt;freq())  return -1;
3791   if (a-&gt;freq() &lt; b-&gt;freq())  return  1;
3792   return 0;
3793 }
3794 
3795 void Compile::ConstantTable::calculate_offsets_and_size() {
3796   // First, sort the array by frequencies.
3797   _constants.sort(qsort_comparator);
3798 
3799 #ifdef ASSERT
3800   // Make sure all jump-table entries were sorted to the end of the
3801   // array (they have a negative frequency).
3802   bool found_void = false;
3803   for (int i = 0; i &lt; _constants.length(); i++) {
3804     Constant con = _constants.at(i);
3805     if (con.type() == T_VOID)
3806       found_void = true;  // jump-tables
3807     else
3808       assert(!found_void, "wrong sorting");
3809   }
3810 #endif
3811 
3812   int offset = 0;
3813   for (int i = 0; i &lt; _constants.length(); i++) {
3814     Constant* con = _constants.adr_at(i);
3815 
3816     // Align offset for type.
3817     int typesize = type_to_size_in_bytes(con-&gt;type());
3818     offset = align_size_up(offset, typesize);
3819     con-&gt;set_offset(offset);   // set constant's offset
3820 
3821     if (con-&gt;type() == T_VOID) {
3822       MachConstantNode* n = (MachConstantNode*) con-&gt;get_jobject();
3823       offset = offset + typesize * n-&gt;outcnt();  // expand jump-table
3824     } else {
3825       offset = offset + typesize;
3826     }
3827   }
3828 
3829   // Align size up to the next section start (which is insts; see
3830   // CodeBuffer::align_at_start).
3831   assert(_size == -1, "already set?");
3832   _size = align_size_up(offset, CodeEntryAlignment);
3833 }
3834 
3835 void Compile::ConstantTable::emit(CodeBuffer&amp; cb) {
3836   MacroAssembler _masm(&amp;cb);
3837   for (int i = 0; i &lt; _constants.length(); i++) {
3838     Constant con = _constants.at(i);
3839     address constant_addr = NULL;
3840     switch (con.type()) {
3841     case T_LONG:   constant_addr = _masm.long_constant(  con.get_jlong()  ); break;
3842     case T_FLOAT:  constant_addr = _masm.float_constant( con.get_jfloat() ); break;
3843     case T_DOUBLE: constant_addr = _masm.double_constant(con.get_jdouble()); break;
3844     case T_OBJECT: {
3845       jobject obj = con.get_jobject();
3846       int oop_index = _masm.oop_recorder()-&gt;find_index(obj);
3847       constant_addr = _masm.address_constant((address) obj, oop_Relocation::spec(oop_index));
3848       break;
3849     }
3850     case T_ADDRESS: {
3851       address addr = (address) con.get_jobject();
3852       constant_addr = _masm.address_constant(addr);
3853       break;
3854     }
3855     // We use T_VOID as marker for jump-table entries (labels) which
3856     // need an internal word relocation.
3857     case T_VOID: {
3858       MachConstantNode* n = (MachConstantNode*) con.get_jobject();
3859       // Fill the jump-table with a dummy word.  The real value is
3860       // filled in later in fill_jump_table.
3861       address dummy = (address) n;
3862       constant_addr = _masm.address_constant(dummy);
3863       // Expand jump-table
3864       for (uint i = 1; i &lt; n-&gt;outcnt(); i++) {
3865         address temp_addr = _masm.address_constant(dummy + i);
3866         assert(temp_addr, "consts section too small");
3867       }
3868       break;
3869     }
3870     case T_METADATA: {
3871       Metadata* obj = con.get_metadata();
3872       int metadata_index = _masm.oop_recorder()-&gt;find_index(obj);
3873       constant_addr = _masm.address_constant((address) obj, metadata_Relocation::spec(metadata_index));
3874       break;
3875     }
3876     default: ShouldNotReachHere();
3877     }
3878     assert(constant_addr, "consts section too small");
3879     assert((constant_addr - _masm.code()-&gt;consts()-&gt;start()) == con.offset(),
3880             "must be: %d == %d", (int) (constant_addr - _masm.code()-&gt;consts()-&gt;start()), (int)(con.offset()));
3881   }
3882 }
3883 
3884 int Compile::ConstantTable::find_offset(Constant&amp; con) const {
3885   int idx = _constants.find(con);
3886   assert(idx != -1, "constant must be in constant table");
3887   int offset = _constants.at(idx).offset();
3888   assert(offset != -1, "constant table not emitted yet?");
3889   return offset;
3890 }
3891 
3892 void Compile::ConstantTable::add(Constant&amp; con) {
3893   if (con.can_be_reused()) {
3894     int idx = _constants.find(con);
3895     if (idx != -1 &amp;&amp; _constants.at(idx).can_be_reused()) {
3896       _constants.adr_at(idx)-&gt;inc_freq(con.freq());  // increase the frequency by the current value
3897       return;
3898     }
3899   }
3900   (void) _constants.append(con);
3901 }
3902 
3903 Compile::Constant Compile::ConstantTable::add(MachConstantNode* n, BasicType type, jvalue value) {
3904   Block* b = Compile::current()-&gt;cfg()-&gt;get_block_for_node(n);
3905   Constant con(type, value, b-&gt;_freq);
3906   add(con);
3907   return con;
3908 }
3909 
3910 Compile::Constant Compile::ConstantTable::add(Metadata* metadata) {
3911   Constant con(metadata);
3912   add(con);
3913   return con;
3914 }
3915 
3916 Compile::Constant Compile::ConstantTable::add(MachConstantNode* n, MachOper* oper) {
3917   jvalue value;
3918   BasicType type = oper-&gt;type()-&gt;basic_type();
3919   switch (type) {
3920   case T_LONG:    value.j = oper-&gt;constantL(); break;
3921   case T_FLOAT:   value.f = oper-&gt;constantF(); break;
3922   case T_DOUBLE:  value.d = oper-&gt;constantD(); break;
3923   case T_OBJECT:
3924   case T_ADDRESS: value.l = (jobject) oper-&gt;constant(); break;
3925   case T_METADATA: return add((Metadata*)oper-&gt;constant()); break;
3926   default: guarantee(false, "unhandled type: %s", type2name(type));
3927   }
3928   return add(n, type, value);
3929 }
3930 
3931 Compile::Constant Compile::ConstantTable::add_jump_table(MachConstantNode* n) {
3932   jvalue value;
3933   // We can use the node pointer here to identify the right jump-table
3934   // as this method is called from Compile::Fill_buffer right before
3935   // the MachNodes are emitted and the jump-table is filled (means the
3936   // MachNode pointers do not change anymore).
3937   value.l = (jobject) n;
3938   Constant con(T_VOID, value, next_jump_table_freq(), false);  // Labels of a jump-table cannot be reused.
3939   add(con);
3940   return con;
3941 }
3942 
3943 void Compile::ConstantTable::fill_jump_table(CodeBuffer&amp; cb, MachConstantNode* n, GrowableArray&lt;Label*&gt; labels) const {
3944   // If called from Compile::scratch_emit_size do nothing.
3945   if (Compile::current()-&gt;in_scratch_emit_size())  return;
3946 
3947   assert(labels.is_nonempty(), "must be");
3948   assert((uint) labels.length() == n-&gt;outcnt(), "must be equal: %d == %d", labels.length(), n-&gt;outcnt());
3949 
3950   // Since MachConstantNode::constant_offset() also contains
3951   // table_base_offset() we need to subtract the table_base_offset()
3952   // to get the plain offset into the constant table.
3953   int offset = n-&gt;constant_offset() - table_base_offset();
3954 
3955   MacroAssembler _masm(&amp;cb);
3956   address* jump_table_base = (address*) (_masm.code()-&gt;consts()-&gt;start() + offset);
3957 
3958   for (uint i = 0; i &lt; n-&gt;outcnt(); i++) {
3959     address* constant_addr = &amp;jump_table_base[i];
3960     assert(*constant_addr == (((address) n) + i), "all jump-table entries must contain adjusted node pointer: " INTPTR_FORMAT " == " INTPTR_FORMAT, p2i(*constant_addr), p2i(((address) n) + i));
3961     *constant_addr = cb.consts()-&gt;target(*labels.at(i), (address) constant_addr);
3962     cb.consts()-&gt;relocate((address) constant_addr, relocInfo::internal_word_type);
3963   }
3964 }
3965 
3966 //----------------------------static_subtype_check-----------------------------
3967 // Shortcut important common cases when superklass is exact:
3968 // (0) superklass is java.lang.Object (can occur in reflective code)
3969 // (1) subklass is already limited to a subtype of superklass =&gt; always ok
3970 // (2) subklass does not overlap with superklass =&gt; always fail
3971 // (3) superklass has NO subtypes and we can check with a simple compare.
3972 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
3973   if (StressReflectiveCode) {
3974     return SSC_full_test;       // Let caller generate the general case.
3975   }
3976 
3977   if (superk == env()-&gt;Object_klass()) {
3978     return SSC_always_true;     // (0) this test cannot fail
3979   }
3980 
3981   ciType* superelem = superk;
3982   if (superelem-&gt;is_array_klass())
3983     superelem = superelem-&gt;as_array_klass()-&gt;base_element_type();
3984 
3985   if (!subk-&gt;is_interface()) {  // cannot trust static interface types yet
3986     if (subk-&gt;is_subtype_of(superk)) {
3987       return SSC_always_true;   // (1) false path dead; no dynamic test needed
3988     }
3989     if (!(superelem-&gt;is_klass() &amp;&amp; superelem-&gt;as_klass()-&gt;is_interface()) &amp;&amp;
3990         !superk-&gt;is_subtype_of(subk)) {
3991       return SSC_always_false;
3992     }
3993   }
3994 
3995   // If casting to an instance klass, it must have no subtypes
3996   if (superk-&gt;is_interface()) {
3997     // Cannot trust interfaces yet.
3998     // %%% S.B. superk-&gt;nof_implementors() == 1
3999   } else if (superelem-&gt;is_instance_klass()) {
4000     ciInstanceKlass* ik = superelem-&gt;as_instance_klass();
4001     if (!ik-&gt;has_subklass() &amp;&amp; !ik-&gt;is_interface()) {
4002       if (!ik-&gt;is_final()) {
4003         // Add a dependency if there is a chance of a later subclass.
4004         dependencies()-&gt;assert_leaf_type(ik);
4005       }
4006       return SSC_easy_test;     // (3) caller can do a simple ptr comparison
4007     }
4008   } else {
4009     // A primitive array type has no subtypes.
4010     return SSC_easy_test;       // (3) caller can do a simple ptr comparison
4011   }
4012 
4013   return SSC_full_test;
4014 }
4015 
4016 Node* Compile::conv_I2X_index(PhaseGVN* phase, Node* idx, const TypeInt* sizetype, Node* ctrl) {
4017 #ifdef _LP64
4018   // The scaled index operand to AddP must be a clean 64-bit value.
4019   // Java allows a 32-bit int to be incremented to a negative
4020   // value, which appears in a 64-bit register as a large
4021   // positive number.  Using that large positive number as an
4022   // operand in pointer arithmetic has bad consequences.
4023   // On the other hand, 32-bit overflow is rare, and the possibility
4024   // can often be excluded, if we annotate the ConvI2L node with
4025   // a type assertion that its value is known to be a small positive
4026   // number.  (The prior range check has ensured this.)
4027   // This assertion is used by ConvI2LNode::Ideal.
4028   int index_max = max_jint - 1;  // array size is max_jint, index is one less
4029   if (sizetype != NULL) index_max = sizetype-&gt;_hi - 1;
4030   const TypeInt* iidxtype = TypeInt::make(0, index_max, Type::WidenMax);
4031   idx = constrained_convI2L(phase, idx, iidxtype, ctrl);
4032 #endif
4033   return idx;
4034 }
4035 
4036 // Convert integer value to a narrowed long type dependent on ctrl (for example, a range check)
4037 Node* Compile::constrained_convI2L(PhaseGVN* phase, Node* value, const TypeInt* itype, Node* ctrl) {
4038   if (ctrl != NULL) {
4039     // Express control dependency by a CastII node with a narrow type.
4040     value = new CastIINode(value, itype, false, true /* range check dependency */);
4041     // Make the CastII node dependent on the control input to prevent the narrowed ConvI2L
4042     // node from floating above the range check during loop optimizations. Otherwise, the
4043     // ConvI2L node may be eliminated independently of the range check, causing the data path
4044     // to become TOP while the control path is still there (although it's unreachable).
4045     value-&gt;set_req(0, ctrl);
4046     // Save CastII node to remove it after loop optimizations.
4047     phase-&gt;C-&gt;add_range_check_cast(value);
4048     value = phase-&gt;transform(value);
4049   }
4050   const TypeLong* ltype = TypeLong::make(itype-&gt;_lo, itype-&gt;_hi, itype-&gt;_widen);
4051   return phase-&gt;transform(new ConvI2LNode(value, ltype));
4052 }
4053 
4054 // The message about the current inlining is accumulated in
4055 // _print_inlining_stream and transfered into the _print_inlining_list
4056 // once we know whether inlining succeeds or not. For regular
4057 // inlining, messages are appended to the buffer pointed by
4058 // _print_inlining_idx in the _print_inlining_list. For late inlining,
4059 // a new buffer is added after _print_inlining_idx in the list. This
4060 // way we can update the inlining message for late inlining call site
4061 // when the inlining is attempted again.
4062 void Compile::print_inlining_init() {
4063   if (print_inlining() || print_intrinsics()) {
4064     _print_inlining_stream = new stringStream();
4065     _print_inlining_list = new (comp_arena())GrowableArray&lt;PrintInliningBuffer&gt;(comp_arena(), 1, 1, PrintInliningBuffer());
4066   }
4067 }
4068 
4069 void Compile::print_inlining_reinit() {
4070   if (print_inlining() || print_intrinsics()) {
4071     // Re allocate buffer when we change ResourceMark
4072     _print_inlining_stream = new stringStream();
4073   }
4074 }
4075 
4076 void Compile::print_inlining_reset() {
4077   _print_inlining_stream-&gt;reset();
4078 }
4079 
4080 void Compile::print_inlining_commit() {
4081   assert(print_inlining() || print_intrinsics(), "PrintInlining off?");
4082   // Transfer the message from _print_inlining_stream to the current
4083   // _print_inlining_list buffer and clear _print_inlining_stream.
4084   _print_inlining_list-&gt;at(_print_inlining_idx).ss()-&gt;write(_print_inlining_stream-&gt;as_string(), _print_inlining_stream-&gt;size());
4085   print_inlining_reset();
4086 }
4087 
4088 void Compile::print_inlining_push() {
4089   // Add new buffer to the _print_inlining_list at current position
4090   _print_inlining_idx++;
4091   _print_inlining_list-&gt;insert_before(_print_inlining_idx, PrintInliningBuffer());
4092 }
4093 
4094 Compile::PrintInliningBuffer&amp; Compile::print_inlining_current() {
4095   return _print_inlining_list-&gt;at(_print_inlining_idx);
4096 }
4097 
4098 void Compile::print_inlining_update(CallGenerator* cg) {
4099   if (print_inlining() || print_intrinsics()) {
4100     if (!cg-&gt;is_late_inline()) {
4101       if (print_inlining_current().cg() != NULL) {
4102         print_inlining_push();
4103       }
4104       print_inlining_commit();
4105     } else {
4106       if (print_inlining_current().cg() != cg &amp;&amp;
4107           (print_inlining_current().cg() != NULL ||
4108            print_inlining_current().ss()-&gt;size() != 0)) {
4109         print_inlining_push();
4110       }
4111       print_inlining_commit();
4112       print_inlining_current().set_cg(cg);
4113     }
4114   }
4115 }
4116 
4117 void Compile::print_inlining_move_to(CallGenerator* cg) {
4118   // We resume inlining at a late inlining call site. Locate the
4119   // corresponding inlining buffer so that we can update it.
4120   if (print_inlining()) {
4121     for (int i = 0; i &lt; _print_inlining_list-&gt;length(); i++) {
4122       if (_print_inlining_list-&gt;adr_at(i)-&gt;cg() == cg) {
4123         _print_inlining_idx = i;
4124         return;
4125       }
4126     }
4127     ShouldNotReachHere();
4128   }
4129 }
4130 
4131 void Compile::print_inlining_update_delayed(CallGenerator* cg) {
4132   if (print_inlining()) {
4133     assert(_print_inlining_stream-&gt;size() &gt; 0, "missing inlining msg");
4134     assert(print_inlining_current().cg() == cg, "wrong entry");
4135     // replace message with new message
4136     _print_inlining_list-&gt;at_put(_print_inlining_idx, PrintInliningBuffer());
4137     print_inlining_commit();
4138     print_inlining_current().set_cg(cg);
4139   }
4140 }
4141 
4142 void Compile::print_inlining_assert_ready() {
4143   assert(!_print_inlining || _print_inlining_stream-&gt;size() == 0, "loosing data");
4144 }
4145 
4146 void Compile::process_print_inlining() {
4147   bool do_print_inlining = print_inlining() || print_intrinsics();
4148   if (do_print_inlining || log() != NULL) {
4149     // Print inlining message for candidates that we couldn't inline
4150     // for lack of space
4151     for (int i = 0; i &lt; _late_inlines.length(); i++) {
4152       CallGenerator* cg = _late_inlines.at(i);
4153       if (!cg-&gt;is_mh_late_inline()) {
4154         const char* msg = "live nodes &gt; LiveNodeCountInliningCutoff";
4155         if (do_print_inlining) {
4156           cg-&gt;print_inlining_late(msg);
4157         }
4158         log_late_inline_failure(cg, msg);
4159       }
4160     }
4161   }
4162   if (do_print_inlining) {
4163     ResourceMark rm;
4164     stringStream ss;
4165     for (int i = 0; i &lt; _print_inlining_list-&gt;length(); i++) {
4166       ss.print("%s", _print_inlining_list-&gt;adr_at(i)-&gt;ss()-&gt;as_string());
4167     }
4168     size_t end = ss.size();
4169     _print_inlining_output = NEW_ARENA_ARRAY(comp_arena(), char, end+1);
4170     strncpy(_print_inlining_output, ss.base(), end+1);
4171     _print_inlining_output[end] = 0;
4172   }
4173 }
4174 
4175 void Compile::dump_print_inlining() {
4176   if (_print_inlining_output != NULL) {
4177     tty-&gt;print_raw(_print_inlining_output);
4178   }
4179 }
4180 
4181 void Compile::log_late_inline(CallGenerator* cg) {
4182   if (log() != NULL) {
4183     log()-&gt;head("late_inline method='%d'  inline_id='" JLONG_FORMAT "'", log()-&gt;identify(cg-&gt;method()),
4184                 cg-&gt;unique_id());
4185     JVMState* p = cg-&gt;call_node()-&gt;jvms();
4186     while (p != NULL) {
4187       log()-&gt;elem("jvms bci='%d' method='%d'", p-&gt;bci(), log()-&gt;identify(p-&gt;method()));
4188       p = p-&gt;caller();
4189     }
4190     log()-&gt;tail("late_inline");
4191   }
4192 }
4193 
4194 void Compile::log_late_inline_failure(CallGenerator* cg, const char* msg) {
4195   log_late_inline(cg);
4196   if (log() != NULL) {
4197     log()-&gt;inline_fail(msg);
4198   }
4199 }
4200 
4201 void Compile::log_inline_id(CallGenerator* cg) {
4202   if (log() != NULL) {
4203     // The LogCompilation tool needs a unique way to identify late
4204     // inline call sites. This id must be unique for this call site in
4205     // this compilation. Try to have it unique across compilations as
4206     // well because it can be convenient when grepping through the log
4207     // file.
4208     // Distinguish OSR compilations from others in case CICountOSR is
4209     // on.
4210     jlong id = ((jlong)unique()) + (((jlong)compile_id()) &lt;&lt; 33) + (CICountOSR &amp;&amp; is_osr_compilation() ? ((jlong)1) &lt;&lt; 32 : 0);
4211     cg-&gt;set_unique_id(id);
4212     log()-&gt;elem("inline_id id='" JLONG_FORMAT "'", id);
4213   }
4214 }
4215 
4216 void Compile::log_inline_failure(const char* msg) {
4217   if (C-&gt;log() != NULL) {
4218     C-&gt;log()-&gt;inline_fail(msg);
4219   }
4220 }
4221 
4222 
4223 // Dump inlining replay data to the stream.
4224 // Don't change thread state and acquire any locks.
4225 void Compile::dump_inline_data(outputStream* out) {
4226   InlineTree* inl_tree = ilt();
4227   if (inl_tree != NULL) {
4228     //tty-&gt;print("&gt;DUMP_INLINE_DATA FOR:");method()-&gt;print_name(tty);tty-&gt;print("\n");
4229     //tty-&gt;print("&gt;INL_TREE-&gt;COUNT() = %d&lt;\n",inl_tree-&gt;count());
4230     out-&gt;print(" inline %d", inl_tree-&gt;count());
4231     inl_tree-&gt;dump_replay_data(out);
4232   }
4233 }
4234 
4235 int Compile::cmp_expensive_nodes(Node* n1, Node* n2) {
4236   if (n1-&gt;Opcode() &lt; n2-&gt;Opcode())      return -1;
4237   else if (n1-&gt;Opcode() &gt; n2-&gt;Opcode()) return 1;
4238 
4239   assert(n1-&gt;req() == n2-&gt;req(), "can't compare %s nodes: n1-&gt;req() = %d, n2-&gt;req() = %d", NodeClassNames[n1-&gt;Opcode()], n1-&gt;req(), n2-&gt;req());
4240   for (uint i = 1; i &lt; n1-&gt;req(); i++) {
4241     if (n1-&gt;in(i) &lt; n2-&gt;in(i))      return -1;
4242     else if (n1-&gt;in(i) &gt; n2-&gt;in(i)) return 1;
4243   }
4244 
4245   return 0;
4246 }
4247 
4248 int Compile::cmp_expensive_nodes(Node** n1p, Node** n2p) {
4249   Node* n1 = *n1p;
4250   Node* n2 = *n2p;
4251 
4252   return cmp_expensive_nodes(n1, n2);
4253 }
4254 
4255 void Compile::sort_expensive_nodes() {
4256   if (!expensive_nodes_sorted()) {
4257     _expensive_nodes-&gt;sort(cmp_expensive_nodes);
4258   }
4259 }
4260 
4261 bool Compile::expensive_nodes_sorted() const {
4262   for (int i = 1; i &lt; _expensive_nodes-&gt;length(); i++) {
4263     if (cmp_expensive_nodes(_expensive_nodes-&gt;adr_at(i), _expensive_nodes-&gt;adr_at(i-1)) &lt; 0) {
4264       return false;
4265     }
4266   }
4267   return true;
4268 }
4269 
4270 bool Compile::should_optimize_expensive_nodes(PhaseIterGVN &amp;igvn) {
4271   if (_expensive_nodes-&gt;length() == 0) {
4272     return false;
4273   }
4274 
4275   assert(OptimizeExpensiveOps, "optimization off?");
4276 
4277   // Take this opportunity to remove dead nodes from the list
4278   int j = 0;
4279   for (int i = 0; i &lt; _expensive_nodes-&gt;length(); i++) {
4280     Node* n = _expensive_nodes-&gt;at(i);
4281     if (!n-&gt;is_unreachable(igvn)) {
4282       assert(n-&gt;is_expensive(), "should be expensive");
4283       _expensive_nodes-&gt;at_put(j, n);
4284       j++;
4285     }
4286   }
4287   _expensive_nodes-&gt;trunc_to(j);
4288 
4289   // Then sort the list so that similar nodes are next to each other
4290   // and check for at least two nodes of identical kind with same data
4291   // inputs.
4292   sort_expensive_nodes();
4293 
4294   for (int i = 0; i &lt; _expensive_nodes-&gt;length()-1; i++) {
4295     if (cmp_expensive_nodes(_expensive_nodes-&gt;adr_at(i), _expensive_nodes-&gt;adr_at(i+1)) == 0) {
4296       return true;
4297     }
4298   }
4299 
4300   return false;
4301 }
4302 
4303 void Compile::cleanup_expensive_nodes(PhaseIterGVN &amp;igvn) {
4304   if (_expensive_nodes-&gt;length() == 0) {
4305     return;
4306   }
4307 
4308   assert(OptimizeExpensiveOps, "optimization off?");
4309 
4310   // Sort to bring similar nodes next to each other and clear the
4311   // control input of nodes for which there's only a single copy.
4312   sort_expensive_nodes();
4313 
4314   int j = 0;
4315   int identical = 0;
4316   int i = 0;
4317   bool modified = false;
4318   for (; i &lt; _expensive_nodes-&gt;length()-1; i++) {
4319     assert(j &lt;= i, "can't write beyond current index");
4320     if (_expensive_nodes-&gt;at(i)-&gt;Opcode() == _expensive_nodes-&gt;at(i+1)-&gt;Opcode()) {
4321       identical++;
4322       _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4323       continue;
4324     }
4325     if (identical &gt; 0) {
4326       _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4327       identical = 0;
4328     } else {
4329       Node* n = _expensive_nodes-&gt;at(i);
4330       igvn.replace_input_of(n, 0, NULL);
4331       igvn.hash_insert(n);
4332       modified = true;
4333     }
4334   }
4335   if (identical &gt; 0) {
4336     _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4337   } else if (_expensive_nodes-&gt;length() &gt;= 1) {
4338     Node* n = _expensive_nodes-&gt;at(i);
4339     igvn.replace_input_of(n, 0, NULL);
4340     igvn.hash_insert(n);
4341     modified = true;
4342   }
4343   _expensive_nodes-&gt;trunc_to(j);
4344   if (modified) {
4345     igvn.optimize();
4346   }
4347 }
4348 
4349 void Compile::add_expensive_node(Node * n) {
4350   assert(!_expensive_nodes-&gt;contains(n), "duplicate entry in expensive list");
4351   assert(n-&gt;is_expensive(), "expensive nodes with non-null control here only");
4352   assert(!n-&gt;is_CFG() &amp;&amp; !n-&gt;is_Mem(), "no cfg or memory nodes here");
4353   if (OptimizeExpensiveOps) {
4354     _expensive_nodes-&gt;append(n);
4355   } else {
4356     // Clear control input and let IGVN optimize expensive nodes if
4357     // OptimizeExpensiveOps is off.
4358     n-&gt;set_req(0, NULL);
4359   }
4360 }
4361 
4362 /**
4363  * Remove the speculative part of types and clean up the graph
4364  */
4365 void Compile::remove_speculative_types(PhaseIterGVN &amp;igvn) {
4366   if (UseTypeSpeculation) {
4367     Unique_Node_List worklist;
4368     worklist.push(root());
4369     int modified = 0;
4370     // Go over all type nodes that carry a speculative type, drop the
4371     // speculative part of the type and enqueue the node for an igvn
4372     // which may optimize it out.
4373     for (uint next = 0; next &lt; worklist.size(); ++next) {
4374       Node *n  = worklist.at(next);
4375       if (n-&gt;is_Type()) {
4376         TypeNode* tn = n-&gt;as_Type();
4377         const Type* t = tn-&gt;type();
4378         const Type* t_no_spec = t-&gt;remove_speculative();
4379         if (t_no_spec != t) {
4380           bool in_hash = igvn.hash_delete(n);
4381           assert(in_hash, "node should be in igvn hash table");
4382           tn-&gt;set_type(t_no_spec);
4383           igvn.hash_insert(n);
4384           igvn._worklist.push(n); // give it a chance to go away
4385           modified++;
4386         }
4387       }
4388       uint max = n-&gt;len();
4389       for( uint i = 0; i &lt; max; ++i ) {
4390         Node *m = n-&gt;in(i);
4391         if (not_a_node(m))  continue;
4392         worklist.push(m);
4393       }
4394     }
4395     // Drop the speculative part of all types in the igvn's type table
4396     igvn.remove_speculative_types();
4397     if (modified &gt; 0) {
4398       igvn.optimize();
4399     }
4400 #ifdef ASSERT
4401     // Verify that after the IGVN is over no speculative type has resurfaced
4402     worklist.clear();
4403     worklist.push(root());
4404     for (uint next = 0; next &lt; worklist.size(); ++next) {
4405       Node *n  = worklist.at(next);
4406       const Type* t = igvn.type_or_null(n);
4407       assert((t == NULL) || (t == t-&gt;remove_speculative()), "no more speculative types");
4408       if (n-&gt;is_Type()) {
4409         t = n-&gt;as_Type()-&gt;type();
4410         assert(t == t-&gt;remove_speculative(), "no more speculative types");
4411       }
4412       uint max = n-&gt;len();
4413       for( uint i = 0; i &lt; max; ++i ) {
4414         Node *m = n-&gt;in(i);
4415         if (not_a_node(m))  continue;
4416         worklist.push(m);
4417       }
4418     }
4419     igvn.check_no_speculative_types();
4420 #endif
4421   }
4422 }
4423 
4424 // Auxiliary method to support randomized stressing/fuzzing.
4425 //
4426 // This method can be called the arbitrary number of times, with current count
4427 // as the argument. The logic allows selecting a single candidate from the
4428 // running list of candidates as follows:
4429 //    int count = 0;
4430 //    Cand* selected = null;
4431 //    while(cand = cand-&gt;next()) {
4432 //      if (randomized_select(++count)) {
4433 //        selected = cand;
4434 //      }
4435 //    }
4436 //
4437 // Including count equalizes the chances any candidate is "selected".
4438 // This is useful when we don't have the complete list of candidates to choose
4439 // from uniformly. In this case, we need to adjust the randomicity of the
4440 // selection, or else we will end up biasing the selection towards the latter
4441 // candidates.
4442 //
4443 // Quick back-envelope calculation shows that for the list of n candidates
4444 // the equal probability for the candidate to persist as "best" can be
4445 // achieved by replacing it with "next" k-th candidate with the probability
4446 // of 1/k. It can be easily shown that by the end of the run, the
4447 // probability for any candidate is converged to 1/n, thus giving the
4448 // uniform distribution among all the candidates.
4449 //
4450 // We don't care about the domain size as long as (RANDOMIZED_DOMAIN / count) is large.
4451 #define RANDOMIZED_DOMAIN_POW 29
4452 #define RANDOMIZED_DOMAIN (1 &lt;&lt; RANDOMIZED_DOMAIN_POW)
4453 #define RANDOMIZED_DOMAIN_MASK ((1 &lt;&lt; (RANDOMIZED_DOMAIN_POW + 1)) - 1)
4454 bool Compile::randomized_select(int count) {
4455   assert(count &gt; 0, "only positive");
4456   return (os::random() &amp; RANDOMIZED_DOMAIN_MASK) &lt; (RANDOMIZED_DOMAIN / count);
4457 }
4458 
4459 CloneMap&amp;     Compile::clone_map()                 { return _clone_map; }
4460 void          Compile::set_clone_map(Dict* d)      { _clone_map._dict = d; }
4461 
4462 void NodeCloneInfo::dump() const {
4463   tty-&gt;print(" {%d:%d} ", idx(), gen());
4464 }
4465 
4466 void CloneMap::clone(Node* old, Node* nnn, int gen) {
4467   uint64_t val = value(old-&gt;_idx);
4468   NodeCloneInfo cio(val);
4469   assert(val != 0, "old node should be in the map");
4470   NodeCloneInfo cin(cio.idx(), gen + cio.gen());
4471   insert(nnn-&gt;_idx, cin.get());
4472 #ifndef PRODUCT
4473   if (is_debug()) {
4474     tty-&gt;print_cr("CloneMap::clone inserted node %d info {%d:%d} into CloneMap", nnn-&gt;_idx, cin.idx(), cin.gen());
4475   }
4476 #endif
4477 }
4478 
4479 void CloneMap::verify_insert_and_clone(Node* old, Node* nnn, int gen) {
4480   NodeCloneInfo cio(value(old-&gt;_idx));
4481   if (cio.get() == 0) {
4482     cio.set(old-&gt;_idx, 0);
4483     insert(old-&gt;_idx, cio.get());
4484 #ifndef PRODUCT
4485     if (is_debug()) {
4486       tty-&gt;print_cr("CloneMap::verify_insert_and_clone inserted node %d info {%d:%d} into CloneMap", old-&gt;_idx, cio.idx(), cio.gen());
4487     }
4488 #endif
4489   }
4490   clone(old, nnn, gen);
4491 }
4492 
4493 int CloneMap::max_gen() const {
4494   int g = 0;
4495   DictI di(_dict);
4496   for(; di.test(); ++di) {
4497     int t = gen(di._key);
4498     if (g &lt; t) {
4499       g = t;
4500 #ifndef PRODUCT
4501       if (is_debug()) {
4502         tty-&gt;print_cr("CloneMap::max_gen() update max=%d from %d", g, _2_node_idx_t(di._key));
4503       }
4504 #endif
4505     }
4506   }
4507   return g;
4508 }
4509 
4510 void CloneMap::dump(node_idx_t key) const {
4511   uint64_t val = value(key);
4512   if (val != 0) {
4513     NodeCloneInfo ni(val);
4514     ni.dump();
4515   }
4516 }
</pre></body></html>
