<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>Old src/share/vm/opto/compile.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1997, 2015, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/macroAssembler.hpp"
  27 #include "asm/macroAssembler.inline.hpp"
  28 #include "ci/ciReplay.hpp"
  29 #include "classfile/systemDictionary.hpp"
  30 #include "code/exceptionHandlerTable.hpp"
  31 #include "code/nmethod.hpp"
  32 #include "compiler/compileBroker.hpp"
  33 #include "compiler/compileLog.hpp"
  34 #include "compiler/disassembler.hpp"
  35 #include "compiler/oopMap.hpp"
  36 #include "opto/addnode.hpp"
  37 #include "opto/block.hpp"
  38 #include "opto/c2compiler.hpp"
  39 #include "opto/callGenerator.hpp"
  40 #include "opto/callnode.hpp"
  41 #include "opto/castnode.hpp"
  42 #include "opto/cfgnode.hpp"
  43 #include "opto/chaitin.hpp"
  44 #include "opto/compile.hpp"
  45 #include "opto/connode.hpp"
  46 #include "opto/convertnode.hpp"
  47 #include "opto/divnode.hpp"
  48 #include "opto/escape.hpp"
  49 #include "opto/idealGraphPrinter.hpp"
  50 #include "opto/loopnode.hpp"
  51 #include "opto/machnode.hpp"
  52 #include "opto/macro.hpp"
  53 #include "opto/matcher.hpp"
  54 #include "opto/mathexactnode.hpp"
  55 #include "opto/memnode.hpp"
  56 #include "opto/mulnode.hpp"
  57 #include "opto/narrowptrnode.hpp"
  58 #include "opto/node.hpp"
  59 #include "opto/opcodes.hpp"
  60 #include "opto/output.hpp"
  61 #include "opto/parse.hpp"
  62 #include "opto/phaseX.hpp"
  63 #include "opto/rootnode.hpp"
  64 #include "opto/runtime.hpp"
  65 #include "opto/stringopts.hpp"
  66 #include "opto/type.hpp"
  67 #include "opto/vectornode.hpp"
  68 #include "runtime/arguments.hpp"
  69 #include "runtime/sharedRuntime.hpp"
  70 #include "runtime/signature.hpp"
  71 #include "runtime/stubRoutines.hpp"
  72 #include "runtime/timer.hpp"
  73 #include "utilities/copy.hpp"
  74 
  75 
  76 // -------------------- Compile::mach_constant_base_node -----------------------
  77 // Constant table base node singleton.
  78 MachConstantBaseNode* Compile::mach_constant_base_node() {
  79   if (_mach_constant_base_node == NULL) {
  80     _mach_constant_base_node = new MachConstantBaseNode();
  81     _mach_constant_base_node-&gt;add_req(C-&gt;root());
  82   }
  83   return _mach_constant_base_node;
  84 }
  85 
  86 
  87 /// Support for intrinsics.
  88 
  89 // Return the index at which m must be inserted (or already exists).
  90 // The sort order is by the address of the ciMethod, with is_virtual as minor key.
  91 class IntrinsicDescPair {
  92  private:
  93   ciMethod* _m;
  94   bool _is_virtual;
  95  public:
  96   IntrinsicDescPair(ciMethod* m, bool is_virtual) : _m(m), _is_virtual(is_virtual) {}
  97   static int compare(IntrinsicDescPair* const&amp; key, CallGenerator* const&amp; elt) {
  98     ciMethod* m= elt-&gt;method();
  99     ciMethod* key_m = key-&gt;_m;
 100     if (key_m &lt; m)      return -1;
 101     else if (key_m &gt; m) return 1;
 102     else {
 103       bool is_virtual = elt-&gt;is_virtual();
 104       bool key_virtual = key-&gt;_is_virtual;
 105       if (key_virtual &lt; is_virtual)      return -1;
 106       else if (key_virtual &gt; is_virtual) return 1;
 107       else                               return 0;
 108     }
 109   }
 110 };
 111 int Compile::intrinsic_insertion_index(ciMethod* m, bool is_virtual, bool&amp; found) {
 112 #ifdef ASSERT
 113   for (int i = 1; i &lt; _intrinsics-&gt;length(); i++) {
 114     CallGenerator* cg1 = _intrinsics-&gt;at(i-1);
 115     CallGenerator* cg2 = _intrinsics-&gt;at(i);
 116     assert(cg1-&gt;method() != cg2-&gt;method()
 117            ? cg1-&gt;method()     &lt; cg2-&gt;method()
 118            : cg1-&gt;is_virtual() &lt; cg2-&gt;is_virtual(),
 119            "compiler intrinsics list must stay sorted");
 120   }
 121 #endif
 122   IntrinsicDescPair pair(m, is_virtual);
 123   return _intrinsics-&gt;find_sorted&lt;IntrinsicDescPair*, IntrinsicDescPair::compare&gt;(&amp;pair, found);
 124 }
 125 
 126 void Compile::register_intrinsic(CallGenerator* cg) {
 127   if (_intrinsics == NULL) {
 128     _intrinsics = new (comp_arena())GrowableArray&lt;CallGenerator*&gt;(comp_arena(), 60, 0, NULL);
 129   }
 130   int len = _intrinsics-&gt;length();
 131   bool found = false;
 132   int index = intrinsic_insertion_index(cg-&gt;method(), cg-&gt;is_virtual(), found);
 133   assert(!found, "registering twice");
 134   _intrinsics-&gt;insert_before(index, cg);
 135   assert(find_intrinsic(cg-&gt;method(), cg-&gt;is_virtual()) == cg, "registration worked");
 136 }
 137 
 138 CallGenerator* Compile::find_intrinsic(ciMethod* m, bool is_virtual) {
 139   assert(m-&gt;is_loaded(), "don't try this on unloaded methods");
 140   if (_intrinsics != NULL) {
 141     bool found = false;
 142     int index = intrinsic_insertion_index(m, is_virtual, found);
 143      if (found) {
 144       return _intrinsics-&gt;at(index);
 145     }
 146   }
 147   // Lazily create intrinsics for intrinsic IDs well-known in the runtime.
 148   if (m-&gt;intrinsic_id() != vmIntrinsics::_none &amp;&amp;
 149       m-&gt;intrinsic_id() &lt;= vmIntrinsics::LAST_COMPILER_INLINE) {
 150     CallGenerator* cg = make_vm_intrinsic(m, is_virtual);
 151     if (cg != NULL) {
 152       // Save it for next time:
 153       register_intrinsic(cg);
 154       return cg;
 155     } else {
 156       gather_intrinsic_statistics(m-&gt;intrinsic_id(), is_virtual, _intrinsic_disabled);
 157     }
 158   }
 159   return NULL;
 160 }
 161 
 162 // Compile:: register_library_intrinsics and make_vm_intrinsic are defined
 163 // in library_call.cpp.
 164 
 165 
 166 #ifndef PRODUCT
 167 // statistics gathering...
 168 
 169 juint  Compile::_intrinsic_hist_count[vmIntrinsics::ID_LIMIT] = {0};
 170 jubyte Compile::_intrinsic_hist_flags[vmIntrinsics::ID_LIMIT] = {0};
 171 
 172 bool Compile::gather_intrinsic_statistics(vmIntrinsics::ID id, bool is_virtual, int flags) {
 173   assert(id &gt; vmIntrinsics::_none &amp;&amp; id &lt; vmIntrinsics::ID_LIMIT, "oob");
 174   int oflags = _intrinsic_hist_flags[id];
 175   assert(flags != 0, "what happened?");
 176   if (is_virtual) {
 177     flags |= _intrinsic_virtual;
 178   }
 179   bool changed = (flags != oflags);
 180   if ((flags &amp; _intrinsic_worked) != 0) {
 181     juint count = (_intrinsic_hist_count[id] += 1);
 182     if (count == 1) {
 183       changed = true;           // first time
 184     }
 185     // increment the overall count also:
 186     _intrinsic_hist_count[vmIntrinsics::_none] += 1;
 187   }
 188   if (changed) {
 189     if (((oflags ^ flags) &amp; _intrinsic_virtual) != 0) {
 190       // Something changed about the intrinsic's virtuality.
 191       if ((flags &amp; _intrinsic_virtual) != 0) {
 192         // This is the first use of this intrinsic as a virtual call.
 193         if (oflags != 0) {
 194           // We already saw it as a non-virtual, so note both cases.
 195           flags |= _intrinsic_both;
 196         }
 197       } else if ((oflags &amp; _intrinsic_both) == 0) {
 198         // This is the first use of this intrinsic as a non-virtual
 199         flags |= _intrinsic_both;
 200       }
 201     }
 202     _intrinsic_hist_flags[id] = (jubyte) (oflags | flags);
 203   }
 204   // update the overall flags also:
 205   _intrinsic_hist_flags[vmIntrinsics::_none] |= (jubyte) flags;
 206   return changed;
 207 }
 208 
 209 static char* format_flags(int flags, char* buf) {
 210   buf[0] = 0;
 211   if ((flags &amp; Compile::_intrinsic_worked) != 0)    strcat(buf, ",worked");
 212   if ((flags &amp; Compile::_intrinsic_failed) != 0)    strcat(buf, ",failed");
 213   if ((flags &amp; Compile::_intrinsic_disabled) != 0)  strcat(buf, ",disabled");
 214   if ((flags &amp; Compile::_intrinsic_virtual) != 0)   strcat(buf, ",virtual");
 215   if ((flags &amp; Compile::_intrinsic_both) != 0)      strcat(buf, ",nonvirtual");
 216   if (buf[0] == 0)  strcat(buf, ",");
 217   assert(buf[0] == ',', "must be");
 218   return &amp;buf[1];
 219 }
 220 
 221 void Compile::print_intrinsic_statistics() {
 222   char flagsbuf[100];
 223   ttyLocker ttyl;
 224   if (xtty != NULL)  xtty-&gt;head("statistics type='intrinsic'");
 225   tty-&gt;print_cr("Compiler intrinsic usage:");
 226   juint total = _intrinsic_hist_count[vmIntrinsics::_none];
 227   if (total == 0)  total = 1;  // avoid div0 in case of no successes
 228   #define PRINT_STAT_LINE(name, c, f) \
 229     tty-&gt;print_cr("  %4d (%4.1f%%) %s (%s)", (int)(c), ((c) * 100.0) / total, name, f);
 230   for (int index = 1 + (int)vmIntrinsics::_none; index &lt; (int)vmIntrinsics::ID_LIMIT; index++) {
 231     vmIntrinsics::ID id = (vmIntrinsics::ID) index;
 232     int   flags = _intrinsic_hist_flags[id];
 233     juint count = _intrinsic_hist_count[id];
 234     if ((flags | count) != 0) {
 235       PRINT_STAT_LINE(vmIntrinsics::name_at(id), count, format_flags(flags, flagsbuf));
 236     }
 237   }
 238   PRINT_STAT_LINE("total", total, format_flags(_intrinsic_hist_flags[vmIntrinsics::_none], flagsbuf));
 239   if (xtty != NULL)  xtty-&gt;tail("statistics");
 240 }
 241 
 242 void Compile::print_statistics() {
 243   { ttyLocker ttyl;
 244     if (xtty != NULL)  xtty-&gt;head("statistics type='opto'");
 245     Parse::print_statistics();
 246     PhaseCCP::print_statistics();
 247     PhaseRegAlloc::print_statistics();
 248     Scheduling::print_statistics();
 249     PhasePeephole::print_statistics();
 250     PhaseIdealLoop::print_statistics();
 251     if (xtty != NULL)  xtty-&gt;tail("statistics");
 252   }
 253   if (_intrinsic_hist_flags[vmIntrinsics::_none] != 0) {
 254     // put this under its own &lt;statistics&gt; element.
 255     print_intrinsic_statistics();
 256   }
 257 }
 258 #endif //PRODUCT
 259 
 260 // Support for bundling info
 261 Bundle* Compile::node_bundling(const Node *n) {
 262   assert(valid_bundle_info(n), "oob");
 263   return &amp;_node_bundling_base[n-&gt;_idx];
 264 }
 265 
 266 bool Compile::valid_bundle_info(const Node *n) {
 267   return (_node_bundling_limit &gt; n-&gt;_idx);
 268 }
 269 
 270 
 271 void Compile::gvn_replace_by(Node* n, Node* nn) {
 272   for (DUIterator_Last imin, i = n-&gt;last_outs(imin); i &gt;= imin; ) {
 273     Node* use = n-&gt;last_out(i);
 274     bool is_in_table = initial_gvn()-&gt;hash_delete(use);
 275     uint uses_found = 0;
 276     for (uint j = 0; j &lt; use-&gt;len(); j++) {
 277       if (use-&gt;in(j) == n) {
 278         if (j &lt; use-&gt;req())
 279           use-&gt;set_req(j, nn);
 280         else
 281           use-&gt;set_prec(j, nn);
 282         uses_found++;
 283       }
 284     }
 285     if (is_in_table) {
 286       // reinsert into table
 287       initial_gvn()-&gt;hash_find_insert(use);
 288     }
 289     record_for_igvn(use);
 290     i -= uses_found;    // we deleted 1 or more copies of this edge
 291   }
 292 }
 293 
 294 
 295 static inline bool not_a_node(const Node* n) {
 296   if (n == NULL)                   return true;
 297   if (((intptr_t)n &amp; 1) != 0)      return true;  // uninitialized, etc.
 298   if (*(address*)n == badAddress)  return true;  // kill by Node::destruct
 299   return false;
 300 }
 301 
 302 // Identify all nodes that are reachable from below, useful.
 303 // Use breadth-first pass that records state in a Unique_Node_List,
 304 // recursive traversal is slower.
 305 void Compile::identify_useful_nodes(Unique_Node_List &amp;useful) {
 306   int estimated_worklist_size = live_nodes();
 307   useful.map( estimated_worklist_size, NULL );  // preallocate space
 308 
 309   // Initialize worklist
 310   if (root() != NULL)     { useful.push(root()); }
 311   // If 'top' is cached, declare it useful to preserve cached node
 312   if( cached_top_node() ) { useful.push(cached_top_node()); }
 313 
 314   // Push all useful nodes onto the list, breadthfirst
 315   for( uint next = 0; next &lt; useful.size(); ++next ) {
 316     assert( next &lt; unique(), "Unique useful nodes &lt; total nodes");
 317     Node *n  = useful.at(next);
 318     uint max = n-&gt;len();
 319     for( uint i = 0; i &lt; max; ++i ) {
 320       Node *m = n-&gt;in(i);
 321       if (not_a_node(m))  continue;
 322       useful.push(m);
 323     }
 324   }
 325 }
 326 
 327 // Update dead_node_list with any missing dead nodes using useful
 328 // list. Consider all non-useful nodes to be useless i.e., dead nodes.
 329 void Compile::update_dead_node_list(Unique_Node_List &amp;useful) {
 330   uint max_idx = unique();
 331   VectorSet&amp; useful_node_set = useful.member_set();
 332 
 333   for (uint node_idx = 0; node_idx &lt; max_idx; node_idx++) {
 334     // If node with index node_idx is not in useful set,
 335     // mark it as dead in dead node list.
 336     if (! useful_node_set.test(node_idx) ) {
 337       record_dead_node(node_idx);
 338     }
 339   }
 340 }
 341 
 342 void Compile::remove_useless_late_inlines(GrowableArray&lt;CallGenerator*&gt;* inlines, Unique_Node_List &amp;useful) {
 343   int shift = 0;
 344   for (int i = 0; i &lt; inlines-&gt;length(); i++) {
 345     CallGenerator* cg = inlines-&gt;at(i);
 346     CallNode* call = cg-&gt;call_node();
 347     if (shift &gt; 0) {
 348       inlines-&gt;at_put(i-shift, cg);
 349     }
 350     if (!useful.member(call)) {
 351       shift++;
 352     }
 353   }
 354   inlines-&gt;trunc_to(inlines-&gt;length()-shift);
 355 }
 356 
 357 // Disconnect all useless nodes by disconnecting those at the boundary.
 358 void Compile::remove_useless_nodes(Unique_Node_List &amp;useful) {
 359   uint next = 0;
 360   while (next &lt; useful.size()) {
 361     Node *n = useful.at(next++);
 362     if (n-&gt;is_SafePoint()) {
 363       // We're done with a parsing phase. Replaced nodes are not valid
 364       // beyond that point.
 365       n-&gt;as_SafePoint()-&gt;delete_replaced_nodes();
 366     }
 367     // Use raw traversal of out edges since this code removes out edges
 368     int max = n-&gt;outcnt();
 369     for (int j = 0; j &lt; max; ++j) {
 370       Node* child = n-&gt;raw_out(j);
 371       if (! useful.member(child)) {
 372         assert(!child-&gt;is_top() || child != top(),
 373                "If top is cached in Compile object it is in useful list");
 374         // Only need to remove this out-edge to the useless node
 375         n-&gt;raw_del_out(j);
 376         --j;
 377         --max;
 378       }
 379     }
 380     if (n-&gt;outcnt() == 1 &amp;&amp; n-&gt;has_special_unique_user()) {
 381       record_for_igvn(n-&gt;unique_out());
 382     }
 383   }
 384   // Remove useless macro and predicate opaq nodes
 385   for (int i = C-&gt;macro_count()-1; i &gt;= 0; i--) {
 386     Node* n = C-&gt;macro_node(i);
 387     if (!useful.member(n)) {
 388       remove_macro_node(n);
 389     }
 390   }
 391   // Remove useless CastII nodes with range check dependency
 392   for (int i = range_check_cast_count() - 1; i &gt;= 0; i--) {
 393     Node* cast = range_check_cast_node(i);
 394     if (!useful.member(cast)) {
 395       remove_range_check_cast(cast);
 396     }
 397   }
 398   // Remove useless expensive node
 399   for (int i = C-&gt;expensive_count()-1; i &gt;= 0; i--) {
 400     Node* n = C-&gt;expensive_node(i);
 401     if (!useful.member(n)) {
 402       remove_expensive_node(n);
 403     }
 404   }
 405   // clean up the late inline lists
 406   remove_useless_late_inlines(&amp;_string_late_inlines, useful);
 407   remove_useless_late_inlines(&amp;_boxing_late_inlines, useful);
 408   remove_useless_late_inlines(&amp;_late_inlines, useful);
 409   debug_only(verify_graph_edges(true/*check for no_dead_code*/);)
 410 }
 411 
 412 //------------------------------frame_size_in_words-----------------------------
 413 // frame_slots in units of words
 414 int Compile::frame_size_in_words() const {
 415   // shift is 0 in LP32 and 1 in LP64
 416   const int shift = (LogBytesPerWord - LogBytesPerInt);
 417   int words = _frame_slots &gt;&gt; shift;
 418   assert( words &lt;&lt; shift == _frame_slots, "frame size must be properly aligned in LP64" );
 419   return words;
 420 }
 421 
 422 // To bang the stack of this compiled method we use the stack size
 423 // that the interpreter would need in case of a deoptimization. This
 424 // removes the need to bang the stack in the deoptimization blob which
 425 // in turn simplifies stack overflow handling.
 426 int Compile::bang_size_in_bytes() const {
 427   return MAX2(frame_size_in_bytes() + os::extra_bang_size_in_bytes(), _interpreter_frame_size);
 428 }
 429 
 430 // ============================================================================
 431 //------------------------------CompileWrapper---------------------------------
 432 class CompileWrapper : public StackObj {
 433   Compile *const _compile;
 434  public:
 435   CompileWrapper(Compile* compile);
 436 
 437   ~CompileWrapper();
 438 };
 439 
 440 CompileWrapper::CompileWrapper(Compile* compile) : _compile(compile) {
 441   // the Compile* pointer is stored in the current ciEnv:
 442   ciEnv* env = compile-&gt;env();
 443   assert(env == ciEnv::current(), "must already be a ciEnv active");
 444   assert(env-&gt;compiler_data() == NULL, "compile already active?");
 445   env-&gt;set_compiler_data(compile);
 446   assert(compile == Compile::current(), "sanity");
 447 
 448   compile-&gt;set_type_dict(NULL);
 449   compile-&gt;set_clone_map(new Dict(cmpkey, hashkey, _compile-&gt;comp_arena()));
 450   compile-&gt;clone_map().set_clone_idx(0);
 451   compile-&gt;set_type_hwm(NULL);
 452   compile-&gt;set_type_last_size(0);
 453   compile-&gt;set_last_tf(NULL, NULL);
 454   compile-&gt;set_indexSet_arena(NULL);
 455   compile-&gt;set_indexSet_free_block_list(NULL);
 456   compile-&gt;init_type_arena();
 457   Type::Initialize(compile);
 458   _compile-&gt;set_scratch_buffer_blob(NULL);
 459   _compile-&gt;begin_method();
 460   _compile-&gt;clone_map().set_debug(_compile-&gt;has_method() &amp;&amp; _compile-&gt;directive()-&gt;CloneMapDebugOption);
 461 }
 462 CompileWrapper::~CompileWrapper() {
 463   _compile-&gt;end_method();
 464   if (_compile-&gt;scratch_buffer_blob() != NULL)
 465     BufferBlob::free(_compile-&gt;scratch_buffer_blob());
 466   _compile-&gt;env()-&gt;set_compiler_data(NULL);
 467 }
 468 
 469 
 470 //----------------------------print_compile_messages---------------------------
 471 void Compile::print_compile_messages() {
 472 #ifndef PRODUCT
 473   // Check if recompiling
 474   if (_subsume_loads == false &amp;&amp; PrintOpto) {
 475     // Recompiling without allowing machine instructions to subsume loads
 476     tty-&gt;print_cr("*********************************************************");
 477     tty-&gt;print_cr("** Bailout: Recompile without subsuming loads          **");
 478     tty-&gt;print_cr("*********************************************************");
 479   }
 480   if (_do_escape_analysis != DoEscapeAnalysis &amp;&amp; PrintOpto) {
 481     // Recompiling without escape analysis
 482     tty-&gt;print_cr("*********************************************************");
 483     tty-&gt;print_cr("** Bailout: Recompile without escape analysis          **");
 484     tty-&gt;print_cr("*********************************************************");
 485   }
 486   if (_eliminate_boxing != EliminateAutoBox &amp;&amp; PrintOpto) {
 487     // Recompiling without boxing elimination
 488     tty-&gt;print_cr("*********************************************************");
 489     tty-&gt;print_cr("** Bailout: Recompile without boxing elimination       **");
 490     tty-&gt;print_cr("*********************************************************");
 491   }
 492   if (C-&gt;directive()-&gt;BreakAtCompileOption) {
 493     // Open the debugger when compiling this method.
 494     tty-&gt;print("### Breaking when compiling: ");
 495     method()-&gt;print_short_name();
 496     tty-&gt;cr();
 497     BREAKPOINT;
 498   }
 499 
 500   if( PrintOpto ) {
 501     if (is_osr_compilation()) {
 502       tty-&gt;print("[OSR]%3d", _compile_id);
 503     } else {
 504       tty-&gt;print("%3d", _compile_id);
 505     }
 506   }
 507 #endif
 508 }
 509 
 510 
 511 //-----------------------init_scratch_buffer_blob------------------------------
 512 // Construct a temporary BufferBlob and cache it for this compile.
 513 void Compile::init_scratch_buffer_blob(int const_size) {
 514   // If there is already a scratch buffer blob allocated and the
 515   // constant section is big enough, use it.  Otherwise free the
 516   // current and allocate a new one.
 517   BufferBlob* blob = scratch_buffer_blob();
 518   if ((blob != NULL) &amp;&amp; (const_size &lt;= _scratch_const_size)) {
 519     // Use the current blob.
 520   } else {
 521     if (blob != NULL) {
 522       BufferBlob::free(blob);
 523     }
 524 
 525     ResourceMark rm;
 526     _scratch_const_size = const_size;
 527     int size = (MAX_inst_size + MAX_stubs_size + _scratch_const_size);
 528     blob = BufferBlob::create("Compile::scratch_buffer", size);
 529     // Record the buffer blob for next time.
 530     set_scratch_buffer_blob(blob);
 531     // Have we run out of code space?
 532     if (scratch_buffer_blob() == NULL) {
 533       // Let CompilerBroker disable further compilations.
 534       record_failure("Not enough space for scratch buffer in CodeCache");
 535       return;
 536     }
 537   }
 538 
 539   // Initialize the relocation buffers
 540   relocInfo* locs_buf = (relocInfo*) blob-&gt;content_end() - MAX_locs_size;
 541   set_scratch_locs_memory(locs_buf);
 542 }
 543 
 544 
 545 //-----------------------scratch_emit_size-------------------------------------
 546 // Helper function that computes size by emitting code
 547 uint Compile::scratch_emit_size(const Node* n) {
 548   // Start scratch_emit_size section.
 549   set_in_scratch_emit_size(true);
 550 
 551   // Emit into a trash buffer and count bytes emitted.
 552   // This is a pretty expensive way to compute a size,
 553   // but it works well enough if seldom used.
 554   // All common fixed-size instructions are given a size
 555   // method by the AD file.
 556   // Note that the scratch buffer blob and locs memory are
 557   // allocated at the beginning of the compile task, and
 558   // may be shared by several calls to scratch_emit_size.
 559   // The allocation of the scratch buffer blob is particularly
 560   // expensive, since it has to grab the code cache lock.
 561   BufferBlob* blob = this-&gt;scratch_buffer_blob();
 562   assert(blob != NULL, "Initialize BufferBlob at start");
 563   assert(blob-&gt;size() &gt; MAX_inst_size, "sanity");
 564   relocInfo* locs_buf = scratch_locs_memory();
 565   address blob_begin = blob-&gt;content_begin();
 566   address blob_end   = (address)locs_buf;
 567   assert(blob-&gt;content_contains(blob_end), "sanity");
 568   CodeBuffer buf(blob_begin, blob_end - blob_begin);
 569   buf.initialize_consts_size(_scratch_const_size);
 570   buf.initialize_stubs_size(MAX_stubs_size);
 571   assert(locs_buf != NULL, "sanity");
 572   int lsize = MAX_locs_size / 3;
 573   buf.consts()-&gt;initialize_shared_locs(&amp;locs_buf[lsize * 0], lsize);
 574   buf.insts()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 1], lsize);
 575   buf.stubs()-&gt;initialize_shared_locs( &amp;locs_buf[lsize * 2], lsize);
 576 
 577   // Do the emission.
 578 
 579   Label fakeL; // Fake label for branch instructions.
 580   Label*   saveL = NULL;
 581   uint save_bnum = 0;
 582   bool is_branch = n-&gt;is_MachBranch();
 583   if (is_branch) {
 584     MacroAssembler masm(&amp;buf);
 585     masm.bind(fakeL);
 586     n-&gt;as_MachBranch()-&gt;save_label(&amp;saveL, &amp;save_bnum);
 587     n-&gt;as_MachBranch()-&gt;label_set(&amp;fakeL, 0);
 588   }
 589   n-&gt;emit(buf, this-&gt;regalloc());
 590 
 591   // Emitting into the scratch buffer should not fail
 592   assert (!failing(), "Must not have pending failure. Reason is: %s", failure_reason());
 593 
 594   if (is_branch) // Restore label.
 595     n-&gt;as_MachBranch()-&gt;label_set(saveL, save_bnum);
 596 
 597   // End scratch_emit_size section.
 598   set_in_scratch_emit_size(false);
 599 
 600   return buf.insts_size();
 601 }
 602 
 603 
 604 // ============================================================================
 605 //------------------------------Compile standard-------------------------------
 606 debug_only( int Compile::_debug_idx = 100000; )
 607 
 608 // Compile a method.  entry_bci is -1 for normal compilations and indicates
 609 // the continuation bci for on stack replacement.
 610 
 611 
 612 Compile::Compile( ciEnv* ci_env, C2Compiler* compiler, ciMethod* target, int osr_bci,
 613                   bool subsume_loads, bool do_escape_analysis, bool eliminate_boxing, DirectiveSet* directive)
 614                 : Phase(Compiler),
 615                   _env(ci_env),
 616                   _directive(directive),
 617                   _log(ci_env-&gt;log()),
 618                   _compile_id(ci_env-&gt;compile_id()),
 619                   _save_argument_registers(false),
 620                   _stub_name(NULL),
 621                   _stub_function(NULL),
 622                   _stub_entry_point(NULL),
 623                   _method(target),
 624                   _entry_bci(osr_bci),
 625                   _initial_gvn(NULL),
 626                   _for_igvn(NULL),
 627                   _warm_calls(NULL),
 628                   _subsume_loads(subsume_loads),
 629                   _do_escape_analysis(do_escape_analysis),
 630                   _eliminate_boxing(eliminate_boxing),
 631                   _failure_reason(NULL),
 632                   _code_buffer("Compile::Fill_buffer"),
 633                   _orig_pc_slot(0),
 634                   _orig_pc_slot_offset_in_bytes(0),
 635                   _has_method_handle_invokes(false),
 636                   _mach_constant_base_node(NULL),
 637                   _node_bundling_limit(0),
 638                   _node_bundling_base(NULL),
 639                   _java_calls(0),
 640                   _inner_loops(0),
 641                   _scratch_const_size(-1),
 642                   _in_scratch_emit_size(false),
 643                   _dead_node_list(comp_arena()),
 644                   _dead_node_count(0),
 645 #ifndef PRODUCT
 646                   _trace_opto_output(directive-&gt;TraceOptoOutputOption),
 647                   _in_dump_cnt(0),
 648                   _printer(IdealGraphPrinter::printer()),
 649 #endif
 650                   _congraph(NULL),
 651                   _comp_arena(mtCompiler),
 652                   _node_arena(mtCompiler),
 653                   _old_arena(mtCompiler),
 654                   _Compile_types(mtCompiler),
 655                   _replay_inline_data(NULL),
 656                   _late_inlines(comp_arena(), 2, 0, NULL),
 657                   _string_late_inlines(comp_arena(), 2, 0, NULL),
 658                   _boxing_late_inlines(comp_arena(), 2, 0, NULL),
 659                   _late_inlines_pos(0),
 660                   _number_of_mh_late_inlines(0),
 661                   _inlining_progress(false),
 662                   _inlining_incrementally(false),
 663                   _print_inlining_list(NULL),
 664                   _print_inlining_stream(NULL),
 665                   _print_inlining_idx(0),
 666                   _print_inlining_output(NULL),
 667                   _interpreter_frame_size(0),
 668                   _max_node_limit(MaxNodeLimit),
 669                   _has_reserved_stack_access(target-&gt;has_reserved_stack_access()) {
 670   C = this;
 671 #ifndef PRODUCT
 672   if (_printer != NULL) {
 673     _printer-&gt;set_compile(this);
 674   }
 675 #endif
 676   CompileWrapper cw(this);
 677 
 678   if (CITimeVerbose) {
 679     tty-&gt;print(" ");
 680     target-&gt;holder()-&gt;name()-&gt;print();
 681     tty-&gt;print(".");
 682     target-&gt;print_short_name();
 683     tty-&gt;print("  ");
 684   }
 685   TraceTime t1("Total compilation time", &amp;_t_totalCompilation, CITime, CITimeVerbose);
 686   TraceTime t2(NULL, &amp;_t_methodCompilation, CITime, false);
 687 
 688 #ifndef PRODUCT
 689   bool print_opto_assembly = directive-&gt;PrintOptoAssemblyOption;
 690   if (!print_opto_assembly) {
 691     bool print_assembly = directive-&gt;PrintAssemblyOption;
 692     if (print_assembly &amp;&amp; !Disassembler::can_decode()) {
 693       tty-&gt;print_cr("PrintAssembly request changed to PrintOptoAssembly");
 694       print_opto_assembly = true;
 695     }
 696   }
 697   set_print_assembly(print_opto_assembly);
 698   set_parsed_irreducible_loop(false);
 699 
 700   if (directive-&gt;ReplayInlineOption) {
 701     _replay_inline_data = ciReplay::load_inline_data(method(), entry_bci(), ci_env-&gt;comp_level());
 702   }
 703 #endif
 704   set_print_inlining(directive-&gt;PrintInliningOption || PrintOptoInlining);
 705   set_print_intrinsics(directive-&gt;PrintIntrinsicsOption);
 706   set_has_irreducible_loop(true); // conservative until build_loop_tree() reset it
 707 
 708   if (ProfileTraps RTM_OPT_ONLY( || UseRTMLocking )) {
 709     // Make sure the method being compiled gets its own MDO,
 710     // so we can at least track the decompile_count().
 711     // Need MDO to record RTM code generation state.
 712     method()-&gt;ensure_method_data();
 713   }
 714 
 715   Init(::AliasLevel);
 716 
 717 
 718   print_compile_messages();
 719 
 720   _ilt = InlineTree::build_inline_tree_root();
 721 
 722   // Even if NO memory addresses are used, MergeMem nodes must have at least 1 slice
 723   assert(num_alias_types() &gt;= AliasIdxRaw, "");
 724 
 725 #define MINIMUM_NODE_HASH  1023
 726   // Node list that Iterative GVN will start with
 727   Unique_Node_List for_igvn(comp_arena());
 728   set_for_igvn(&amp;for_igvn);
 729 
 730   // GVN that will be run immediately on new nodes
 731   uint estimated_size = method()-&gt;code_size()*4+64;
 732   estimated_size = (estimated_size &lt; MINIMUM_NODE_HASH ? MINIMUM_NODE_HASH : estimated_size);
 733   PhaseGVN gvn(node_arena(), estimated_size);
 734   set_initial_gvn(&amp;gvn);
 735 
 736   print_inlining_init();
 737   { // Scope for timing the parser
 738     TracePhase tp("parse", &amp;timers[_t_parser]);
 739 
 740     // Put top into the hash table ASAP.
 741     initial_gvn()-&gt;transform_no_reclaim(top());
 742 
 743     // Set up tf(), start(), and find a CallGenerator.
 744     CallGenerator* cg = NULL;
 745     if (is_osr_compilation()) {
 746       const TypeTuple *domain = StartOSRNode::osr_domain();
 747       const TypeTuple *range = TypeTuple::make_range(method()-&gt;signature());
 748       init_tf(TypeFunc::make(domain, range));
 749       StartNode* s = new StartOSRNode(root(), domain);
 750       initial_gvn()-&gt;set_type_bottom(s);
 751       init_start(s);
 752       cg = CallGenerator::for_osr(method(), entry_bci());
 753     } else {
 754       // Normal case.
 755       init_tf(TypeFunc::make(method()));
 756       StartNode* s = new StartNode(root(), tf()-&gt;domain());
 757       initial_gvn()-&gt;set_type_bottom(s);
 758       init_start(s);
 759       if (method()-&gt;intrinsic_id() == vmIntrinsics::_Reference_get &amp;&amp; UseG1GC) {
 760         // With java.lang.ref.reference.get() we must go through the
 761         // intrinsic when G1 is enabled - even when get() is the root
 762         // method of the compile - so that, if necessary, the value in
 763         // the referent field of the reference object gets recorded by
 764         // the pre-barrier code.
 765         // Specifically, if G1 is enabled, the value in the referent
 766         // field is recorded by the G1 SATB pre barrier. This will
 767         // result in the referent being marked live and the reference
 768         // object removed from the list of discovered references during
 769         // reference processing.
 770         cg = find_intrinsic(method(), false);
 771       }
 772       if (cg == NULL) {
 773         float past_uses = method()-&gt;interpreter_invocation_count();
 774         float expected_uses = past_uses;
 775         cg = CallGenerator::for_inline(method(), expected_uses);
 776       }
 777     }
 778     if (failing())  return;
 779     if (cg == NULL) {
 780       record_method_not_compilable_all_tiers("cannot parse method");
 781       return;
 782     }
 783     JVMState* jvms = build_start_state(start(), tf());
 784     if ((jvms = cg-&gt;generate(jvms)) == NULL) {
 785       if (!failure_reason_is(C2Compiler::retry_class_loading_during_parsing())) {
 786         record_method_not_compilable("method parse failed");
 787       }
 788       return;
 789     }
 790     GraphKit kit(jvms);
 791 
 792     if (!kit.stopped()) {
 793       // Accept return values, and transfer control we know not where.
 794       // This is done by a special, unique ReturnNode bound to root.
 795       return_values(kit.jvms());
 796     }
 797 
 798     if (kit.has_exceptions()) {
 799       // Any exceptions that escape from this call must be rethrown
 800       // to whatever caller is dynamically above us on the stack.
 801       // This is done by a special, unique RethrowNode bound to root.
 802       rethrow_exceptions(kit.transfer_exceptions_into_jvms());
 803     }
 804 
 805     assert(IncrementalInline || (_late_inlines.length() == 0 &amp;&amp; !has_mh_late_inlines()), "incremental inlining is off");
 806 
 807     if (_late_inlines.length() == 0 &amp;&amp; !has_mh_late_inlines() &amp;&amp; !failing() &amp;&amp; has_stringbuilder()) {
 808       inline_string_calls(true);
 809     }
 810 
 811     if (failing())  return;
 812 
 813     print_method(PHASE_BEFORE_REMOVEUSELESS, 3);
 814 
 815     // Remove clutter produced by parsing.
 816     if (!failing()) {
 817       ResourceMark rm;
 818       PhaseRemoveUseless pru(initial_gvn(), &amp;for_igvn);
 819     }
 820   }
 821 
 822   // Note:  Large methods are capped off in do_one_bytecode().
 823   if (failing())  return;
 824 
 825   // After parsing, node notes are no longer automagic.
 826   // They must be propagated by register_new_node_with_optimizer(),
 827   // clone(), or the like.
 828   set_default_node_notes(NULL);
 829 
 830   for (;;) {
 831     int successes = Inline_Warm();
 832     if (failing())  return;
 833     if (successes == 0)  break;
 834   }
 835 
 836   // Drain the list.
 837   Finish_Warm();
 838 #ifndef PRODUCT
 839   if (_printer &amp;&amp; _printer-&gt;should_print(1)) {
 840     _printer-&gt;print_inlining();
 841   }
 842 #endif
 843 
 844   if (failing())  return;
 845   NOT_PRODUCT( verify_graph_edges(); )
 846 
 847   // Now optimize
 848   Optimize();
 849   if (failing())  return;
 850   NOT_PRODUCT( verify_graph_edges(); )
 851 
 852 #ifndef PRODUCT
 853   if (PrintIdeal) {
 854     ttyLocker ttyl;  // keep the following output all in one block
 855     // This output goes directly to the tty, not the compiler log.
 856     // To enable tools to match it up with the compilation activity,
 857     // be sure to tag this tty output with the compile ID.
 858     if (xtty != NULL) {
 859       xtty-&gt;head("ideal compile_id='%d'%s", compile_id(),
 860                  is_osr_compilation()    ? " compile_kind='osr'" :
 861                  "");
 862     }
 863     root()-&gt;dump(9999);
 864     if (xtty != NULL) {
 865       xtty-&gt;tail("ideal");
 866     }
 867   }
 868 #endif
 869 
 870   NOT_PRODUCT( verify_barriers(); )
 871 
 872   // Dump compilation data to replay it.
 873   if (directive-&gt;DumpReplayOption) {
 874     env()-&gt;dump_replay_data(_compile_id);
 875   }
 876   if (directive-&gt;DumpInlineOption &amp;&amp; (ilt() != NULL)) {
 877     env()-&gt;dump_inline_data(_compile_id);
 878   }
 879 
 880   // Now that we know the size of all the monitors we can add a fixed slot
 881   // for the original deopt pc.
 882 
 883   _orig_pc_slot =  fixed_slots();
 884   int next_slot = _orig_pc_slot + (sizeof(address) / VMRegImpl::stack_slot_size);
 885   set_fixed_slots(next_slot);
 886 
 887   // Compute when to use implicit null checks. Used by matching trap based
 888   // nodes and NullCheck optimization.
 889   set_allowed_deopt_reasons();
 890 
 891   // Now generate code
 892   Code_Gen();
 893   if (failing())  return;
 894 
 895   // Check if we want to skip execution of all compiled code.
 896   {
 897 #ifndef PRODUCT
 898     if (OptoNoExecute) {
 899       record_method_not_compilable("+OptoNoExecute");  // Flag as failed
 900       return;
 901     }
 902 #endif
 903     TracePhase tp("install_code", &amp;timers[_t_registerMethod]);
 904 
 905     if (is_osr_compilation()) {
 906       _code_offsets.set_value(CodeOffsets::Verified_Entry, 0);
 907       _code_offsets.set_value(CodeOffsets::OSR_Entry, _first_block_size);
 908     } else {
 909       _code_offsets.set_value(CodeOffsets::Verified_Entry, _first_block_size);
 910       _code_offsets.set_value(CodeOffsets::OSR_Entry, 0);
 911     }
 912 
 913     env()-&gt;register_method(_method, _entry_bci,
 914                            &amp;_code_offsets,
 915                            _orig_pc_slot_offset_in_bytes,
 916                            code_buffer(),
 917                            frame_size_in_words(), _oop_map_set,
 918                            &amp;_handler_table, &amp;_inc_table,
 919                            compiler,
 920                            has_unsafe_access(),
 921                            SharedRuntime::is_wide_vector(max_vector_size()),
 922                            rtm_state()
 923                            );
 924 
 925     if (log() != NULL) // Print code cache state into compiler log
 926       log()-&gt;code_cache_state();
 927   }
 928 }
 929 
 930 //------------------------------Compile----------------------------------------
 931 // Compile a runtime stub
 932 Compile::Compile( ciEnv* ci_env,
 933                   TypeFunc_generator generator,
 934                   address stub_function,
 935                   const char *stub_name,
 936                   int is_fancy_jump,
 937                   bool pass_tls,
 938                   bool save_arg_registers,
 939                   bool return_pc,
 940                   DirectiveSet* directive)
 941   : Phase(Compiler),
 942     _env(ci_env),
 943     _directive(directive),
 944     _log(ci_env-&gt;log()),
 945     _compile_id(0),
 946     _save_argument_registers(save_arg_registers),
 947     _method(NULL),
 948     _stub_name(stub_name),
 949     _stub_function(stub_function),
 950     _stub_entry_point(NULL),
 951     _entry_bci(InvocationEntryBci),
 952     _initial_gvn(NULL),
 953     _for_igvn(NULL),
 954     _warm_calls(NULL),
 955     _orig_pc_slot(0),
 956     _orig_pc_slot_offset_in_bytes(0),
 957     _subsume_loads(true),
 958     _do_escape_analysis(false),
 959     _eliminate_boxing(false),
 960     _failure_reason(NULL),
 961     _code_buffer("Compile::Fill_buffer"),
 962     _has_method_handle_invokes(false),
 963     _mach_constant_base_node(NULL),
 964     _node_bundling_limit(0),
 965     _node_bundling_base(NULL),
 966     _java_calls(0),
 967     _inner_loops(0),
 968 #ifndef PRODUCT
 969     _trace_opto_output(TraceOptoOutput),
 970     _in_dump_cnt(0),
 971     _printer(NULL),
 972 #endif
 973     _comp_arena(mtCompiler),
 974     _node_arena(mtCompiler),
 975     _old_arena(mtCompiler),
 976     _Compile_types(mtCompiler),
 977     _dead_node_list(comp_arena()),
 978     _dead_node_count(0),
 979     _congraph(NULL),
 980     _replay_inline_data(NULL),
 981     _number_of_mh_late_inlines(0),
 982     _inlining_progress(false),
 983     _inlining_incrementally(false),
 984     _print_inlining_list(NULL),
 985     _print_inlining_stream(NULL),
 986     _print_inlining_idx(0),
 987     _print_inlining_output(NULL),
 988     _allowed_reasons(0),
 989     _interpreter_frame_size(0),
 990     _max_node_limit(MaxNodeLimit) {
 991   C = this;
 992 
 993   TraceTime t1(NULL, &amp;_t_totalCompilation, CITime, false);
 994   TraceTime t2(NULL, &amp;_t_stubCompilation, CITime, false);
 995 
 996 #ifndef PRODUCT
 997   set_print_assembly(PrintFrameConverterAssembly);
 998   set_parsed_irreducible_loop(false);
 999 #endif
1000   set_has_irreducible_loop(false); // no loops
1001 
1002   CompileWrapper cw(this);
1003   Init(/*AliasLevel=*/ 0);
1004   init_tf((*generator)());
1005 
1006   {
1007     // The following is a dummy for the sake of GraphKit::gen_stub
1008     Unique_Node_List for_igvn(comp_arena());
1009     set_for_igvn(&amp;for_igvn);  // not used, but some GraphKit guys push on this
1010     PhaseGVN gvn(Thread::current()-&gt;resource_area(),255);
1011     set_initial_gvn(&amp;gvn);    // not significant, but GraphKit guys use it pervasively
1012     gvn.transform_no_reclaim(top());
1013 
1014     GraphKit kit;
1015     kit.gen_stub(stub_function, stub_name, is_fancy_jump, pass_tls, return_pc);
1016   }
1017 
1018   NOT_PRODUCT( verify_graph_edges(); )
1019   Code_Gen();
1020   if (failing())  return;
1021 
1022 
1023   // Entry point will be accessed using compile-&gt;stub_entry_point();
1024   if (code_buffer() == NULL) {
1025     Matcher::soft_match_failure();
1026   } else {
1027     if (PrintAssembly &amp;&amp; (WizardMode || Verbose))
1028       tty-&gt;print_cr("### Stub::%s", stub_name);
1029 
1030     if (!failing()) {
1031       assert(_fixed_slots == 0, "no fixed slots used for runtime stubs");
1032 
1033       // Make the NMethod
1034       // For now we mark the frame as never safe for profile stackwalking
1035       RuntimeStub *rs = RuntimeStub::new_runtime_stub(stub_name,
1036                                                       code_buffer(),
1037                                                       CodeOffsets::frame_never_safe,
1038                                                       // _code_offsets.value(CodeOffsets::Frame_Complete),
1039                                                       frame_size_in_words(),
1040                                                       _oop_map_set,
1041                                                       save_arg_registers);
1042       assert(rs != NULL &amp;&amp; rs-&gt;is_runtime_stub(), "sanity check");
1043 
1044       _stub_entry_point = rs-&gt;entry_point();
1045     }
1046   }
1047 }
1048 
1049 //------------------------------Init-------------------------------------------
1050 // Prepare for a single compilation
1051 void Compile::Init(int aliaslevel) {
1052   _unique  = 0;
1053   _regalloc = NULL;
1054 
1055   _tf      = NULL;  // filled in later
1056   _top     = NULL;  // cached later
1057   _matcher = NULL;  // filled in later
1058   _cfg     = NULL;  // filled in later
1059 
1060   set_24_bit_selection_and_mode(Use24BitFP, false);
1061 
1062   _node_note_array = NULL;
1063   _default_node_notes = NULL;
1064   DEBUG_ONLY( _modified_nodes = NULL; ) // Used in Optimize()
1065 
1066   _immutable_memory = NULL; // filled in at first inquiry
1067 
1068   // Globally visible Nodes
1069   // First set TOP to NULL to give safe behavior during creation of RootNode
1070   set_cached_top_node(NULL);
1071   set_root(new RootNode());
1072   // Now that you have a Root to point to, create the real TOP
1073   set_cached_top_node( new ConNode(Type::TOP) );
1074   set_recent_alloc(NULL, NULL);
1075 
1076   // Create Debug Information Recorder to record scopes, oopmaps, etc.
1077   env()-&gt;set_oop_recorder(new OopRecorder(env()-&gt;arena()));
1078   env()-&gt;set_debug_info(new DebugInformationRecorder(env()-&gt;oop_recorder()));
1079   env()-&gt;set_dependencies(new Dependencies(env()));
1080 
1081   _fixed_slots = 0;
1082   set_has_split_ifs(false);
1083   set_has_loops(has_method() &amp;&amp; method()-&gt;has_loops()); // first approximation
1084   set_has_stringbuilder(false);
1085   set_has_boxed_value(false);
1086   _trap_can_recompile = false;  // no traps emitted yet
1087   _major_progress = true; // start out assuming good things will happen
1088   set_has_unsafe_access(false);
1089   set_max_vector_size(0);
1090   Copy::zero_to_bytes(_trap_hist, sizeof(_trap_hist));
1091   set_decompile_count(0);
1092 
1093   set_do_freq_based_layout(_directive-&gt;BlockLayoutByFrequencyOption);
1094   set_num_loop_opts(LoopOptsCount);
1095   set_do_inlining(Inline);
1096   set_max_inline_size(MaxInlineSize);
1097   set_freq_inline_size(FreqInlineSize);
1098   set_do_scheduling(OptoScheduling);
1099   set_do_count_invocations(false);
1100   set_do_method_data_update(false);
1101 
1102   set_do_vector_loop(false);
1103 
1104   if (AllowVectorizeOnDemand) {
1105     if (has_method() &amp;&amp; (_directive-&gt;VectorizeOption || _directive-&gt;VectorizeDebugOption)) {
1106       set_do_vector_loop(true);
1107       NOT_PRODUCT(if (do_vector_loop() &amp;&amp; Verbose) {tty-&gt;print("Compile::Init: do vectorized loops (SIMD like) for method %s\n",  method()-&gt;name()-&gt;as_quoted_ascii());})
1108     } else if (has_method() &amp;&amp; method()-&gt;name() != 0 &amp;&amp;
1109                method()-&gt;intrinsic_id() == vmIntrinsics::_forEachRemaining) {
1110       set_do_vector_loop(true);
1111     }
1112   }
1113   set_use_cmove(UseCMoveUnconditionally /* || do_vector_loop()*/); //TODO: consider do_vector_loop() mandate use_cmove unconditionally
1114   NOT_PRODUCT(if (use_cmove() &amp;&amp; Verbose &amp;&amp; has_method()) {tty-&gt;print("Compile::Init: use CMove without profitability tests for method %s\n",  method()-&gt;name()-&gt;as_quoted_ascii());})
1115 
1116   set_age_code(has_method() &amp;&amp; method()-&gt;profile_aging());
1117   set_rtm_state(NoRTM); // No RTM lock eliding by default
1118   _max_node_limit = _directive-&gt;MaxNodeLimitOption;
1119 
1120 #if INCLUDE_RTM_OPT
1121   if (UseRTMLocking &amp;&amp; has_method() &amp;&amp; (method()-&gt;method_data_or_null() != NULL)) {
1122     int rtm_state = method()-&gt;method_data()-&gt;rtm_state();
1123     if (method_has_option("NoRTMLockEliding") || ((rtm_state &amp; NoRTM) != 0)) {
1124       // Don't generate RTM lock eliding code.
1125       set_rtm_state(NoRTM);
1126     } else if (method_has_option("UseRTMLockEliding") || ((rtm_state &amp; UseRTM) != 0) || !UseRTMDeopt) {
1127       // Generate RTM lock eliding code without abort ratio calculation code.
1128       set_rtm_state(UseRTM);
1129     } else if (UseRTMDeopt) {
1130       // Generate RTM lock eliding code and include abort ratio calculation
1131       // code if UseRTMDeopt is on.
1132       set_rtm_state(ProfileRTM);
1133     }
1134   }
1135 #endif
1136   if (debug_info()-&gt;recording_non_safepoints()) {
1137     set_node_note_array(new(comp_arena()) GrowableArray&lt;Node_Notes*&gt;
1138                         (comp_arena(), 8, 0, NULL));
1139     set_default_node_notes(Node_Notes::make(this));
1140   }
1141 
1142   // // -- Initialize types before each compile --
1143   // // Update cached type information
1144   // if( _method &amp;&amp; _method-&gt;constants() )
1145   //   Type::update_loaded_types(_method, _method-&gt;constants());
1146 
1147   // Init alias_type map.
1148   if (!_do_escape_analysis &amp;&amp; aliaslevel == 3)
1149     aliaslevel = 2;  // No unique types without escape analysis
1150   _AliasLevel = aliaslevel;
1151   const int grow_ats = 16;
1152   _max_alias_types = grow_ats;
1153   _alias_types   = NEW_ARENA_ARRAY(comp_arena(), AliasType*, grow_ats);
1154   AliasType* ats = NEW_ARENA_ARRAY(comp_arena(), AliasType,  grow_ats);
1155   Copy::zero_to_bytes(ats, sizeof(AliasType)*grow_ats);
1156   {
1157     for (int i = 0; i &lt; grow_ats; i++)  _alias_types[i] = &amp;ats[i];
1158   }
1159   // Initialize the first few types.
1160   _alias_types[AliasIdxTop]-&gt;Init(AliasIdxTop, NULL);
1161   _alias_types[AliasIdxBot]-&gt;Init(AliasIdxBot, TypePtr::BOTTOM);
1162   _alias_types[AliasIdxRaw]-&gt;Init(AliasIdxRaw, TypeRawPtr::BOTTOM);
1163   _num_alias_types = AliasIdxRaw+1;
1164   // Zero out the alias type cache.
1165   Copy::zero_to_bytes(_alias_cache, sizeof(_alias_cache));
1166   // A NULL adr_type hits in the cache right away.  Preload the right answer.
1167   probe_alias_cache(NULL)-&gt;_index = AliasIdxTop;
1168 
1169   _intrinsics = NULL;
1170   _macro_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1171   _predicate_opaqs = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1172   _expensive_nodes = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1173   _range_check_casts = new(comp_arena()) GrowableArray&lt;Node*&gt;(comp_arena(), 8,  0, NULL);
1174   register_library_intrinsics();
1175 }
1176 
1177 //---------------------------init_start----------------------------------------
1178 // Install the StartNode on this compile object.
1179 void Compile::init_start(StartNode* s) {
1180   if (failing())
1181     return; // already failing
1182   assert(s == start(), "");
1183 }
1184 
1185 /**
1186  * Return the 'StartNode'. We must not have a pending failure, since the ideal graph
1187  * can be in an inconsistent state, i.e., we can get segmentation faults when traversing
1188  * the ideal graph.
1189  */
1190 StartNode* Compile::start() const {
1191   assert (!failing(), "Must not have pending failure. Reason is: %s", failure_reason());
1192   for (DUIterator_Fast imax, i = root()-&gt;fast_outs(imax); i &lt; imax; i++) {
1193     Node* start = root()-&gt;fast_out(i);
1194     if (start-&gt;is_Start()) {
1195       return start-&gt;as_Start();
1196     }
1197   }
1198   fatal("Did not find Start node!");
1199   return NULL;
1200 }
1201 
1202 //-------------------------------immutable_memory-------------------------------------
1203 // Access immutable memory
1204 Node* Compile::immutable_memory() {
1205   if (_immutable_memory != NULL) {
1206     return _immutable_memory;
1207   }
1208   StartNode* s = start();
1209   for (DUIterator_Fast imax, i = s-&gt;fast_outs(imax); true; i++) {
1210     Node *p = s-&gt;fast_out(i);
1211     if (p != s &amp;&amp; p-&gt;as_Proj()-&gt;_con == TypeFunc::Memory) {
1212       _immutable_memory = p;
1213       return _immutable_memory;
1214     }
1215   }
1216   ShouldNotReachHere();
1217   return NULL;
1218 }
1219 
1220 //----------------------set_cached_top_node------------------------------------
1221 // Install the cached top node, and make sure Node::is_top works correctly.
1222 void Compile::set_cached_top_node(Node* tn) {
1223   if (tn != NULL)  verify_top(tn);
1224   Node* old_top = _top;
1225   _top = tn;
1226   // Calling Node::setup_is_top allows the nodes the chance to adjust
1227   // their _out arrays.
1228   if (_top != NULL)     _top-&gt;setup_is_top();
1229   if (old_top != NULL)  old_top-&gt;setup_is_top();
1230   assert(_top == NULL || top()-&gt;is_top(), "");
1231 }
1232 
1233 #ifdef ASSERT
1234 uint Compile::count_live_nodes_by_graph_walk() {
1235   Unique_Node_List useful(comp_arena());
1236   // Get useful node list by walking the graph.
1237   identify_useful_nodes(useful);
1238   return useful.size();
1239 }
1240 
1241 void Compile::print_missing_nodes() {
1242 
1243   // Return if CompileLog is NULL and PrintIdealNodeCount is false.
1244   if ((_log == NULL) &amp;&amp; (! PrintIdealNodeCount)) {
1245     return;
1246   }
1247 
1248   // This is an expensive function. It is executed only when the user
1249   // specifies VerifyIdealNodeCount option or otherwise knows the
1250   // additional work that needs to be done to identify reachable nodes
1251   // by walking the flow graph and find the missing ones using
1252   // _dead_node_list.
1253 
1254   Unique_Node_List useful(comp_arena());
1255   // Get useful node list by walking the graph.
1256   identify_useful_nodes(useful);
1257 
1258   uint l_nodes = C-&gt;live_nodes();
1259   uint l_nodes_by_walk = useful.size();
1260 
1261   if (l_nodes != l_nodes_by_walk) {
1262     if (_log != NULL) {
1263       _log-&gt;begin_head("mismatched_nodes count='%d'", abs((int) (l_nodes - l_nodes_by_walk)));
1264       _log-&gt;stamp();
1265       _log-&gt;end_head();
1266     }
1267     VectorSet&amp; useful_member_set = useful.member_set();
1268     int last_idx = l_nodes_by_walk;
1269     for (int i = 0; i &lt; last_idx; i++) {
1270       if (useful_member_set.test(i)) {
1271         if (_dead_node_list.test(i)) {
1272           if (_log != NULL) {
1273             _log-&gt;elem("mismatched_node_info node_idx='%d' type='both live and dead'", i);
1274           }
1275           if (PrintIdealNodeCount) {
1276             // Print the log message to tty
1277               tty-&gt;print_cr("mismatched_node idx='%d' both live and dead'", i);
1278               useful.at(i)-&gt;dump();
1279           }
1280         }
1281       }
1282       else if (! _dead_node_list.test(i)) {
1283         if (_log != NULL) {
1284           _log-&gt;elem("mismatched_node_info node_idx='%d' type='neither live nor dead'", i);
1285         }
1286         if (PrintIdealNodeCount) {
1287           // Print the log message to tty
1288           tty-&gt;print_cr("mismatched_node idx='%d' type='neither live nor dead'", i);
1289         }
1290       }
1291     }
1292     if (_log != NULL) {
1293       _log-&gt;tail("mismatched_nodes");
1294     }
1295   }
1296 }
1297 void Compile::record_modified_node(Node* n) {
1298   if (_modified_nodes != NULL &amp;&amp; !_inlining_incrementally &amp;&amp;
1299       n-&gt;outcnt() != 0 &amp;&amp; !n-&gt;is_Con()) {
1300     _modified_nodes-&gt;push(n);
1301   }
1302 }
1303 
1304 void Compile::remove_modified_node(Node* n) {
1305   if (_modified_nodes != NULL) {
1306     _modified_nodes-&gt;remove(n);
1307   }
1308 }
1309 #endif
1310 
1311 #ifndef PRODUCT
1312 void Compile::verify_top(Node* tn) const {
1313   if (tn != NULL) {
1314     assert(tn-&gt;is_Con(), "top node must be a constant");
1315     assert(((ConNode*)tn)-&gt;type() == Type::TOP, "top node must have correct type");
1316     assert(tn-&gt;in(0) != NULL, "must have live top node");
1317   }
1318 }
1319 #endif
1320 
1321 
1322 ///-------------------Managing Per-Node Debug &amp; Profile Info-------------------
1323 
1324 void Compile::grow_node_notes(GrowableArray&lt;Node_Notes*&gt;* arr, int grow_by) {
1325   guarantee(arr != NULL, "");
1326   int num_blocks = arr-&gt;length();
1327   if (grow_by &lt; num_blocks)  grow_by = num_blocks;
1328   int num_notes = grow_by * _node_notes_block_size;
1329   Node_Notes* notes = NEW_ARENA_ARRAY(node_arena(), Node_Notes, num_notes);
1330   Copy::zero_to_bytes(notes, num_notes * sizeof(Node_Notes));
1331   while (num_notes &gt; 0) {
1332     arr-&gt;append(notes);
1333     notes     += _node_notes_block_size;
1334     num_notes -= _node_notes_block_size;
1335   }
1336   assert(num_notes == 0, "exact multiple, please");
1337 }
1338 
1339 bool Compile::copy_node_notes_to(Node* dest, Node* source) {
1340   if (source == NULL || dest == NULL)  return false;
1341 
1342   if (dest-&gt;is_Con())
1343     return false;               // Do not push debug info onto constants.
1344 
1345 #ifdef ASSERT
1346   // Leave a bread crumb trail pointing to the original node:
1347   if (dest != NULL &amp;&amp; dest != source &amp;&amp; dest-&gt;debug_orig() == NULL) {
1348     dest-&gt;set_debug_orig(source);
1349   }
1350 #endif
1351 
1352   if (node_note_array() == NULL)
1353     return false;               // Not collecting any notes now.
1354 
1355   // This is a copy onto a pre-existing node, which may already have notes.
1356   // If both nodes have notes, do not overwrite any pre-existing notes.
1357   Node_Notes* source_notes = node_notes_at(source-&gt;_idx);
1358   if (source_notes == NULL || source_notes-&gt;is_clear())  return false;
1359   Node_Notes* dest_notes   = node_notes_at(dest-&gt;_idx);
1360   if (dest_notes == NULL || dest_notes-&gt;is_clear()) {
1361     return set_node_notes_at(dest-&gt;_idx, source_notes);
1362   }
1363 
1364   Node_Notes merged_notes = (*source_notes);
1365   // The order of operations here ensures that dest notes will win...
1366   merged_notes.update_from(dest_notes);
1367   return set_node_notes_at(dest-&gt;_idx, &amp;merged_notes);
1368 }
1369 
1370 
1371 //--------------------------allow_range_check_smearing-------------------------
1372 // Gating condition for coalescing similar range checks.
1373 // Sometimes we try 'speculatively' replacing a series of a range checks by a
1374 // single covering check that is at least as strong as any of them.
1375 // If the optimization succeeds, the simplified (strengthened) range check
1376 // will always succeed.  If it fails, we will deopt, and then give up
1377 // on the optimization.
1378 bool Compile::allow_range_check_smearing() const {
1379   // If this method has already thrown a range-check,
1380   // assume it was because we already tried range smearing
1381   // and it failed.
1382   uint already_trapped = trap_count(Deoptimization::Reason_range_check);
1383   return !already_trapped;
1384 }
1385 
1386 
1387 //------------------------------flatten_alias_type-----------------------------
1388 const TypePtr *Compile::flatten_alias_type( const TypePtr *tj ) const {
1389   int offset = tj-&gt;offset();
1390   TypePtr::PTR ptr = tj-&gt;ptr();
1391 
1392   // Known instance (scalarizable allocation) alias only with itself.
1393   bool is_known_inst = tj-&gt;isa_oopptr() != NULL &amp;&amp;
1394                        tj-&gt;is_oopptr()-&gt;is_known_instance();
1395 
1396   // Process weird unsafe references.
1397   if (offset == Type::OffsetBot &amp;&amp; (tj-&gt;isa_instptr() /*|| tj-&gt;isa_klassptr()*/)) {
1398     assert(InlineUnsafeOps, "indeterminate pointers come only from unsafe ops");
1399     assert(!is_known_inst, "scalarizable allocation should not have unsafe references");
1400     tj = TypeOopPtr::BOTTOM;
1401     ptr = tj-&gt;ptr();
1402     offset = tj-&gt;offset();
1403   }
1404 
1405   // Array pointers need some flattening
1406   const TypeAryPtr *ta = tj-&gt;isa_aryptr();
1407   if (ta &amp;&amp; ta-&gt;is_stable()) {
1408     // Erase stability property for alias analysis.
1409     tj = ta = ta-&gt;cast_to_stable(false);
1410   }
1411   if( ta &amp;&amp; is_known_inst ) {
1412     if ( offset != Type::OffsetBot &amp;&amp;
1413          offset &gt; arrayOopDesc::length_offset_in_bytes() ) {
1414       offset = Type::OffsetBot; // Flatten constant access into array body only
1415       tj = ta = TypeAryPtr::make(ptr, ta-&gt;ary(), ta-&gt;klass(), true, offset, ta-&gt;instance_id());
1416     }
1417   } else if( ta &amp;&amp; _AliasLevel &gt;= 2 ) {
1418     // For arrays indexed by constant indices, we flatten the alias
1419     // space to include all of the array body.  Only the header, klass
1420     // and array length can be accessed un-aliased.
1421     if( offset != Type::OffsetBot ) {
1422       if( ta-&gt;const_oop() ) { // MethodData* or Method*
1423         offset = Type::OffsetBot;   // Flatten constant access into array body
1424         tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),ta-&gt;ary(),ta-&gt;klass(),false,offset);
1425       } else if( offset == arrayOopDesc::length_offset_in_bytes() ) {
1426         // range is OK as-is.
1427         tj = ta = TypeAryPtr::RANGE;
1428       } else if( offset == oopDesc::klass_offset_in_bytes() ) {
1429         tj = TypeInstPtr::KLASS; // all klass loads look alike
1430         ta = TypeAryPtr::RANGE; // generic ignored junk
1431         ptr = TypePtr::BotPTR;
1432       } else if( offset == oopDesc::mark_offset_in_bytes() ) {
1433         tj = TypeInstPtr::MARK;
1434         ta = TypeAryPtr::RANGE; // generic ignored junk
1435         ptr = TypePtr::BotPTR;
1436       } else {                  // Random constant offset into array body
1437         offset = Type::OffsetBot;   // Flatten constant access into array body
1438         tj = ta = TypeAryPtr::make(ptr,ta-&gt;ary(),ta-&gt;klass(),false,offset);
1439       }
1440     }
1441     // Arrays of fixed size alias with arrays of unknown size.
1442     if (ta-&gt;size() != TypeInt::POS) {
1443       const TypeAry *tary = TypeAry::make(ta-&gt;elem(), TypeInt::POS);
1444       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,ta-&gt;klass(),false,offset);
1445     }
1446     // Arrays of known objects become arrays of unknown objects.
1447     if (ta-&gt;elem()-&gt;isa_narrowoop() &amp;&amp; ta-&gt;elem() != TypeNarrowOop::BOTTOM) {
1448       const TypeAry *tary = TypeAry::make(TypeNarrowOop::BOTTOM, ta-&gt;size());
1449       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);
1450     }
1451     if (ta-&gt;elem()-&gt;isa_oopptr() &amp;&amp; ta-&gt;elem() != TypeInstPtr::BOTTOM) {
1452       const TypeAry *tary = TypeAry::make(TypeInstPtr::BOTTOM, ta-&gt;size());
1453       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,NULL,false,offset);
1454     }
1455     // Arrays of bytes and of booleans both use 'bastore' and 'baload' so
1456     // cannot be distinguished by bytecode alone.
1457     if (ta-&gt;elem() == TypeInt::BOOL) {
1458       const TypeAry *tary = TypeAry::make(TypeInt::BYTE, ta-&gt;size());
1459       ciKlass* aklass = ciTypeArrayKlass::make(T_BYTE);
1460       tj = ta = TypeAryPtr::make(ptr,ta-&gt;const_oop(),tary,aklass,false,offset);
1461     }
1462     // During the 2nd round of IterGVN, NotNull castings are removed.
1463     // Make sure the Bottom and NotNull variants alias the same.
1464     // Also, make sure exact and non-exact variants alias the same.
1465     if (ptr == TypePtr::NotNull || ta-&gt;klass_is_exact() || ta-&gt;speculative() != NULL) {
1466       tj = ta = TypeAryPtr::make(TypePtr::BotPTR,ta-&gt;ary(),ta-&gt;klass(),false,offset);
1467     }
1468   }
1469 
1470   // Oop pointers need some flattening
1471   const TypeInstPtr *to = tj-&gt;isa_instptr();
1472   if( to &amp;&amp; _AliasLevel &gt;= 2 &amp;&amp; to != TypeOopPtr::BOTTOM ) {
1473     ciInstanceKlass *k = to-&gt;klass()-&gt;as_instance_klass();
1474     if( ptr == TypePtr::Constant ) {
1475       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass() ||
1476           offset &lt; k-&gt;size_helper() * wordSize) {
1477         // No constant oop pointers (such as Strings); they alias with
1478         // unknown strings.
1479         assert(!is_known_inst, "not scalarizable allocation");
1480         tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);
1481       }
1482     } else if( is_known_inst ) {
1483       tj = to; // Keep NotNull and klass_is_exact for instance type
1484     } else if( ptr == TypePtr::NotNull || to-&gt;klass_is_exact() ) {
1485       // During the 2nd round of IterGVN, NotNull castings are removed.
1486       // Make sure the Bottom and NotNull variants alias the same.
1487       // Also, make sure exact and non-exact variants alias the same.
1488       tj = to = TypeInstPtr::make(TypePtr::BotPTR,to-&gt;klass(),false,0,offset);
1489     }
1490     if (to-&gt;speculative() != NULL) {
1491       tj = to = TypeInstPtr::make(to-&gt;ptr(),to-&gt;klass(),to-&gt;klass_is_exact(),to-&gt;const_oop(),to-&gt;offset(), to-&gt;instance_id());
1492     }
1493     // Canonicalize the holder of this field
1494     if (offset &gt;= 0 &amp;&amp; offset &lt; instanceOopDesc::base_offset_in_bytes()) {
1495       // First handle header references such as a LoadKlassNode, even if the
1496       // object's klass is unloaded at compile time (4965979).
1497       if (!is_known_inst) { // Do it only for non-instance types
1498         tj = to = TypeInstPtr::make(TypePtr::BotPTR, env()-&gt;Object_klass(), false, NULL, offset);
1499       }
1500     } else if (offset &lt; 0 || offset &gt;= k-&gt;size_helper() * wordSize) {
1501       // Static fields are in the space above the normal instance
1502       // fields in the java.lang.Class instance.
1503       if (to-&gt;klass() != ciEnv::current()-&gt;Class_klass()) {
1504         to = NULL;
1505         tj = TypeOopPtr::BOTTOM;
1506         offset = tj-&gt;offset();
1507       }
1508     } else {
1509       ciInstanceKlass *canonical_holder = k-&gt;get_canonical_holder(offset);
1510       if (!k-&gt;equals(canonical_holder) || tj-&gt;offset() != offset) {
1511         if( is_known_inst ) {
1512           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, true, NULL, offset, to-&gt;instance_id());
1513         } else {
1514           tj = to = TypeInstPtr::make(to-&gt;ptr(), canonical_holder, false, NULL, offset);
1515         }
1516       }
1517     }
1518   }
1519 
1520   // Klass pointers to object array klasses need some flattening
1521   const TypeKlassPtr *tk = tj-&gt;isa_klassptr();
1522   if( tk ) {
1523     // If we are referencing a field within a Klass, we need
1524     // to assume the worst case of an Object.  Both exact and
1525     // inexact types must flatten to the same alias class so
1526     // use NotNull as the PTR.
1527     if ( offset == Type::OffsetBot || (offset &gt;= 0 &amp;&amp; (size_t)offset &lt; sizeof(Klass)) ) {
1528 
1529       tj = tk = TypeKlassPtr::make(TypePtr::NotNull,
1530                                    TypeKlassPtr::OBJECT-&gt;klass(),
1531                                    offset);
1532     }
1533 
1534     ciKlass* klass = tk-&gt;klass();
1535     if( klass-&gt;is_obj_array_klass() ) {
1536       ciKlass* k = TypeAryPtr::OOPS-&gt;klass();
1537       if( !k || !k-&gt;is_loaded() )                  // Only fails for some -Xcomp runs
1538         k = TypeInstPtr::BOTTOM-&gt;klass();
1539       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, k, offset );
1540     }
1541 
1542     // Check for precise loads from the primary supertype array and force them
1543     // to the supertype cache alias index.  Check for generic array loads from
1544     // the primary supertype array and also force them to the supertype cache
1545     // alias index.  Since the same load can reach both, we need to merge
1546     // these 2 disparate memories into the same alias class.  Since the
1547     // primary supertype array is read-only, there's no chance of confusion
1548     // where we bypass an array load and an array store.
1549     int primary_supers_offset = in_bytes(Klass::primary_supers_offset());
1550     if (offset == Type::OffsetBot ||
1551         (offset &gt;= primary_supers_offset &amp;&amp;
1552          offset &lt; (int)(primary_supers_offset + Klass::primary_super_limit() * wordSize)) ||
1553         offset == (int)in_bytes(Klass::secondary_super_cache_offset())) {
1554       offset = in_bytes(Klass::secondary_super_cache_offset());
1555       tj = tk = TypeKlassPtr::make( TypePtr::NotNull, tk-&gt;klass(), offset );
1556     }
1557   }
1558 
1559   // Flatten all Raw pointers together.
1560   if (tj-&gt;base() == Type::RawPtr)
1561     tj = TypeRawPtr::BOTTOM;
1562 
1563   if (tj-&gt;base() == Type::AnyPtr)
1564     tj = TypePtr::BOTTOM;      // An error, which the caller must check for.
1565 
1566   // Flatten all to bottom for now
1567   switch( _AliasLevel ) {
1568   case 0:
1569     tj = TypePtr::BOTTOM;
1570     break;
1571   case 1:                       // Flatten to: oop, static, field or array
1572     switch (tj-&gt;base()) {
1573     //case Type::AryPtr: tj = TypeAryPtr::RANGE;    break;
1574     case Type::RawPtr:   tj = TypeRawPtr::BOTTOM;   break;
1575     case Type::AryPtr:   // do not distinguish arrays at all
1576     case Type::InstPtr:  tj = TypeInstPtr::BOTTOM;  break;
1577     case Type::KlassPtr: tj = TypeKlassPtr::OBJECT; break;
1578     case Type::AnyPtr:   tj = TypePtr::BOTTOM;      break;  // caller checks it
1579     default: ShouldNotReachHere();
1580     }
1581     break;
1582   case 2:                       // No collapsing at level 2; keep all splits
1583   case 3:                       // No collapsing at level 3; keep all splits
1584     break;
1585   default:
1586     Unimplemented();
1587   }
1588 
1589   offset = tj-&gt;offset();
1590   assert( offset != Type::OffsetTop, "Offset has fallen from constant" );
1591 
1592   assert( (offset != Type::OffsetBot &amp;&amp; tj-&gt;base() != Type::AryPtr) ||
1593           (offset == Type::OffsetBot &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1594           (offset == Type::OffsetBot &amp;&amp; tj == TypeOopPtr::BOTTOM) ||
1595           (offset == Type::OffsetBot &amp;&amp; tj == TypePtr::BOTTOM) ||
1596           (offset == oopDesc::mark_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1597           (offset == oopDesc::klass_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr) ||
1598           (offset == arrayOopDesc::length_offset_in_bytes() &amp;&amp; tj-&gt;base() == Type::AryPtr)  ,
1599           "For oops, klasses, raw offset must be constant; for arrays the offset is never known" );
1600   assert( tj-&gt;ptr() != TypePtr::TopPTR &amp;&amp;
1601           tj-&gt;ptr() != TypePtr::AnyNull &amp;&amp;
1602           tj-&gt;ptr() != TypePtr::Null, "No imprecise addresses" );
1603 //    assert( tj-&gt;ptr() != TypePtr::Constant ||
1604 //            tj-&gt;base() == Type::RawPtr ||
1605 //            tj-&gt;base() == Type::KlassPtr, "No constant oop addresses" );
1606 
1607   return tj;
1608 }
1609 
1610 void Compile::AliasType::Init(int i, const TypePtr* at) {
1611   _index = i;
1612   _adr_type = at;
1613   _field = NULL;
1614   _element = NULL;
1615   _is_rewritable = true; // default
1616   const TypeOopPtr *atoop = (at != NULL) ? at-&gt;isa_oopptr() : NULL;
1617   if (atoop != NULL &amp;&amp; atoop-&gt;is_known_instance()) {
1618     const TypeOopPtr *gt = atoop-&gt;cast_to_instance_id(TypeOopPtr::InstanceBot);
1619     _general_index = Compile::current()-&gt;get_alias_index(gt);
1620   } else {
1621     _general_index = 0;
1622   }
1623 }
1624 
1625 //---------------------------------print_on------------------------------------
1626 #ifndef PRODUCT
1627 void Compile::AliasType::print_on(outputStream* st) {
1628   if (index() &lt; 10)
1629         st-&gt;print("@ &lt;%d&gt; ", index());
1630   else  st-&gt;print("@ &lt;%d&gt;",  index());
1631   st-&gt;print(is_rewritable() ? "   " : " RO");
1632   int offset = adr_type()-&gt;offset();
1633   if (offset == Type::OffsetBot)
1634         st-&gt;print(" +any");
1635   else  st-&gt;print(" +%-3d", offset);
1636   st-&gt;print(" in ");
1637   adr_type()-&gt;dump_on(st);
1638   const TypeOopPtr* tjp = adr_type()-&gt;isa_oopptr();
1639   if (field() != NULL &amp;&amp; tjp) {
1640     if (tjp-&gt;klass()  != field()-&gt;holder() ||
1641         tjp-&gt;offset() != field()-&gt;offset_in_bytes()) {
1642       st-&gt;print(" != ");
1643       field()-&gt;print();
1644       st-&gt;print(" ***");
1645     }
1646   }
1647 }
1648 
1649 void print_alias_types() {
1650   Compile* C = Compile::current();
1651   tty-&gt;print_cr("--- Alias types, AliasIdxBot .. %d", C-&gt;num_alias_types()-1);
1652   for (int idx = Compile::AliasIdxBot; idx &lt; C-&gt;num_alias_types(); idx++) {
1653     C-&gt;alias_type(idx)-&gt;print_on(tty);
1654     tty-&gt;cr();
1655   }
1656 }
1657 #endif
1658 
1659 
1660 //----------------------------probe_alias_cache--------------------------------
1661 Compile::AliasCacheEntry* Compile::probe_alias_cache(const TypePtr* adr_type) {
1662   intptr_t key = (intptr_t) adr_type;
1663   key ^= key &gt;&gt; logAliasCacheSize;
1664   return &amp;_alias_cache[key &amp; right_n_bits(logAliasCacheSize)];
1665 }
1666 
1667 
1668 //-----------------------------grow_alias_types--------------------------------
1669 void Compile::grow_alias_types() {
1670   const int old_ats  = _max_alias_types; // how many before?
1671   const int new_ats  = old_ats;          // how many more?
1672   const int grow_ats = old_ats+new_ats;  // how many now?
1673   _max_alias_types = grow_ats;
1674   _alias_types =  REALLOC_ARENA_ARRAY(comp_arena(), AliasType*, _alias_types, old_ats, grow_ats);
1675   AliasType* ats =    NEW_ARENA_ARRAY(comp_arena(), AliasType, new_ats);
1676   Copy::zero_to_bytes(ats, sizeof(AliasType)*new_ats);
1677   for (int i = 0; i &lt; new_ats; i++)  _alias_types[old_ats+i] = &amp;ats[i];
1678 }
1679 
1680 
1681 //--------------------------------find_alias_type------------------------------
1682 Compile::AliasType* Compile::find_alias_type(const TypePtr* adr_type, bool no_create, ciField* original_field) {
1683   if (_AliasLevel == 0)
1684     return alias_type(AliasIdxBot);
1685 
1686   AliasCacheEntry* ace = probe_alias_cache(adr_type);
1687   if (ace-&gt;_adr_type == adr_type) {
1688     return alias_type(ace-&gt;_index);
1689   }
1690 
1691   // Handle special cases.
1692   if (adr_type == NULL)             return alias_type(AliasIdxTop);
1693   if (adr_type == TypePtr::BOTTOM)  return alias_type(AliasIdxBot);
1694 
1695   // Do it the slow way.
1696   const TypePtr* flat = flatten_alias_type(adr_type);
1697 
1698 #ifdef ASSERT
1699   assert(flat == flatten_alias_type(flat), "idempotent");
1700   assert(flat != TypePtr::BOTTOM,     "cannot alias-analyze an untyped ptr");
1701   if (flat-&gt;isa_oopptr() &amp;&amp; !flat-&gt;isa_klassptr()) {
1702     const TypeOopPtr* foop = flat-&gt;is_oopptr();
1703     // Scalarizable allocations have exact klass always.
1704     bool exact = !foop-&gt;klass_is_exact() || foop-&gt;is_known_instance();
1705     const TypePtr* xoop = foop-&gt;cast_to_exactness(exact)-&gt;is_ptr();
1706     assert(foop == flatten_alias_type(xoop), "exactness must not affect alias type");
1707   }
1708   assert(flat == flatten_alias_type(flat), "exact bit doesn't matter");
1709 #endif
1710 
1711   int idx = AliasIdxTop;
1712   for (int i = 0; i &lt; num_alias_types(); i++) {
1713     if (alias_type(i)-&gt;adr_type() == flat) {
1714       idx = i;
1715       break;
1716     }
1717   }
1718 
1719   if (idx == AliasIdxTop) {
1720     if (no_create)  return NULL;
1721     // Grow the array if necessary.
1722     if (_num_alias_types == _max_alias_types)  grow_alias_types();
1723     // Add a new alias type.
1724     idx = _num_alias_types++;
1725     _alias_types[idx]-&gt;Init(idx, flat);
1726     if (flat == TypeInstPtr::KLASS)  alias_type(idx)-&gt;set_rewritable(false);
1727     if (flat == TypeAryPtr::RANGE)   alias_type(idx)-&gt;set_rewritable(false);
1728     if (flat-&gt;isa_instptr()) {
1729       if (flat-&gt;offset() == java_lang_Class::klass_offset_in_bytes()
1730           &amp;&amp; flat-&gt;is_instptr()-&gt;klass() == env()-&gt;Class_klass())
1731         alias_type(idx)-&gt;set_rewritable(false);
1732     }
1733     if (flat-&gt;isa_aryptr()) {
1734 #ifdef ASSERT
1735       const int header_size_min  = arrayOopDesc::base_offset_in_bytes(T_BYTE);
1736       // (T_BYTE has the weakest alignment and size restrictions...)
1737       assert(flat-&gt;offset() &lt; header_size_min, "array body reference must be OffsetBot");
1738 #endif
1739       if (flat-&gt;offset() == TypePtr::OffsetBot) {
1740         alias_type(idx)-&gt;set_element(flat-&gt;is_aryptr()-&gt;elem());
1741       }
1742     }
1743     if (flat-&gt;isa_klassptr()) {
1744       if (flat-&gt;offset() == in_bytes(Klass::super_check_offset_offset()))
1745         alias_type(idx)-&gt;set_rewritable(false);
1746       if (flat-&gt;offset() == in_bytes(Klass::modifier_flags_offset()))
1747         alias_type(idx)-&gt;set_rewritable(false);
1748       if (flat-&gt;offset() == in_bytes(Klass::access_flags_offset()))
1749         alias_type(idx)-&gt;set_rewritable(false);
1750       if (flat-&gt;offset() == in_bytes(Klass::java_mirror_offset()))
1751         alias_type(idx)-&gt;set_rewritable(false);
1752     }
1753     // %%% (We would like to finalize JavaThread::threadObj_offset(),
1754     // but the base pointer type is not distinctive enough to identify
1755     // references into JavaThread.)
1756 
1757     // Check for final fields.
1758     const TypeInstPtr* tinst = flat-&gt;isa_instptr();
1759     if (tinst &amp;&amp; tinst-&gt;offset() &gt;= instanceOopDesc::base_offset_in_bytes()) {
1760       ciField* field;
1761       if (tinst-&gt;const_oop() != NULL &amp;&amp;
1762           tinst-&gt;klass() == ciEnv::current()-&gt;Class_klass() &amp;&amp;
1763           tinst-&gt;offset() &gt;= (tinst-&gt;klass()-&gt;as_instance_klass()-&gt;size_helper() * wordSize)) {
1764         // static field
1765         ciInstanceKlass* k = tinst-&gt;const_oop()-&gt;as_instance()-&gt;java_lang_Class_klass()-&gt;as_instance_klass();
1766         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), true);
1767       } else {
1768         ciInstanceKlass *k = tinst-&gt;klass()-&gt;as_instance_klass();
1769         field = k-&gt;get_field_by_offset(tinst-&gt;offset(), false);
1770       }
1771       assert(field == NULL ||
1772              original_field == NULL ||
1773              (field-&gt;holder() == original_field-&gt;holder() &amp;&amp;
1774               field-&gt;offset() == original_field-&gt;offset() &amp;&amp;
1775               field-&gt;is_static() == original_field-&gt;is_static()), "wrong field?");
1776       // Set field() and is_rewritable() attributes.
1777       if (field != NULL)  alias_type(idx)-&gt;set_field(field);
1778     }
1779   }
1780 
1781   // Fill the cache for next time.
1782   ace-&gt;_adr_type = adr_type;
1783   ace-&gt;_index    = idx;
1784   assert(alias_type(adr_type) == alias_type(idx),  "type must be installed");
1785 
1786   // Might as well try to fill the cache for the flattened version, too.
1787   AliasCacheEntry* face = probe_alias_cache(flat);
1788   if (face-&gt;_adr_type == NULL) {
1789     face-&gt;_adr_type = flat;
1790     face-&gt;_index    = idx;
1791     assert(alias_type(flat) == alias_type(idx), "flat type must work too");
1792   }
1793 
1794   return alias_type(idx);
1795 }
1796 
1797 
1798 Compile::AliasType* Compile::alias_type(ciField* field) {
1799   const TypeOopPtr* t;
1800   if (field-&gt;is_static())
1801     t = TypeInstPtr::make(field-&gt;holder()-&gt;java_mirror());
1802   else
1803     t = TypeOopPtr::make_from_klass_raw(field-&gt;holder());
1804   AliasType* atp = alias_type(t-&gt;add_offset(field-&gt;offset_in_bytes()), field);
1805   assert((field-&gt;is_final() || field-&gt;is_stable()) == !atp-&gt;is_rewritable(), "must get the rewritable bits correct");
1806   return atp;
1807 }
1808 
1809 
1810 //------------------------------have_alias_type--------------------------------
1811 bool Compile::have_alias_type(const TypePtr* adr_type) {
1812   AliasCacheEntry* ace = probe_alias_cache(adr_type);
1813   if (ace-&gt;_adr_type == adr_type) {
1814     return true;
1815   }
1816 
1817   // Handle special cases.
1818   if (adr_type == NULL)             return true;
1819   if (adr_type == TypePtr::BOTTOM)  return true;
1820 
1821   return find_alias_type(adr_type, true, NULL) != NULL;
1822 }
1823 
1824 //-----------------------------must_alias--------------------------------------
1825 // True if all values of the given address type are in the given alias category.
1826 bool Compile::must_alias(const TypePtr* adr_type, int alias_idx) {
1827   if (alias_idx == AliasIdxBot)         return true;  // the universal category
1828   if (adr_type == NULL)                 return true;  // NULL serves as TypePtr::TOP
1829   if (alias_idx == AliasIdxTop)         return false; // the empty category
1830   if (adr_type-&gt;base() == Type::AnyPtr) return false; // TypePtr::BOTTOM or its twins
1831 
1832   // the only remaining possible overlap is identity
1833   int adr_idx = get_alias_index(adr_type);
1834   assert(adr_idx != AliasIdxBot &amp;&amp; adr_idx != AliasIdxTop, "");
1835   assert(adr_idx == alias_idx ||
1836          (alias_type(alias_idx)-&gt;adr_type() != TypeOopPtr::BOTTOM
1837           &amp;&amp; adr_type                       != TypeOopPtr::BOTTOM),
1838          "should not be testing for overlap with an unsafe pointer");
1839   return adr_idx == alias_idx;
1840 }
1841 
1842 //------------------------------can_alias--------------------------------------
1843 // True if any values of the given address type are in the given alias category.
1844 bool Compile::can_alias(const TypePtr* adr_type, int alias_idx) {
1845   if (alias_idx == AliasIdxTop)         return false; // the empty category
1846   if (adr_type == NULL)                 return false; // NULL serves as TypePtr::TOP
1847   if (alias_idx == AliasIdxBot)         return true;  // the universal category
1848   if (adr_type-&gt;base() == Type::AnyPtr) return true;  // TypePtr::BOTTOM or its twins
1849 
1850   // the only remaining possible overlap is identity
1851   int adr_idx = get_alias_index(adr_type);
1852   assert(adr_idx != AliasIdxBot &amp;&amp; adr_idx != AliasIdxTop, "");
1853   return adr_idx == alias_idx;
1854 }
1855 
1856 
1857 
1858 //---------------------------pop_warm_call-------------------------------------
1859 WarmCallInfo* Compile::pop_warm_call() {
1860   WarmCallInfo* wci = _warm_calls;
1861   if (wci != NULL)  _warm_calls = wci-&gt;remove_from(wci);
1862   return wci;
1863 }
1864 
1865 //----------------------------Inline_Warm--------------------------------------
1866 int Compile::Inline_Warm() {
1867   // If there is room, try to inline some more warm call sites.
1868   // %%% Do a graph index compaction pass when we think we're out of space?
1869   if (!InlineWarmCalls)  return 0;
1870 
1871   int calls_made_hot = 0;
1872   int room_to_grow   = NodeCountInliningCutoff - unique();
1873   int amount_to_grow = MIN2(room_to_grow, (int)NodeCountInliningStep);
1874   int amount_grown   = 0;
1875   WarmCallInfo* call;
1876   while (amount_to_grow &gt; 0 &amp;&amp; (call = pop_warm_call()) != NULL) {
1877     int est_size = (int)call-&gt;size();
1878     if (est_size &gt; (room_to_grow - amount_grown)) {
1879       // This one won't fit anyway.  Get rid of it.
1880       call-&gt;make_cold();
1881       continue;
1882     }
1883     call-&gt;make_hot();
1884     calls_made_hot++;
1885     amount_grown   += est_size;
1886     amount_to_grow -= est_size;
1887   }
1888 
1889   if (calls_made_hot &gt; 0)  set_major_progress();
1890   return calls_made_hot;
1891 }
1892 
1893 
1894 //----------------------------Finish_Warm--------------------------------------
1895 void Compile::Finish_Warm() {
1896   if (!InlineWarmCalls)  return;
1897   if (failing())  return;
1898   if (warm_calls() == NULL)  return;
1899 
1900   // Clean up loose ends, if we are out of space for inlining.
1901   WarmCallInfo* call;
1902   while ((call = pop_warm_call()) != NULL) {
1903     call-&gt;make_cold();
1904   }
1905 }
1906 
1907 //---------------------cleanup_loop_predicates-----------------------
1908 // Remove the opaque nodes that protect the predicates so that all unused
1909 // checks and uncommon_traps will be eliminated from the ideal graph
1910 void Compile::cleanup_loop_predicates(PhaseIterGVN &amp;igvn) {
1911   if (predicate_count()==0) return;
1912   for (int i = predicate_count(); i &gt; 0; i--) {
1913     Node * n = predicate_opaque1_node(i-1);
1914     assert(n-&gt;Opcode() == Op_Opaque1, "must be");
1915     igvn.replace_node(n, n-&gt;in(1));
1916   }
1917   assert(predicate_count()==0, "should be clean!");
1918 }
1919 
1920 void Compile::add_range_check_cast(Node* n) {
1921   assert(n-&gt;isa_CastII()-&gt;has_range_check(), "CastII should have range check dependency");
1922   assert(!_range_check_casts-&gt;contains(n), "duplicate entry in range check casts");
1923   _range_check_casts-&gt;append(n);
1924 }
1925 
1926 // Remove all range check dependent CastIINodes.
1927 void Compile::remove_range_check_casts(PhaseIterGVN &amp;igvn) {
1928   for (int i = range_check_cast_count(); i &gt; 0; i--) {
1929     Node* cast = range_check_cast_node(i-1);
1930     assert(cast-&gt;isa_CastII()-&gt;has_range_check(), "CastII should have range check dependency");
1931     igvn.replace_node(cast, cast-&gt;in(1));
1932   }
1933   assert(range_check_cast_count() == 0, "should be empty");
1934 }
1935 
1936 // StringOpts and late inlining of string methods
1937 void Compile::inline_string_calls(bool parse_time) {
1938   {
1939     // remove useless nodes to make the usage analysis simpler
1940     ResourceMark rm;
1941     PhaseRemoveUseless pru(initial_gvn(), for_igvn());
1942   }
1943 
1944   {
1945     ResourceMark rm;
1946     print_method(PHASE_BEFORE_STRINGOPTS, 3);
1947     PhaseStringOpts pso(initial_gvn(), for_igvn());
1948     print_method(PHASE_AFTER_STRINGOPTS, 3);
1949   }
1950 
1951   // now inline anything that we skipped the first time around
1952   if (!parse_time) {
1953     _late_inlines_pos = _late_inlines.length();
1954   }
1955 
1956   while (_string_late_inlines.length() &gt; 0) {
1957     CallGenerator* cg = _string_late_inlines.pop();
1958     cg-&gt;do_late_inline();
1959     if (failing())  return;
1960   }
1961   _string_late_inlines.trunc_to(0);
1962 }
1963 
1964 // Late inlining of boxing methods
1965 void Compile::inline_boxing_calls(PhaseIterGVN&amp; igvn) {
1966   if (_boxing_late_inlines.length() &gt; 0) {
1967     assert(has_boxed_value(), "inconsistent");
1968 
1969     PhaseGVN* gvn = initial_gvn();
1970     set_inlining_incrementally(true);
1971 
1972     assert( igvn._worklist.size() == 0, "should be done with igvn" );
1973     for_igvn()-&gt;clear();
1974     gvn-&gt;replace_with(&amp;igvn);
1975 
1976     _late_inlines_pos = _late_inlines.length();
1977 
1978     while (_boxing_late_inlines.length() &gt; 0) {
1979       CallGenerator* cg = _boxing_late_inlines.pop();
1980       cg-&gt;do_late_inline();
1981       if (failing())  return;
1982     }
1983     _boxing_late_inlines.trunc_to(0);
1984 
1985     {
1986       ResourceMark rm;
1987       PhaseRemoveUseless pru(gvn, for_igvn());
1988     }
1989 
1990     igvn = PhaseIterGVN(gvn);
1991     igvn.optimize();
1992 
1993     set_inlining_progress(false);
1994     set_inlining_incrementally(false);
1995   }
1996 }
1997 
1998 void Compile::inline_incrementally_one(PhaseIterGVN&amp; igvn) {
1999   assert(IncrementalInline, "incremental inlining should be on");
2000   PhaseGVN* gvn = initial_gvn();
2001 
2002   set_inlining_progress(false);
2003   for_igvn()-&gt;clear();
2004   gvn-&gt;replace_with(&amp;igvn);
2005 
2006   {
2007     TracePhase tp("incrementalInline_inline", &amp;timers[_t_incrInline_inline]);
2008     int i = 0;
2009     for (; i &lt;_late_inlines.length() &amp;&amp; !inlining_progress(); i++) {
2010       CallGenerator* cg = _late_inlines.at(i);
2011       _late_inlines_pos = i+1;
2012       cg-&gt;do_late_inline();
2013       if (failing())  return;
2014     }
2015     int j = 0;
2016     for (; i &lt; _late_inlines.length(); i++, j++) {
2017       _late_inlines.at_put(j, _late_inlines.at(i));
2018     }
2019     _late_inlines.trunc_to(j);
2020   }
2021 
2022   {
2023     TracePhase tp("incrementalInline_pru", &amp;timers[_t_incrInline_pru]);
2024     ResourceMark rm;
2025     PhaseRemoveUseless pru(gvn, for_igvn());
2026   }
2027 
2028   {
2029     TracePhase tp("incrementalInline_igvn", &amp;timers[_t_incrInline_igvn]);
2030     igvn = PhaseIterGVN(gvn);
2031   }
2032 }
2033 
2034 // Perform incremental inlining until bound on number of live nodes is reached
2035 void Compile::inline_incrementally(PhaseIterGVN&amp; igvn) {
2036   TracePhase tp("incrementalInline", &amp;timers[_t_incrInline]);
2037 
2038   PhaseGVN* gvn = initial_gvn();
2039 
2040   set_inlining_incrementally(true);
2041   set_inlining_progress(true);
2042   uint low_live_nodes = 0;
2043 
2044   while(inlining_progress() &amp;&amp; _late_inlines.length() &gt; 0) {
2045 
2046     if (live_nodes() &gt; (uint)LiveNodeCountInliningCutoff) {
2047       if (low_live_nodes &lt; (uint)LiveNodeCountInliningCutoff * 8 / 10) {
2048         TracePhase tp("incrementalInline_ideal", &amp;timers[_t_incrInline_ideal]);
2049         // PhaseIdealLoop is expensive so we only try it once we are
2050         // out of live nodes and we only try it again if the previous
2051         // helped got the number of nodes down significantly
2052         PhaseIdealLoop ideal_loop( igvn, false, true );
2053         if (failing())  return;
2054         low_live_nodes = live_nodes();
2055         _major_progress = true;
2056       }
2057 
2058       if (live_nodes() &gt; (uint)LiveNodeCountInliningCutoff) {
2059         break;
2060       }
2061     }
2062 
2063     inline_incrementally_one(igvn);
2064 
2065     if (failing())  return;
2066 
2067     {
2068       TracePhase tp("incrementalInline_igvn", &amp;timers[_t_incrInline_igvn]);
2069       igvn.optimize();
2070     }
2071 
2072     if (failing())  return;
2073   }
2074 
2075   assert( igvn._worklist.size() == 0, "should be done with igvn" );
2076 
2077   if (_string_late_inlines.length() &gt; 0) {
2078     assert(has_stringbuilder(), "inconsistent");
2079     for_igvn()-&gt;clear();
2080     initial_gvn()-&gt;replace_with(&amp;igvn);
2081 
2082     inline_string_calls(false);
2083 
2084     if (failing())  return;
2085 
2086     {
2087       TracePhase tp("incrementalInline_pru", &amp;timers[_t_incrInline_pru]);
2088       ResourceMark rm;
2089       PhaseRemoveUseless pru(initial_gvn(), for_igvn());
2090     }
2091 
2092     {
2093       TracePhase tp("incrementalInline_igvn", &amp;timers[_t_incrInline_igvn]);
2094       igvn = PhaseIterGVN(gvn);
2095       igvn.optimize();
2096     }
2097   }
2098 
2099   set_inlining_incrementally(false);
2100 }
2101 
2102 
2103 //------------------------------Optimize---------------------------------------
2104 // Given a graph, optimize it.
2105 void Compile::Optimize() {
2106   TracePhase tp("optimizer", &amp;timers[_t_optimizer]);
2107 
2108 #ifndef PRODUCT
2109   if (_directive-&gt;BreakAtCompileOption) {
2110     BREAKPOINT;
2111   }
2112 
2113 #endif
2114 
2115   ResourceMark rm;
2116   int          loop_opts_cnt;
2117 
2118   print_inlining_reinit();
2119 
2120   NOT_PRODUCT( verify_graph_edges(); )
2121 
2122   print_method(PHASE_AFTER_PARSING);
2123 
2124  {
2125   // Iterative Global Value Numbering, including ideal transforms
2126   // Initialize IterGVN with types and values from parse-time GVN
2127   PhaseIterGVN igvn(initial_gvn());
2128 #ifdef ASSERT
2129   _modified_nodes = new (comp_arena()) Unique_Node_List(comp_arena());
2130 #endif
2131   {
2132     TracePhase tp("iterGVN", &amp;timers[_t_iterGVN]);
2133     igvn.optimize();
2134   }
2135 
2136   print_method(PHASE_ITER_GVN1, 2);
2137 
2138   if (failing())  return;
2139 
2140   inline_incrementally(igvn);
2141 
2142   print_method(PHASE_INCREMENTAL_INLINE, 2);
2143 
2144   if (failing())  return;
2145 
2146   if (eliminate_boxing()) {
2147     // Inline valueOf() methods now.
2148     inline_boxing_calls(igvn);
2149 
2150     if (AlwaysIncrementalInline) {
2151       inline_incrementally(igvn);
2152     }
2153 
2154     print_method(PHASE_INCREMENTAL_BOXING_INLINE, 2);
2155 
2156     if (failing())  return;
2157   }
2158 
2159   // Remove the speculative part of types and clean up the graph from
2160   // the extra CastPP nodes whose only purpose is to carry them. Do
2161   // that early so that optimizations are not disrupted by the extra
2162   // CastPP nodes.
2163   remove_speculative_types(igvn);
2164 
2165   // No more new expensive nodes will be added to the list from here
2166   // so keep only the actual candidates for optimizations.
2167   cleanup_expensive_nodes(igvn);
2168 
2169   if (!failing() &amp;&amp; RenumberLiveNodes &amp;&amp; live_nodes() + NodeLimitFudgeFactor &lt; unique()) {
2170     Compile::TracePhase tp("", &amp;timers[_t_renumberLive]);
2171     initial_gvn()-&gt;replace_with(&amp;igvn);
2172     for_igvn()-&gt;clear();
2173     Unique_Node_List new_worklist(C-&gt;comp_arena());
2174     {
2175       ResourceMark rm;
2176       PhaseRenumberLive prl = PhaseRenumberLive(initial_gvn(), for_igvn(), &amp;new_worklist);
2177     }
2178     set_for_igvn(&amp;new_worklist);
2179     igvn = PhaseIterGVN(initial_gvn());
2180     igvn.optimize();
2181   }
2182 
2183   // Perform escape analysis
2184   if (_do_escape_analysis &amp;&amp; ConnectionGraph::has_candidates(this)) {
2185     if (has_loops()) {
2186       // Cleanup graph (remove dead nodes).
2187       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2188       PhaseIdealLoop ideal_loop( igvn, false, true );
2189       if (major_progress()) print_method(PHASE_PHASEIDEAL_BEFORE_EA, 2);
2190       if (failing())  return;
2191     }
2192     ConnectionGraph::do_analysis(this, &amp;igvn);
2193 
2194     if (failing())  return;
2195 
2196     // Optimize out fields loads from scalar replaceable allocations.
2197     igvn.optimize();
2198     print_method(PHASE_ITER_GVN_AFTER_EA, 2);
2199 
2200     if (failing())  return;
2201 
2202     if (congraph() != NULL &amp;&amp; macro_count() &gt; 0) {
2203       TracePhase tp("macroEliminate", &amp;timers[_t_macroEliminate]);
2204       PhaseMacroExpand mexp(igvn);
2205       mexp.eliminate_macro_nodes();
2206       igvn.set_delay_transform(false);
2207 
2208       igvn.optimize();
2209       print_method(PHASE_ITER_GVN_AFTER_ELIMINATION, 2);
2210 
2211       if (failing())  return;
2212     }
2213   }
2214 
2215   // Loop transforms on the ideal graph.  Range Check Elimination,
2216   // peeling, unrolling, etc.
2217 
2218   // Set loop opts counter
2219   loop_opts_cnt = num_loop_opts();
2220   if((loop_opts_cnt &gt; 0) &amp;&amp; (has_loops() || has_split_ifs())) {
2221     {
2222       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2223       PhaseIdealLoop ideal_loop( igvn, true );
2224       loop_opts_cnt--;
2225       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP1, 2);
2226       if (failing())  return;
2227     }
2228     // Loop opts pass if partial peeling occurred in previous pass
2229     if(PartialPeelLoop &amp;&amp; major_progress() &amp;&amp; (loop_opts_cnt &gt; 0)) {
2230       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2231       PhaseIdealLoop ideal_loop( igvn, false );
2232       loop_opts_cnt--;
2233       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP2, 2);
2234       if (failing())  return;
2235     }
2236     // Loop opts pass for loop-unrolling before CCP
2237     if(major_progress() &amp;&amp; (loop_opts_cnt &gt; 0)) {
2238       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2239       PhaseIdealLoop ideal_loop( igvn, false );
2240       loop_opts_cnt--;
2241       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP3, 2);
2242     }
2243     if (!failing()) {
2244       // Verify that last round of loop opts produced a valid graph
2245       TracePhase tp("idealLoopVerify", &amp;timers[_t_idealLoopVerify]);
2246       PhaseIdealLoop::verify(igvn);
2247     }
2248   }
2249   if (failing())  return;
2250 
2251   // Conditional Constant Propagation;
2252   PhaseCCP ccp( &amp;igvn );
2253   assert( true, "Break here to ccp.dump_nodes_and_types(_root,999,1)");
2254   {
2255     TracePhase tp("ccp", &amp;timers[_t_ccp]);
2256     ccp.do_transform();
2257   }
2258   print_method(PHASE_CPP1, 2);
2259 
2260   assert( true, "Break here to ccp.dump_old2new_map()");
2261 
2262   // Iterative Global Value Numbering, including ideal transforms
2263   {
2264     TracePhase tp("iterGVN2", &amp;timers[_t_iterGVN2]);
2265     igvn = ccp;
2266     igvn.optimize();
2267   }
2268 
2269   print_method(PHASE_ITER_GVN2, 2);
2270 
2271   if (failing())  return;
2272 
2273   // Loop transforms on the ideal graph.  Range Check Elimination,
2274   // peeling, unrolling, etc.
2275   if(loop_opts_cnt &gt; 0) {
2276     debug_only( int cnt = 0; );
2277     while(major_progress() &amp;&amp; (loop_opts_cnt &gt; 0)) {
2278       TracePhase tp("idealLoop", &amp;timers[_t_idealLoop]);
2279       assert( cnt++ &lt; 40, "infinite cycle in loop optimization" );
2280       PhaseIdealLoop ideal_loop( igvn, true);
2281       loop_opts_cnt--;
2282       if (major_progress()) print_method(PHASE_PHASEIDEALLOOP_ITERATIONS, 2);
2283       if (failing())  return;
2284     }
2285   }
2286   // Ensure that major progress is now clear
2287   C-&gt;clear_major_progress();
2288 
2289   {
2290     // Verify that all previous optimizations produced a valid graph
2291     // at least to this point, even if no loop optimizations were done.
2292     TracePhase tp("idealLoopVerify", &amp;timers[_t_idealLoopVerify]);
2293     PhaseIdealLoop::verify(igvn);
2294   }
2295 
2296   if (range_check_cast_count() &gt; 0) {
2297     // No more loop optimizations. Remove all range check dependent CastIINodes.
2298     C-&gt;remove_range_check_casts(igvn);
2299     igvn.optimize();
2300   }
2301 
2302   {
2303     TracePhase tp("macroExpand", &amp;timers[_t_macroExpand]);
2304     PhaseMacroExpand  mex(igvn);
2305     if (mex.expand_macro_nodes()) {
2306       assert(failing(), "must bail out w/ explicit message");
2307       return;
2308     }
2309   }
2310 
2311   DEBUG_ONLY( _modified_nodes = NULL; )
2312  } // (End scope of igvn; run destructor if necessary for asserts.)
2313 
2314  process_print_inlining();
2315  // A method with only infinite loops has no edges entering loops from root
2316  {
2317    TracePhase tp("graphReshape", &amp;timers[_t_graphReshaping]);
2318    if (final_graph_reshaping()) {
2319      assert(failing(), "must bail out w/ explicit message");
2320      return;
2321    }
2322  }
2323 
2324  print_method(PHASE_OPTIMIZE_FINISHED, 2);
2325 }
2326 
2327 
2328 //------------------------------Code_Gen---------------------------------------
2329 // Given a graph, generate code for it
2330 void Compile::Code_Gen() {
2331   if (failing()) {
2332     return;
2333   }
2334 
2335   // Perform instruction selection.  You might think we could reclaim Matcher
2336   // memory PDQ, but actually the Matcher is used in generating spill code.
2337   // Internals of the Matcher (including some VectorSets) must remain live
2338   // for awhile - thus I cannot reclaim Matcher memory lest a VectorSet usage
2339   // set a bit in reclaimed memory.
2340 
2341   // In debug mode can dump m._nodes.dump() for mapping of ideal to machine
2342   // nodes.  Mapping is only valid at the root of each matched subtree.
2343   NOT_PRODUCT( verify_graph_edges(); )
2344 
2345   Matcher matcher;
2346   _matcher = &amp;matcher;
2347   {
2348     TracePhase tp("matcher", &amp;timers[_t_matcher]);
2349     matcher.match();
2350   }
2351   // In debug mode can dump m._nodes.dump() for mapping of ideal to machine
2352   // nodes.  Mapping is only valid at the root of each matched subtree.
2353   NOT_PRODUCT( verify_graph_edges(); )
2354 
2355   // If you have too many nodes, or if matching has failed, bail out
2356   check_node_count(0, "out of nodes matching instructions");
2357   if (failing()) {
2358     return;
2359   }
2360 
2361   // Build a proper-looking CFG
2362   PhaseCFG cfg(node_arena(), root(), matcher);
2363   _cfg = &amp;cfg;
2364   {
2365     TracePhase tp("scheduler", &amp;timers[_t_scheduler]);
2366     bool success = cfg.do_global_code_motion();
2367     if (!success) {
2368       return;
2369     }
2370 
2371     print_method(PHASE_GLOBAL_CODE_MOTION, 2);
2372     NOT_PRODUCT( verify_graph_edges(); )
2373     debug_only( cfg.verify(); )
2374   }
2375 
2376   PhaseChaitin regalloc(unique(), cfg, matcher, false);
2377   _regalloc = &amp;regalloc;
2378   {
2379     TracePhase tp("regalloc", &amp;timers[_t_registerAllocation]);
2380     // Perform register allocation.  After Chaitin, use-def chains are
2381     // no longer accurate (at spill code) and so must be ignored.
2382     // Node-&gt;LRG-&gt;reg mappings are still accurate.
2383     _regalloc-&gt;Register_Allocate();
2384 
2385     // Bail out if the allocator builds too many nodes
2386     if (failing()) {
2387       return;
2388     }
2389   }
2390 
2391   // Prior to register allocation we kept empty basic blocks in case the
2392   // the allocator needed a place to spill.  After register allocation we
2393   // are not adding any new instructions.  If any basic block is empty, we
2394   // can now safely remove it.
2395   {
2396     TracePhase tp("blockOrdering", &amp;timers[_t_blockOrdering]);
2397     cfg.remove_empty_blocks();
2398     if (do_freq_based_layout()) {
2399       PhaseBlockLayout layout(cfg);
2400     } else {
2401       cfg.set_loop_alignment();
2402     }
2403     cfg.fixup_flow();
2404   }
2405 
2406   // Apply peephole optimizations
2407   if( OptoPeephole ) {
2408     TracePhase tp("peephole", &amp;timers[_t_peephole]);
2409     PhasePeephole peep( _regalloc, cfg);
2410     peep.do_transform();
2411   }
2412 
2413   // Do late expand if CPU requires this.
2414   if (Matcher::require_postalloc_expand) {
2415     TracePhase tp("postalloc_expand", &amp;timers[_t_postalloc_expand]);
2416     cfg.postalloc_expand(_regalloc);
2417   }
2418 
2419   // Convert Nodes to instruction bits in a buffer
2420   {
2421     TraceTime tp("output", &amp;timers[_t_output], CITime);
2422     Output();
2423   }
2424 
2425   print_method(PHASE_FINAL_CODE);
2426 
2427   // He's dead, Jim.
2428   _cfg     = (PhaseCFG*)0xdeadbeef;
2429   _regalloc = (PhaseChaitin*)0xdeadbeef;
2430 }
2431 
2432 
2433 //------------------------------dump_asm---------------------------------------
2434 // Dump formatted assembly
2435 #ifndef PRODUCT
2436 void Compile::dump_asm(int *pcs, uint pc_limit) {
2437   bool cut_short = false;
2438   tty-&gt;print_cr("#");
2439   tty-&gt;print("#  ");  _tf-&gt;dump();  tty-&gt;cr();
2440   tty-&gt;print_cr("#");
2441 
2442   // For all blocks
2443   int pc = 0x0;                 // Program counter
2444   char starts_bundle = ' ';
2445   _regalloc-&gt;dump_frame();
2446 
2447   Node *n = NULL;
2448   for (uint i = 0; i &lt; _cfg-&gt;number_of_blocks(); i++) {
2449     if (VMThread::should_terminate()) {
2450       cut_short = true;
2451       break;
2452     }
2453     Block* block = _cfg-&gt;get_block(i);
2454     if (block-&gt;is_connector() &amp;&amp; !Verbose) {
2455       continue;
2456     }
2457     n = block-&gt;head();
2458     if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit) {
2459       tty-&gt;print("%3.3x   ", pcs[n-&gt;_idx]);
2460     } else {
2461       tty-&gt;print("      ");
2462     }
2463     block-&gt;dump_head(_cfg);
2464     if (block-&gt;is_connector()) {
2465       tty-&gt;print_cr("        # Empty connector block");
2466     } else if (block-&gt;num_preds() == 2 &amp;&amp; block-&gt;pred(1)-&gt;is_CatchProj() &amp;&amp; block-&gt;pred(1)-&gt;as_CatchProj()-&gt;_con == CatchProjNode::fall_through_index) {
2467       tty-&gt;print_cr("        # Block is sole successor of call");
2468     }
2469 
2470     // For all instructions
2471     Node *delay = NULL;
2472     for (uint j = 0; j &lt; block-&gt;number_of_nodes(); j++) {
2473       if (VMThread::should_terminate()) {
2474         cut_short = true;
2475         break;
2476       }
2477       n = block-&gt;get_node(j);
2478       if (valid_bundle_info(n)) {
2479         Bundle* bundle = node_bundling(n);
2480         if (bundle-&gt;used_in_unconditional_delay()) {
2481           delay = n;
2482           continue;
2483         }
2484         if (bundle-&gt;starts_bundle()) {
2485           starts_bundle = '+';
2486         }
2487       }
2488 
2489       if (WizardMode) {
2490         n-&gt;dump();
2491       }
2492 
2493       if( !n-&gt;is_Region() &amp;&amp;    // Dont print in the Assembly
2494           !n-&gt;is_Phi() &amp;&amp;       // a few noisely useless nodes
2495           !n-&gt;is_Proj() &amp;&amp;
2496           !n-&gt;is_MachTemp() &amp;&amp;
2497           !n-&gt;is_SafePointScalarObject() &amp;&amp;
2498           !n-&gt;is_Catch() &amp;&amp;     // Would be nice to print exception table targets
2499           !n-&gt;is_MergeMem() &amp;&amp;  // Not very interesting
2500           !n-&gt;is_top() &amp;&amp;       // Debug info table constants
2501           !(n-&gt;is_Con() &amp;&amp; !n-&gt;is_Mach())// Debug info table constants
2502           ) {
2503         if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit)
2504           tty-&gt;print("%3.3x", pcs[n-&gt;_idx]);
2505         else
2506           tty-&gt;print("   ");
2507         tty-&gt;print(" %c ", starts_bundle);
2508         starts_bundle = ' ';
2509         tty-&gt;print("\t");
2510         n-&gt;format(_regalloc, tty);
2511         tty-&gt;cr();
2512       }
2513 
2514       // If we have an instruction with a delay slot, and have seen a delay,
2515       // then back up and print it
2516       if (valid_bundle_info(n) &amp;&amp; node_bundling(n)-&gt;use_unconditional_delay()) {
2517         assert(delay != NULL, "no unconditional delay instruction");
2518         if (WizardMode) delay-&gt;dump();
2519 
2520         if (node_bundling(delay)-&gt;starts_bundle())
2521           starts_bundle = '+';
2522         if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit)
2523           tty-&gt;print("%3.3x", pcs[n-&gt;_idx]);
2524         else
2525           tty-&gt;print("   ");
2526         tty-&gt;print(" %c ", starts_bundle);
2527         starts_bundle = ' ';
2528         tty-&gt;print("\t");
2529         delay-&gt;format(_regalloc, tty);
2530         tty-&gt;cr();
2531         delay = NULL;
2532       }
2533 
2534       // Dump the exception table as well
2535       if( n-&gt;is_Catch() &amp;&amp; (Verbose || WizardMode) ) {
2536         // Print the exception table for this offset
2537         _handler_table.print_subtable_for(pc);
2538       }
2539     }
2540 
2541     if (pcs &amp;&amp; n-&gt;_idx &lt; pc_limit)
2542       tty-&gt;print_cr("%3.3x", pcs[n-&gt;_idx]);
2543     else
2544       tty-&gt;cr();
2545 
2546     assert(cut_short || delay == NULL, "no unconditional delay branch");
2547 
2548   } // End of per-block dump
2549   tty-&gt;cr();
2550 
2551   if (cut_short)  tty-&gt;print_cr("*** disassembly is cut short ***");
2552 }
2553 #endif
2554 
2555 //------------------------------Final_Reshape_Counts---------------------------
2556 // This class defines counters to help identify when a method
2557 // may/must be executed using hardware with only 24-bit precision.
2558 struct Final_Reshape_Counts : public StackObj {
2559   int  _call_count;             // count non-inlined 'common' calls
2560   int  _float_count;            // count float ops requiring 24-bit precision
2561   int  _double_count;           // count double ops requiring more precision
2562   int  _java_call_count;        // count non-inlined 'java' calls
2563   int  _inner_loop_count;       // count loops which need alignment
2564   VectorSet _visited;           // Visitation flags
2565   Node_List _tests;             // Set of IfNodes &amp; PCTableNodes
2566 
2567   Final_Reshape_Counts() :
2568     _call_count(0), _float_count(0), _double_count(0),
2569     _java_call_count(0), _inner_loop_count(0),
2570     _visited( Thread::current()-&gt;resource_area() ) { }
2571 
2572   void inc_call_count  () { _call_count  ++; }
2573   void inc_float_count () { _float_count ++; }
2574   void inc_double_count() { _double_count++; }
2575   void inc_java_call_count() { _java_call_count++; }
2576   void inc_inner_loop_count() { _inner_loop_count++; }
2577 
2578   int  get_call_count  () const { return _call_count  ; }
2579   int  get_float_count () const { return _float_count ; }
2580   int  get_double_count() const { return _double_count; }
2581   int  get_java_call_count() const { return _java_call_count; }
2582   int  get_inner_loop_count() const { return _inner_loop_count; }
2583 };
2584 
2585 #ifdef ASSERT
2586 static bool oop_offset_is_sane(const TypeInstPtr* tp) {
2587   ciInstanceKlass *k = tp-&gt;klass()-&gt;as_instance_klass();
2588   // Make sure the offset goes inside the instance layout.
2589   return k-&gt;contains_field_offset(tp-&gt;offset());
2590   // Note that OffsetBot and OffsetTop are very negative.
2591 }
2592 #endif
2593 
2594 // Eliminate trivially redundant StoreCMs and accumulate their
2595 // precedence edges.
2596 void Compile::eliminate_redundant_card_marks(Node* n) {
2597   assert(n-&gt;Opcode() == Op_StoreCM, "expected StoreCM");
2598   if (n-&gt;in(MemNode::Address)-&gt;outcnt() &gt; 1) {
2599     // There are multiple users of the same address so it might be
2600     // possible to eliminate some of the StoreCMs
2601     Node* mem = n-&gt;in(MemNode::Memory);
2602     Node* adr = n-&gt;in(MemNode::Address);
2603     Node* val = n-&gt;in(MemNode::ValueIn);
2604     Node* prev = n;
2605     bool done = false;
2606     // Walk the chain of StoreCMs eliminating ones that match.  As
2607     // long as it's a chain of single users then the optimization is
2608     // safe.  Eliminating partially redundant StoreCMs would require
2609     // cloning copies down the other paths.
2610     while (mem-&gt;Opcode() == Op_StoreCM &amp;&amp; mem-&gt;outcnt() == 1 &amp;&amp; !done) {
2611       if (adr == mem-&gt;in(MemNode::Address) &amp;&amp;
2612           val == mem-&gt;in(MemNode::ValueIn)) {
2613         // redundant StoreCM
2614         if (mem-&gt;req() &gt; MemNode::OopStore) {
2615           // Hasn't been processed by this code yet.
2616           n-&gt;add_prec(mem-&gt;in(MemNode::OopStore));
2617         } else {
2618           // Already converted to precedence edge
2619           for (uint i = mem-&gt;req(); i &lt; mem-&gt;len(); i++) {
2620             // Accumulate any precedence edges
2621             if (mem-&gt;in(i) != NULL) {
2622               n-&gt;add_prec(mem-&gt;in(i));
2623             }
2624           }
2625           // Everything above this point has been processed.
2626           done = true;
2627         }
2628         // Eliminate the previous StoreCM
2629         prev-&gt;set_req(MemNode::Memory, mem-&gt;in(MemNode::Memory));
2630         assert(mem-&gt;outcnt() == 0, "should be dead");
2631         mem-&gt;disconnect_inputs(NULL, this);
2632       } else {
2633         prev = mem;
2634       }
2635       mem = prev-&gt;in(MemNode::Memory);
2636     }
2637   }
2638 }
2639 
2640 //------------------------------final_graph_reshaping_impl----------------------
2641 // Implement items 1-5 from final_graph_reshaping below.
2642 void Compile::final_graph_reshaping_impl( Node *n, Final_Reshape_Counts &amp;frc) {
2643 
2644   if ( n-&gt;outcnt() == 0 ) return; // dead node
2645   uint nop = n-&gt;Opcode();
2646 
2647   // Check for 2-input instruction with "last use" on right input.
2648   // Swap to left input.  Implements item (2).
2649   if( n-&gt;req() == 3 &amp;&amp;          // two-input instruction
2650       n-&gt;in(1)-&gt;outcnt() &gt; 1 &amp;&amp; // left use is NOT a last use
2651       (!n-&gt;in(1)-&gt;is_Phi() || n-&gt;in(1)-&gt;in(2) != n) &amp;&amp; // it is not data loop
2652       n-&gt;in(2)-&gt;outcnt() == 1 &amp;&amp;// right use IS a last use
2653       !n-&gt;in(2)-&gt;is_Con() ) {   // right use is not a constant
2654     // Check for commutative opcode
2655     switch( nop ) {
2656     case Op_AddI:  case Op_AddF:  case Op_AddD:  case Op_AddL:
2657     case Op_MaxI:  case Op_MinI:
2658     case Op_MulI:  case Op_MulF:  case Op_MulD:  case Op_MulL:
2659     case Op_AndL:  case Op_XorL:  case Op_OrL:
2660     case Op_AndI:  case Op_XorI:  case Op_OrI: {
2661       // Move "last use" input to left by swapping inputs
2662       n-&gt;swap_edges(1, 2);
2663       break;
2664     }
2665     default:
2666       break;
2667     }
2668   }
2669 
2670 #ifdef ASSERT
2671   if( n-&gt;is_Mem() ) {
2672     int alias_idx = get_alias_index(n-&gt;as_Mem()-&gt;adr_type());
2673     assert( n-&gt;in(0) != NULL || alias_idx != Compile::AliasIdxRaw ||
2674             // oop will be recorded in oop map if load crosses safepoint
2675             n-&gt;is_Load() &amp;&amp; (n-&gt;as_Load()-&gt;bottom_type()-&gt;isa_oopptr() ||
2676                              LoadNode::is_immutable_value(n-&gt;in(MemNode::Address))),
2677             "raw memory operations should have control edge");
2678   }
2679 #endif
2680   // Count FPU ops and common calls, implements item (3)
2681   switch( nop ) {
2682   // Count all float operations that may use FPU
2683   case Op_AddF:
2684   case Op_SubF:
2685   case Op_MulF:
2686   case Op_DivF:
2687   case Op_NegF:
2688   case Op_ModF:
2689   case Op_ConvI2F:
2690   case Op_ConF:
2691   case Op_CmpF:
2692   case Op_CmpF3:
2693   // case Op_ConvL2F: // longs are split into 32-bit halves
2694     frc.inc_float_count();
2695     break;
2696 
2697   case Op_ConvF2D:
2698   case Op_ConvD2F:
2699     frc.inc_float_count();
2700     frc.inc_double_count();
2701     break;
2702 
2703   // Count all double operations that may use FPU
2704   case Op_AddD:
2705   case Op_SubD:
2706   case Op_MulD:
2707   case Op_DivD:
2708   case Op_NegD:
2709   case Op_ModD:
2710   case Op_ConvI2D:
2711   case Op_ConvD2I:
2712   // case Op_ConvL2D: // handled by leaf call
2713   // case Op_ConvD2L: // handled by leaf call
2714   case Op_ConD:
2715   case Op_CmpD:
2716   case Op_CmpD3:
2717     frc.inc_double_count();
2718     break;
2719   case Op_Opaque1:              // Remove Opaque Nodes before matching
2720   case Op_Opaque2:              // Remove Opaque Nodes before matching
2721   case Op_Opaque3:
2722     n-&gt;subsume_by(n-&gt;in(1), this);
2723     break;
2724   case Op_CallStaticJava:
2725   case Op_CallJava:
2726   case Op_CallDynamicJava:
2727     frc.inc_java_call_count(); // Count java call site;
2728   case Op_CallRuntime:
2729   case Op_CallLeaf:
2730   case Op_CallLeafNoFP: {
2731     assert( n-&gt;is_Call(), "" );
2732     CallNode *call = n-&gt;as_Call();
2733     // Count call sites where the FP mode bit would have to be flipped.
2734     // Do not count uncommon runtime calls:
2735     // uncommon_trap, _complete_monitor_locking, _complete_monitor_unlocking,
2736     // _new_Java, _new_typeArray, _new_objArray, _rethrow_Java, ...
2737     if( !call-&gt;is_CallStaticJava() || !call-&gt;as_CallStaticJava()-&gt;_name ) {
2738       frc.inc_call_count();   // Count the call site
2739     } else {                  // See if uncommon argument is shared
2740       Node *n = call-&gt;in(TypeFunc::Parms);
2741       int nop = n-&gt;Opcode();
2742       // Clone shared simple arguments to uncommon calls, item (1).
2743       if( n-&gt;outcnt() &gt; 1 &amp;&amp;
2744           !n-&gt;is_Proj() &amp;&amp;
2745           nop != Op_CreateEx &amp;&amp;
2746           nop != Op_CheckCastPP &amp;&amp;
2747           nop != Op_DecodeN &amp;&amp;
2748           nop != Op_DecodeNKlass &amp;&amp;
2749           !n-&gt;is_Mem() ) {
2750         Node *x = n-&gt;clone();
2751         call-&gt;set_req( TypeFunc::Parms, x );
2752       }
2753     }
2754     break;
2755   }
2756 
2757   case Op_StoreD:
2758   case Op_LoadD:
2759   case Op_LoadD_unaligned:
2760     frc.inc_double_count();
2761     goto handle_mem;
2762   case Op_StoreF:
2763   case Op_LoadF:
2764     frc.inc_float_count();
2765     goto handle_mem;
2766 
2767   case Op_StoreCM:
2768     {
2769       // Convert OopStore dependence into precedence edge
2770       Node* prec = n-&gt;in(MemNode::OopStore);
2771       n-&gt;del_req(MemNode::OopStore);
2772       n-&gt;add_prec(prec);
2773       eliminate_redundant_card_marks(n);
2774     }
2775 
2776     // fall through
2777 
2778   case Op_StoreB:
2779   case Op_StoreC:
2780   case Op_StorePConditional:
2781   case Op_StoreI:
2782   case Op_StoreL:
2783   case Op_StoreIConditional:
2784   case Op_StoreLConditional:
2785   case Op_CompareAndSwapI:
2786   case Op_CompareAndSwapL:
2787   case Op_CompareAndSwapP:
2788   case Op_CompareAndSwapN:
2789   case Op_WeakCompareAndSwapI:
2790   case Op_WeakCompareAndSwapL:
2791   case Op_WeakCompareAndSwapP:
2792   case Op_WeakCompareAndSwapN:
2793   case Op_CompareAndExchangeI:
2794   case Op_CompareAndExchangeL:
2795   case Op_CompareAndExchangeP:
2796   case Op_CompareAndExchangeN:
2797   case Op_GetAndAddI:
2798   case Op_GetAndAddL:
2799   case Op_GetAndSetI:
2800   case Op_GetAndSetL:
2801   case Op_GetAndSetP:
2802   case Op_GetAndSetN:
2803   case Op_StoreP:
2804   case Op_StoreN:
2805   case Op_StoreNKlass:
2806   case Op_LoadB:
2807   case Op_LoadUB:
2808   case Op_LoadUS:
2809   case Op_LoadI:
2810   case Op_LoadKlass:
2811   case Op_LoadNKlass:
2812   case Op_LoadL:
2813   case Op_LoadL_unaligned:
2814   case Op_LoadPLocked:
2815   case Op_LoadP:
2816   case Op_LoadN:
2817   case Op_LoadRange:
2818   case Op_LoadS: {
2819   handle_mem:
2820 #ifdef ASSERT
2821     if( VerifyOptoOopOffsets ) {
2822       assert( n-&gt;is_Mem(), "" );
2823       MemNode *mem  = (MemNode*)n;
2824       // Check to see if address types have grounded out somehow.
2825       const TypeInstPtr *tp = mem-&gt;in(MemNode::Address)-&gt;bottom_type()-&gt;isa_instptr();
2826       assert( !tp || oop_offset_is_sane(tp), "" );
2827     }
2828 #endif
2829     break;
2830   }
2831 
2832   case Op_AddP: {               // Assert sane base pointers
2833     Node *addp = n-&gt;in(AddPNode::Address);
2834     assert( !addp-&gt;is_AddP() ||
2835             addp-&gt;in(AddPNode::Base)-&gt;is_top() || // Top OK for allocation
2836             addp-&gt;in(AddPNode::Base) == n-&gt;in(AddPNode::Base),
2837             "Base pointers must match" );
2838 #ifdef _LP64
2839     if ((UseCompressedOops || UseCompressedClassPointers) &amp;&amp;
2840         addp-&gt;Opcode() == Op_ConP &amp;&amp;
2841         addp == n-&gt;in(AddPNode::Base) &amp;&amp;
2842         n-&gt;in(AddPNode::Offset)-&gt;is_Con()) {
2843       // Use addressing with narrow klass to load with offset on x86.
2844       // On sparc loading 32-bits constant and decoding it have less
2845       // instructions (4) then load 64-bits constant (7).
2846       // Do this transformation here since IGVN will convert ConN back to ConP.
2847       const Type* t = addp-&gt;bottom_type();
2848       if (t-&gt;isa_oopptr() || t-&gt;isa_klassptr()) {
2849         Node* nn = NULL;
2850 
2851         int op = t-&gt;isa_oopptr() ? Op_ConN : Op_ConNKlass;
2852 
2853         // Look for existing ConN node of the same exact type.
2854         Node* r  = root();
2855         uint cnt = r-&gt;outcnt();
2856         for (uint i = 0; i &lt; cnt; i++) {
2857           Node* m = r-&gt;raw_out(i);
2858           if (m!= NULL &amp;&amp; m-&gt;Opcode() == op &amp;&amp;
2859               m-&gt;bottom_type()-&gt;make_ptr() == t) {
2860             nn = m;
2861             break;
2862           }
2863         }
2864         if (nn != NULL) {
2865           // Decode a narrow oop to match address
2866           // [R12 + narrow_oop_reg&lt;&lt;3 + offset]
2867           if (t-&gt;isa_oopptr()) {
2868             nn = new DecodeNNode(nn, t);
2869           } else {
2870             nn = new DecodeNKlassNode(nn, t);
2871           }
2872           n-&gt;set_req(AddPNode::Base, nn);
2873           n-&gt;set_req(AddPNode::Address, nn);
2874           if (addp-&gt;outcnt() == 0) {
2875             addp-&gt;disconnect_inputs(NULL, this);
2876           }
2877         }
2878       }
2879     }
2880 #endif
2881     break;
2882   }
2883 
2884   case Op_CastPP: {
2885     // Remove CastPP nodes to gain more freedom during scheduling but
2886     // keep the dependency they encode as control or precedence edges
2887     // (if control is set already) on memory operations. Some CastPP
2888     // nodes don't have a control (don't carry a dependency): skip
2889     // those.
2890     if (n-&gt;in(0) != NULL) {
2891       ResourceMark rm;
2892       Unique_Node_List wq;
2893       wq.push(n);
2894       for (uint next = 0; next &lt; wq.size(); ++next) {
2895         Node *m = wq.at(next);
2896         for (DUIterator_Fast imax, i = m-&gt;fast_outs(imax); i &lt; imax; i++) {
2897           Node* use = m-&gt;fast_out(i);
2898           if (use-&gt;is_Mem() || use-&gt;is_EncodeNarrowPtr()) {
2899             use-&gt;ensure_control_or_add_prec(n-&gt;in(0));
2900           } else {
2901             switch(use-&gt;Opcode()) {
2902             case Op_AddP:
2903             case Op_DecodeN:
2904             case Op_DecodeNKlass:
2905             case Op_CheckCastPP:
2906             case Op_CastPP:
2907               wq.push(use);
2908               break;
2909             }
2910           }
2911         }
2912       }
2913     }
2914     const bool is_LP64 = LP64_ONLY(true) NOT_LP64(false);
2915     if (is_LP64 &amp;&amp; n-&gt;in(1)-&gt;is_DecodeN() &amp;&amp; Matcher::gen_narrow_oop_implicit_null_checks()) {
2916       Node* in1 = n-&gt;in(1);
2917       const Type* t = n-&gt;bottom_type();
2918       Node* new_in1 = in1-&gt;clone();
2919       new_in1-&gt;as_DecodeN()-&gt;set_type(t);
2920 
2921       if (!Matcher::narrow_oop_use_complex_address()) {
2922         //
2923         // x86, ARM and friends can handle 2 adds in addressing mode
2924         // and Matcher can fold a DecodeN node into address by using
2925         // a narrow oop directly and do implicit NULL check in address:
2926         //
2927         // [R12 + narrow_oop_reg&lt;&lt;3 + offset]
2928         // NullCheck narrow_oop_reg
2929         //
2930         // On other platforms (Sparc) we have to keep new DecodeN node and
2931         // use it to do implicit NULL check in address:
2932         //
2933         // decode_not_null narrow_oop_reg, base_reg
2934         // [base_reg + offset]
2935         // NullCheck base_reg
2936         //
2937         // Pin the new DecodeN node to non-null path on these platform (Sparc)
2938         // to keep the information to which NULL check the new DecodeN node
2939         // corresponds to use it as value in implicit_null_check().
2940         //
2941         new_in1-&gt;set_req(0, n-&gt;in(0));
2942       }
2943 
2944       n-&gt;subsume_by(new_in1, this);
2945       if (in1-&gt;outcnt() == 0) {
2946         in1-&gt;disconnect_inputs(NULL, this);
2947       }
2948     } else {
2949       n-&gt;subsume_by(n-&gt;in(1), this);
2950       if (n-&gt;outcnt() == 0) {
2951         n-&gt;disconnect_inputs(NULL, this);
2952       }
2953     }
2954     break;
2955   }
2956 #ifdef _LP64
2957   case Op_CmpP:
2958     // Do this transformation here to preserve CmpPNode::sub() and
2959     // other TypePtr related Ideal optimizations (for example, ptr nullness).
2960     if (n-&gt;in(1)-&gt;is_DecodeNarrowPtr() || n-&gt;in(2)-&gt;is_DecodeNarrowPtr()) {
2961       Node* in1 = n-&gt;in(1);
2962       Node* in2 = n-&gt;in(2);
2963       if (!in1-&gt;is_DecodeNarrowPtr()) {
2964         in2 = in1;
2965         in1 = n-&gt;in(2);
2966       }
2967       assert(in1-&gt;is_DecodeNarrowPtr(), "sanity");
2968 
2969       Node* new_in2 = NULL;
2970       if (in2-&gt;is_DecodeNarrowPtr()) {
2971         assert(in2-&gt;Opcode() == in1-&gt;Opcode(), "must be same node type");
2972         new_in2 = in2-&gt;in(1);
2973       } else if (in2-&gt;Opcode() == Op_ConP) {
2974         const Type* t = in2-&gt;bottom_type();
2975         if (t == TypePtr::NULL_PTR) {
2976           assert(in1-&gt;is_DecodeN(), "compare klass to null?");
2977           // Don't convert CmpP null check into CmpN if compressed
2978           // oops implicit null check is not generated.
2979           // This will allow to generate normal oop implicit null check.
2980           if (Matcher::gen_narrow_oop_implicit_null_checks())
2981             new_in2 = ConNode::make(TypeNarrowOop::NULL_PTR);
2982           //
2983           // This transformation together with CastPP transformation above
2984           // will generated code for implicit NULL checks for compressed oops.
2985           //
2986           // The original code after Optimize()
2987           //
2988           //    LoadN memory, narrow_oop_reg
2989           //    decode narrow_oop_reg, base_reg
2990           //    CmpP base_reg, NULL
2991           //    CastPP base_reg // NotNull
2992           //    Load [base_reg + offset], val_reg
2993           //
2994           // after these transformations will be
2995           //
2996           //    LoadN memory, narrow_oop_reg
2997           //    CmpN narrow_oop_reg, NULL
2998           //    decode_not_null narrow_oop_reg, base_reg
2999           //    Load [base_reg + offset], val_reg
3000           //
3001           // and the uncommon path (== NULL) will use narrow_oop_reg directly
3002           // since narrow oops can be used in debug info now (see the code in
3003           // final_graph_reshaping_walk()).
3004           //
3005           // At the end the code will be matched to
3006           // on x86:
3007           //
3008           //    Load_narrow_oop memory, narrow_oop_reg
3009           //    Load [R12 + narrow_oop_reg&lt;&lt;3 + offset], val_reg
3010           //    NullCheck narrow_oop_reg
3011           //
3012           // and on sparc:
3013           //
3014           //    Load_narrow_oop memory, narrow_oop_reg
3015           //    decode_not_null narrow_oop_reg, base_reg
3016           //    Load [base_reg + offset], val_reg
3017           //    NullCheck base_reg
3018           //
3019         } else if (t-&gt;isa_oopptr()) {
3020           new_in2 = ConNode::make(t-&gt;make_narrowoop());
3021         } else if (t-&gt;isa_klassptr()) {
3022           new_in2 = ConNode::make(t-&gt;make_narrowklass());
3023         }
3024       }
3025       if (new_in2 != NULL) {
3026         Node* cmpN = new CmpNNode(in1-&gt;in(1), new_in2);
3027         n-&gt;subsume_by(cmpN, this);
3028         if (in1-&gt;outcnt() == 0) {
3029           in1-&gt;disconnect_inputs(NULL, this);
3030         }
3031         if (in2-&gt;outcnt() == 0) {
3032           in2-&gt;disconnect_inputs(NULL, this);
3033         }
3034       }
3035     }
3036     break;
3037 
3038   case Op_DecodeN:
3039   case Op_DecodeNKlass:
3040     assert(!n-&gt;in(1)-&gt;is_EncodeNarrowPtr(), "should be optimized out");
3041     // DecodeN could be pinned when it can't be fold into
3042     // an address expression, see the code for Op_CastPP above.
3043     assert(n-&gt;in(0) == NULL || (UseCompressedOops &amp;&amp; !Matcher::narrow_oop_use_complex_address()), "no control");
3044     break;
3045 
3046   case Op_EncodeP:
3047   case Op_EncodePKlass: {
3048     Node* in1 = n-&gt;in(1);
3049     if (in1-&gt;is_DecodeNarrowPtr()) {
3050       n-&gt;subsume_by(in1-&gt;in(1), this);
3051     } else if (in1-&gt;Opcode() == Op_ConP) {
3052       const Type* t = in1-&gt;bottom_type();
3053       if (t == TypePtr::NULL_PTR) {
3054         assert(t-&gt;isa_oopptr(), "null klass?");
3055         n-&gt;subsume_by(ConNode::make(TypeNarrowOop::NULL_PTR), this);
3056       } else if (t-&gt;isa_oopptr()) {
3057         n-&gt;subsume_by(ConNode::make(t-&gt;make_narrowoop()), this);
3058       } else if (t-&gt;isa_klassptr()) {
3059         n-&gt;subsume_by(ConNode::make(t-&gt;make_narrowklass()), this);
3060       }
3061     }
3062     if (in1-&gt;outcnt() == 0) {
3063       in1-&gt;disconnect_inputs(NULL, this);
3064     }
3065     break;
3066   }
3067 
3068   case Op_Proj: {
3069     if (OptimizeStringConcat) {
3070       ProjNode* p = n-&gt;as_Proj();
3071       if (p-&gt;_is_io_use) {
3072         // Separate projections were used for the exception path which
3073         // are normally removed by a late inline.  If it wasn't inlined
3074         // then they will hang around and should just be replaced with
3075         // the original one.
3076         Node* proj = NULL;
3077         // Replace with just one
3078         for (SimpleDUIterator i(p-&gt;in(0)); i.has_next(); i.next()) {
3079           Node *use = i.get();
3080           if (use-&gt;is_Proj() &amp;&amp; p != use &amp;&amp; use-&gt;as_Proj()-&gt;_con == p-&gt;_con) {
3081             proj = use;
3082             break;
3083           }
3084         }
3085         assert(proj != NULL, "must be found");
3086         p-&gt;subsume_by(proj, this);
3087       }
3088     }
3089     break;
3090   }
3091 
3092   case Op_Phi:
3093     if (n-&gt;as_Phi()-&gt;bottom_type()-&gt;isa_narrowoop() || n-&gt;as_Phi()-&gt;bottom_type()-&gt;isa_narrowklass()) {
3094       // The EncodeP optimization may create Phi with the same edges
3095       // for all paths. It is not handled well by Register Allocator.
3096       Node* unique_in = n-&gt;in(1);
3097       assert(unique_in != NULL, "");
3098       uint cnt = n-&gt;req();
3099       for (uint i = 2; i &lt; cnt; i++) {
3100         Node* m = n-&gt;in(i);
3101         assert(m != NULL, "");
3102         if (unique_in != m)
3103           unique_in = NULL;
3104       }
3105       if (unique_in != NULL) {
3106         n-&gt;subsume_by(unique_in, this);
3107       }
3108     }
3109     break;
3110 
3111 #endif
3112 
3113 #ifdef ASSERT
3114   case Op_CastII:
3115     // Verify that all range check dependent CastII nodes were removed.
3116     if (n-&gt;isa_CastII()-&gt;has_range_check()) {
3117       n-&gt;dump(3);
3118       assert(false, "Range check dependent CastII node was not removed");
3119     }
3120     break;
3121 #endif
3122 
3123   case Op_ModI:
3124     if (UseDivMod) {
3125       // Check if a%b and a/b both exist
3126       Node* d = n-&gt;find_similar(Op_DivI);
3127       if (d) {
3128         // Replace them with a fused divmod if supported
3129         if (Matcher::has_match_rule(Op_DivModI)) {
3130           DivModINode* divmod = DivModINode::make(n);
3131           d-&gt;subsume_by(divmod-&gt;div_proj(), this);
3132           n-&gt;subsume_by(divmod-&gt;mod_proj(), this);
3133         } else {
3134           // replace a%b with a-((a/b)*b)
3135           Node* mult = new MulINode(d, d-&gt;in(2));
3136           Node* sub  = new SubINode(d-&gt;in(1), mult);
3137           n-&gt;subsume_by(sub, this);
3138         }
3139       }
3140     }
3141     break;
3142 
3143   case Op_ModL:
3144     if (UseDivMod) {
3145       // Check if a%b and a/b both exist
3146       Node* d = n-&gt;find_similar(Op_DivL);
3147       if (d) {
3148         // Replace them with a fused divmod if supported
3149         if (Matcher::has_match_rule(Op_DivModL)) {
3150           DivModLNode* divmod = DivModLNode::make(n);
3151           d-&gt;subsume_by(divmod-&gt;div_proj(), this);
3152           n-&gt;subsume_by(divmod-&gt;mod_proj(), this);
3153         } else {
3154           // replace a%b with a-((a/b)*b)
3155           Node* mult = new MulLNode(d, d-&gt;in(2));
3156           Node* sub  = new SubLNode(d-&gt;in(1), mult);
3157           n-&gt;subsume_by(sub, this);
3158         }
3159       }
3160     }
3161     break;
3162 
3163   case Op_LoadVector:
3164   case Op_StoreVector:
3165     break;
3166 
3167   case Op_AddReductionVI:
3168   case Op_AddReductionVL:
3169   case Op_AddReductionVF:
3170   case Op_AddReductionVD:
3171   case Op_MulReductionVI:
3172   case Op_MulReductionVL:
3173   case Op_MulReductionVF:
3174   case Op_MulReductionVD:
3175     break;
3176 
3177   case Op_PackB:
3178   case Op_PackS:
3179   case Op_PackI:
3180   case Op_PackF:
3181   case Op_PackL:
3182   case Op_PackD:
3183     if (n-&gt;req()-1 &gt; 2) {
3184       // Replace many operand PackNodes with a binary tree for matching
3185       PackNode* p = (PackNode*) n;
3186       Node* btp = p-&gt;binary_tree_pack(1, n-&gt;req());
3187       n-&gt;subsume_by(btp, this);
3188     }
3189     break;
3190   case Op_Loop:
3191   case Op_CountedLoop:
3192     if (n-&gt;as_Loop()-&gt;is_inner_loop()) {
3193       frc.inc_inner_loop_count();
3194     }
3195     break;
3196   case Op_LShiftI:
3197   case Op_RShiftI:
3198   case Op_URShiftI:
3199   case Op_LShiftL:
3200   case Op_RShiftL:
3201   case Op_URShiftL:
3202     if (Matcher::need_masked_shift_count) {
3203       // The cpu's shift instructions don't restrict the count to the
3204       // lower 5/6 bits. We need to do the masking ourselves.
3205       Node* in2 = n-&gt;in(2);
3206       juint mask = (n-&gt;bottom_type() == TypeInt::INT) ? (BitsPerInt - 1) : (BitsPerLong - 1);
3207       const TypeInt* t = in2-&gt;find_int_type();
3208       if (t != NULL &amp;&amp; t-&gt;is_con()) {
3209         juint shift = t-&gt;get_con();
3210         if (shift &gt; mask) { // Unsigned cmp
3211           n-&gt;set_req(2, ConNode::make(TypeInt::make(shift &amp; mask)));
3212         }
3213       } else {
3214         if (t == NULL || t-&gt;_lo &lt; 0 || t-&gt;_hi &gt; (int)mask) {
3215           Node* shift = new AndINode(in2, ConNode::make(TypeInt::make(mask)));
3216           n-&gt;set_req(2, shift);
3217         }
3218       }
3219       if (in2-&gt;outcnt() == 0) { // Remove dead node
3220         in2-&gt;disconnect_inputs(NULL, this);
3221       }
3222     }
3223     break;
3224   case Op_MemBarStoreStore:
3225   case Op_MemBarRelease:
3226     // Break the link with AllocateNode: it is no longer useful and
3227     // confuses register allocation.
3228     if (n-&gt;req() &gt; MemBarNode::Precedent) {
3229       n-&gt;set_req(MemBarNode::Precedent, top());
3230     }
3231     break;
3232   case Op_RangeCheck: {
3233     RangeCheckNode* rc = n-&gt;as_RangeCheck();
3234     Node* iff = new IfNode(rc-&gt;in(0), rc-&gt;in(1), rc-&gt;_prob, rc-&gt;_fcnt);
3235     n-&gt;subsume_by(iff, this);
3236     frc._tests.push(iff);
3237     break;
3238   }
3239   default:
3240     assert( !n-&gt;is_Call(), "" );
3241     assert( !n-&gt;is_Mem(), "" );
3242     assert( nop != Op_ProfileBoolean, "should be eliminated during IGVN");
3243     break;
3244   }
3245 
3246   // Collect CFG split points
3247   if (n-&gt;is_MultiBranch() &amp;&amp; !n-&gt;is_RangeCheck()) {
3248     frc._tests.push(n);
3249   }
3250 }
3251 
3252 //------------------------------final_graph_reshaping_walk---------------------
3253 // Replacing Opaque nodes with their input in final_graph_reshaping_impl(),
3254 // requires that the walk visits a node's inputs before visiting the node.
3255 void Compile::final_graph_reshaping_walk( Node_Stack &amp;nstack, Node *root, Final_Reshape_Counts &amp;frc ) {
3256   ResourceArea *area = Thread::current()-&gt;resource_area();
3257   Unique_Node_List sfpt(area);
3258 
3259   frc._visited.set(root-&gt;_idx); // first, mark node as visited
3260   uint cnt = root-&gt;req();
3261   Node *n = root;
3262   uint  i = 0;
3263   while (true) {
3264     if (i &lt; cnt) {
3265       // Place all non-visited non-null inputs onto stack
3266       Node* m = n-&gt;in(i);
3267       ++i;
3268       if (m != NULL &amp;&amp; !frc._visited.test_set(m-&gt;_idx)) {
3269         if (m-&gt;is_SafePoint() &amp;&amp; m-&gt;as_SafePoint()-&gt;jvms() != NULL) {
3270           // compute worst case interpreter size in case of a deoptimization
3271           update_interpreter_frame_size(m-&gt;as_SafePoint()-&gt;jvms()-&gt;interpreter_frame_size());
3272 
3273           sfpt.push(m);
3274         }
3275         cnt = m-&gt;req();
3276         nstack.push(n, i); // put on stack parent and next input's index
3277         n = m;
3278         i = 0;
3279       }
3280     } else {
3281       // Now do post-visit work
3282       final_graph_reshaping_impl( n, frc );
3283       if (nstack.is_empty())
3284         break;             // finished
3285       n = nstack.node();   // Get node from stack
3286       cnt = n-&gt;req();
3287       i = nstack.index();
3288       nstack.pop();        // Shift to the next node on stack
3289     }
3290   }
3291 
3292   // Skip next transformation if compressed oops are not used.
3293   if ((UseCompressedOops &amp;&amp; !Matcher::gen_narrow_oop_implicit_null_checks()) ||
3294       (!UseCompressedOops &amp;&amp; !UseCompressedClassPointers))
3295     return;
3296 
3297   // Go over safepoints nodes to skip DecodeN/DecodeNKlass nodes for debug edges.
3298   // It could be done for an uncommon traps or any safepoints/calls
3299   // if the DecodeN/DecodeNKlass node is referenced only in a debug info.
3300   while (sfpt.size() &gt; 0) {
3301     n = sfpt.pop();
3302     JVMState *jvms = n-&gt;as_SafePoint()-&gt;jvms();
3303     assert(jvms != NULL, "sanity");
3304     int start = jvms-&gt;debug_start();
3305     int end   = n-&gt;req();
3306     bool is_uncommon = (n-&gt;is_CallStaticJava() &amp;&amp;
3307                         n-&gt;as_CallStaticJava()-&gt;uncommon_trap_request() != 0);
3308     for (int j = start; j &lt; end; j++) {
3309       Node* in = n-&gt;in(j);
3310       if (in-&gt;is_DecodeNarrowPtr()) {
3311         bool safe_to_skip = true;
3312         if (!is_uncommon ) {
3313           // Is it safe to skip?
3314           for (uint i = 0; i &lt; in-&gt;outcnt(); i++) {
3315             Node* u = in-&gt;raw_out(i);
3316             if (!u-&gt;is_SafePoint() ||
3317                  u-&gt;is_Call() &amp;&amp; u-&gt;as_Call()-&gt;has_non_debug_use(n)) {
3318               safe_to_skip = false;
3319             }
3320           }
3321         }
3322         if (safe_to_skip) {
3323           n-&gt;set_req(j, in-&gt;in(1));
3324         }
3325         if (in-&gt;outcnt() == 0) {
3326           in-&gt;disconnect_inputs(NULL, this);
3327         }
3328       }
3329     }
3330   }
3331 }
3332 
3333 //------------------------------final_graph_reshaping--------------------------
3334 // Final Graph Reshaping.
3335 //
3336 // (1) Clone simple inputs to uncommon calls, so they can be scheduled late
3337 //     and not commoned up and forced early.  Must come after regular
3338 //     optimizations to avoid GVN undoing the cloning.  Clone constant
3339 //     inputs to Loop Phis; these will be split by the allocator anyways.
3340 //     Remove Opaque nodes.
3341 // (2) Move last-uses by commutative operations to the left input to encourage
3342 //     Intel update-in-place two-address operations and better register usage
3343 //     on RISCs.  Must come after regular optimizations to avoid GVN Ideal
3344 //     calls canonicalizing them back.
3345 // (3) Count the number of double-precision FP ops, single-precision FP ops
3346 //     and call sites.  On Intel, we can get correct rounding either by
3347 //     forcing singles to memory (requires extra stores and loads after each
3348 //     FP bytecode) or we can set a rounding mode bit (requires setting and
3349 //     clearing the mode bit around call sites).  The mode bit is only used
3350 //     if the relative frequency of single FP ops to calls is low enough.
3351 //     This is a key transform for SPEC mpeg_audio.
3352 // (4) Detect infinite loops; blobs of code reachable from above but not
3353 //     below.  Several of the Code_Gen algorithms fail on such code shapes,
3354 //     so we simply bail out.  Happens a lot in ZKM.jar, but also happens
3355 //     from time to time in other codes (such as -Xcomp finalizer loops, etc).
3356 //     Detection is by looking for IfNodes where only 1 projection is
3357 //     reachable from below or CatchNodes missing some targets.
3358 // (5) Assert for insane oop offsets in debug mode.
3359 
3360 bool Compile::final_graph_reshaping() {
3361   // an infinite loop may have been eliminated by the optimizer,
3362   // in which case the graph will be empty.
3363   if (root()-&gt;req() == 1) {
3364     record_method_not_compilable("trivial infinite loop");
3365     return true;
3366   }
3367 
3368   // Expensive nodes have their control input set to prevent the GVN
3369   // from freely commoning them. There's no GVN beyond this point so
3370   // no need to keep the control input. We want the expensive nodes to
3371   // be freely moved to the least frequent code path by gcm.
3372   assert(OptimizeExpensiveOps || expensive_count() == 0, "optimization off but list non empty?");
3373   for (int i = 0; i &lt; expensive_count(); i++) {
3374     _expensive_nodes-&gt;at(i)-&gt;set_req(0, NULL);
3375   }
3376 
3377   Final_Reshape_Counts frc;
3378 
3379   // Visit everybody reachable!
3380   // Allocate stack of size C-&gt;live_nodes()/2 to avoid frequent realloc
3381   Node_Stack nstack(live_nodes() &gt;&gt; 1);
3382   final_graph_reshaping_walk(nstack, root(), frc);
3383 
3384   // Check for unreachable (from below) code (i.e., infinite loops).
3385   for( uint i = 0; i &lt; frc._tests.size(); i++ ) {
3386     MultiBranchNode *n = frc._tests[i]-&gt;as_MultiBranch();
3387     // Get number of CFG targets.
3388     // Note that PCTables include exception targets after calls.
3389     uint required_outcnt = n-&gt;required_outcnt();
3390     if (n-&gt;outcnt() != required_outcnt) {
3391       // Check for a few special cases.  Rethrow Nodes never take the
3392       // 'fall-thru' path, so expected kids is 1 less.
3393       if (n-&gt;is_PCTable() &amp;&amp; n-&gt;in(0) &amp;&amp; n-&gt;in(0)-&gt;in(0)) {
3394         if (n-&gt;in(0)-&gt;in(0)-&gt;is_Call()) {
3395           CallNode *call = n-&gt;in(0)-&gt;in(0)-&gt;as_Call();
3396           if (call-&gt;entry_point() == OptoRuntime::rethrow_stub()) {
3397             required_outcnt--;      // Rethrow always has 1 less kid
3398           } else if (call-&gt;req() &gt; TypeFunc::Parms &amp;&amp;
3399                      call-&gt;is_CallDynamicJava()) {
3400             // Check for null receiver. In such case, the optimizer has
3401             // detected that the virtual call will always result in a null
3402             // pointer exception. The fall-through projection of this CatchNode
3403             // will not be populated.
3404             Node *arg0 = call-&gt;in(TypeFunc::Parms);
3405             if (arg0-&gt;is_Type() &amp;&amp;
3406                 arg0-&gt;as_Type()-&gt;type()-&gt;higher_equal(TypePtr::NULL_PTR)) {
3407               required_outcnt--;
3408             }
3409           } else if (call-&gt;entry_point() == OptoRuntime::new_array_Java() &amp;&amp;
3410                      call-&gt;req() &gt; TypeFunc::Parms+1 &amp;&amp;
3411                      call-&gt;is_CallStaticJava()) {
3412             // Check for negative array length. In such case, the optimizer has
3413             // detected that the allocation attempt will always result in an
3414             // exception. There is no fall-through projection of this CatchNode .
3415             Node *arg1 = call-&gt;in(TypeFunc::Parms+1);
3416             if (arg1-&gt;is_Type() &amp;&amp;
3417                 arg1-&gt;as_Type()-&gt;type()-&gt;join(TypeInt::POS)-&gt;empty()) {
3418               required_outcnt--;
3419             }
3420           }
3421         }
3422       }
3423       // Recheck with a better notion of 'required_outcnt'
3424       if (n-&gt;outcnt() != required_outcnt) {
3425         record_method_not_compilable("malformed control flow");
3426         return true;            // Not all targets reachable!
3427       }
3428     }
3429     // Check that I actually visited all kids.  Unreached kids
3430     // must be infinite loops.
3431     for (DUIterator_Fast jmax, j = n-&gt;fast_outs(jmax); j &lt; jmax; j++)
3432       if (!frc._visited.test(n-&gt;fast_out(j)-&gt;_idx)) {
3433         record_method_not_compilable("infinite loop");
3434         return true;            // Found unvisited kid; must be unreach
3435       }
3436   }
3437 
3438   // If original bytecodes contained a mixture of floats and doubles
3439   // check if the optimizer has made it homogenous, item (3).
3440   if( Use24BitFPMode &amp;&amp; Use24BitFP &amp;&amp; UseSSE == 0 &amp;&amp;
3441       frc.get_float_count() &gt; 32 &amp;&amp;
3442       frc.get_double_count() == 0 &amp;&amp;
3443       (10 * frc.get_call_count() &lt; frc.get_float_count()) ) {
3444     set_24_bit_selection_and_mode( false,  true );
3445   }
3446 
3447   set_java_calls(frc.get_java_call_count());
3448   set_inner_loops(frc.get_inner_loop_count());
3449 
3450   // No infinite loops, no reason to bail out.
3451   return false;
3452 }
3453 
3454 //-----------------------------too_many_traps----------------------------------
3455 // Report if there are too many traps at the current method and bci.
3456 // Return true if there was a trap, and/or PerMethodTrapLimit is exceeded.
3457 bool Compile::too_many_traps(ciMethod* method,
3458                              int bci,
3459                              Deoptimization::DeoptReason reason) {
3460   ciMethodData* md = method-&gt;method_data();
3461   if (md-&gt;is_empty()) {
3462     // Assume the trap has not occurred, or that it occurred only
3463     // because of a transient condition during start-up in the interpreter.
3464     return false;
3465   }
3466   ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this-&gt;method() : NULL;
3467   if (md-&gt;has_trap_at(bci, m, reason) != 0) {
3468     // Assume PerBytecodeTrapLimit==0, for a more conservative heuristic.
3469     // Also, if there are multiple reasons, or if there is no per-BCI record,
3470     // assume the worst.
3471     if (log())
3472       log()-&gt;elem("observe trap='%s' count='%d'",
3473                   Deoptimization::trap_reason_name(reason),
3474                   md-&gt;trap_count(reason));
3475     return true;
3476   } else {
3477     // Ignore method/bci and see if there have been too many globally.
3478     return too_many_traps(reason, md);
3479   }
3480 }
3481 
3482 // Less-accurate variant which does not require a method and bci.
3483 bool Compile::too_many_traps(Deoptimization::DeoptReason reason,
3484                              ciMethodData* logmd) {
3485   if (trap_count(reason) &gt;= Deoptimization::per_method_trap_limit(reason)) {
3486     // Too many traps globally.
3487     // Note that we use cumulative trap_count, not just md-&gt;trap_count.
3488     if (log()) {
3489       int mcount = (logmd == NULL)? -1: (int)logmd-&gt;trap_count(reason);
3490       log()-&gt;elem("observe trap='%s' count='0' mcount='%d' ccount='%d'",
3491                   Deoptimization::trap_reason_name(reason),
3492                   mcount, trap_count(reason));
3493     }
3494     return true;
3495   } else {
3496     // The coast is clear.
3497     return false;
3498   }
3499 }
3500 
3501 //--------------------------too_many_recompiles--------------------------------
3502 // Report if there are too many recompiles at the current method and bci.
3503 // Consults PerBytecodeRecompilationCutoff and PerMethodRecompilationCutoff.
3504 // Is not eager to return true, since this will cause the compiler to use
3505 // Action_none for a trap point, to avoid too many recompilations.
3506 bool Compile::too_many_recompiles(ciMethod* method,
3507                                   int bci,
3508                                   Deoptimization::DeoptReason reason) {
3509   ciMethodData* md = method-&gt;method_data();
3510   if (md-&gt;is_empty()) {
3511     // Assume the trap has not occurred, or that it occurred only
3512     // because of a transient condition during start-up in the interpreter.
3513     return false;
3514   }
3515   // Pick a cutoff point well within PerBytecodeRecompilationCutoff.
3516   uint bc_cutoff = (uint) PerBytecodeRecompilationCutoff / 8;
3517   uint m_cutoff  = (uint) PerMethodRecompilationCutoff / 2 + 1;  // not zero
3518   Deoptimization::DeoptReason per_bc_reason
3519     = Deoptimization::reason_recorded_per_bytecode_if_any(reason);
3520   ciMethod* m = Deoptimization::reason_is_speculate(reason) ? this-&gt;method() : NULL;
3521   if ((per_bc_reason == Deoptimization::Reason_none
3522        || md-&gt;has_trap_at(bci, m, reason) != 0)
3523       // The trap frequency measure we care about is the recompile count:
3524       &amp;&amp; md-&gt;trap_recompiled_at(bci, m)
3525       &amp;&amp; md-&gt;overflow_recompile_count() &gt;= bc_cutoff) {
3526     // Do not emit a trap here if it has already caused recompilations.
3527     // Also, if there are multiple reasons, or if there is no per-BCI record,
3528     // assume the worst.
3529     if (log())
3530       log()-&gt;elem("observe trap='%s recompiled' count='%d' recompiles2='%d'",
3531                   Deoptimization::trap_reason_name(reason),
3532                   md-&gt;trap_count(reason),
3533                   md-&gt;overflow_recompile_count());
3534     return true;
3535   } else if (trap_count(reason) != 0
3536              &amp;&amp; decompile_count() &gt;= m_cutoff) {
3537     // Too many recompiles globally, and we have seen this sort of trap.
3538     // Use cumulative decompile_count, not just md-&gt;decompile_count.
3539     if (log())
3540       log()-&gt;elem("observe trap='%s' count='%d' mcount='%d' decompiles='%d' mdecompiles='%d'",
3541                   Deoptimization::trap_reason_name(reason),
3542                   md-&gt;trap_count(reason), trap_count(reason),
3543                   md-&gt;decompile_count(), decompile_count());
3544     return true;
3545   } else {
3546     // The coast is clear.
3547     return false;
3548   }
3549 }
3550 
3551 // Compute when not to trap. Used by matching trap based nodes and
3552 // NullCheck optimization.
3553 void Compile::set_allowed_deopt_reasons() {
3554   _allowed_reasons = 0;
3555   if (is_method_compilation()) {
3556     for (int rs = (int)Deoptimization::Reason_none+1; rs &lt; Compile::trapHistLength; rs++) {
3557       assert(rs &lt; BitsPerInt, "recode bit map");
3558       if (!too_many_traps((Deoptimization::DeoptReason) rs)) {
3559         _allowed_reasons |= nth_bit(rs);
3560       }
3561     }
3562   }
3563 }
3564 
3565 #ifndef PRODUCT
3566 //------------------------------verify_graph_edges---------------------------
3567 // Walk the Graph and verify that there is a one-to-one correspondence
3568 // between Use-Def edges and Def-Use edges in the graph.
3569 void Compile::verify_graph_edges(bool no_dead_code) {
3570   if (VerifyGraphEdges) {
3571     ResourceArea *area = Thread::current()-&gt;resource_area();
3572     Unique_Node_List visited(area);
3573     // Call recursive graph walk to check edges
3574     _root-&gt;verify_edges(visited);
3575     if (no_dead_code) {
3576       // Now make sure that no visited node is used by an unvisited node.
3577       bool dead_nodes = false;
3578       Unique_Node_List checked(area);
3579       while (visited.size() &gt; 0) {
3580         Node* n = visited.pop();
3581         checked.push(n);
3582         for (uint i = 0; i &lt; n-&gt;outcnt(); i++) {
3583           Node* use = n-&gt;raw_out(i);
3584           if (checked.member(use))  continue;  // already checked
3585           if (visited.member(use))  continue;  // already in the graph
3586           if (use-&gt;is_Con())        continue;  // a dead ConNode is OK
3587           // At this point, we have found a dead node which is DU-reachable.
3588           if (!dead_nodes) {
3589             tty-&gt;print_cr("*** Dead nodes reachable via DU edges:");
3590             dead_nodes = true;
3591           }
3592           use-&gt;dump(2);
3593           tty-&gt;print_cr("---");
3594           checked.push(use);  // No repeats; pretend it is now checked.
3595         }
3596       }
3597       assert(!dead_nodes, "using nodes must be reachable from root");
3598     }
3599   }
3600 }
3601 
3602 // Verify GC barriers consistency
3603 // Currently supported:
3604 // - G1 pre-barriers (see GraphKit::g1_write_barrier_pre())
3605 void Compile::verify_barriers() {
3606   if (UseG1GC) {
3607     // Verify G1 pre-barriers
3608     const int marking_offset = in_bytes(JavaThread::satb_mark_queue_offset() + SATBMarkQueue::byte_offset_of_active());
3609 
3610     ResourceArea *area = Thread::current()-&gt;resource_area();
3611     Unique_Node_List visited(area);
3612     Node_List worklist(area);
3613     // We're going to walk control flow backwards starting from the Root
3614     worklist.push(_root);
3615     while (worklist.size() &gt; 0) {
3616       Node* x = worklist.pop();
3617       if (x == NULL || x == top()) continue;
3618       if (visited.member(x)) {
3619         continue;
3620       } else {
3621         visited.push(x);
3622       }
3623 
3624       if (x-&gt;is_Region()) {
3625         for (uint i = 1; i &lt; x-&gt;req(); i++) {
3626           worklist.push(x-&gt;in(i));
3627         }
3628       } else {
3629         worklist.push(x-&gt;in(0));
3630         // We are looking for the pattern:
3631         //                            /-&gt;ThreadLocal
3632         // If-&gt;Bool-&gt;CmpI-&gt;LoadB-&gt;AddP-&gt;ConL(marking_offset)
3633         //              \-&gt;ConI(0)
3634         // We want to verify that the If and the LoadB have the same control
3635         // See GraphKit::g1_write_barrier_pre()
3636         if (x-&gt;is_If()) {
3637           IfNode *iff = x-&gt;as_If();
3638           if (iff-&gt;in(1)-&gt;is_Bool() &amp;&amp; iff-&gt;in(1)-&gt;in(1)-&gt;is_Cmp()) {
3639             CmpNode *cmp = iff-&gt;in(1)-&gt;in(1)-&gt;as_Cmp();
3640             if (cmp-&gt;Opcode() == Op_CmpI &amp;&amp; cmp-&gt;in(2)-&gt;is_Con() &amp;&amp; cmp-&gt;in(2)-&gt;bottom_type()-&gt;is_int()-&gt;get_con() == 0
3641                 &amp;&amp; cmp-&gt;in(1)-&gt;is_Load()) {
3642               LoadNode* load = cmp-&gt;in(1)-&gt;as_Load();
3643               if (load-&gt;Opcode() == Op_LoadB &amp;&amp; load-&gt;in(2)-&gt;is_AddP() &amp;&amp; load-&gt;in(2)-&gt;in(2)-&gt;Opcode() == Op_ThreadLocal
3644                   &amp;&amp; load-&gt;in(2)-&gt;in(3)-&gt;is_Con()
3645                   &amp;&amp; load-&gt;in(2)-&gt;in(3)-&gt;bottom_type()-&gt;is_intptr_t()-&gt;get_con() == marking_offset) {
3646 
3647                 Node* if_ctrl = iff-&gt;in(0);
3648                 Node* load_ctrl = load-&gt;in(0);
3649 
3650                 if (if_ctrl != load_ctrl) {
3651                   // Skip possible CProj-&gt;NeverBranch in infinite loops
3652                   if ((if_ctrl-&gt;is_Proj() &amp;&amp; if_ctrl-&gt;Opcode() == Op_CProj)
3653                       &amp;&amp; (if_ctrl-&gt;in(0)-&gt;is_MultiBranch() &amp;&amp; if_ctrl-&gt;in(0)-&gt;Opcode() == Op_NeverBranch)) {
3654                     if_ctrl = if_ctrl-&gt;in(0)-&gt;in(0);
3655                   }
3656                 }
3657                 assert(load_ctrl != NULL &amp;&amp; if_ctrl == load_ctrl, "controls must match");
3658               }
3659             }
3660           }
3661         }
3662       }
3663     }
3664   }
3665 }
3666 
3667 #endif
3668 
3669 // The Compile object keeps track of failure reasons separately from the ciEnv.
3670 // This is required because there is not quite a 1-1 relation between the
3671 // ciEnv and its compilation task and the Compile object.  Note that one
3672 // ciEnv might use two Compile objects, if C2Compiler::compile_method decides
3673 // to backtrack and retry without subsuming loads.  Other than this backtracking
3674 // behavior, the Compile's failure reason is quietly copied up to the ciEnv
3675 // by the logic in C2Compiler.
3676 void Compile::record_failure(const char* reason) {
3677   if (log() != NULL) {
3678     log()-&gt;elem("failure reason='%s' phase='compile'", reason);
3679   }
3680   if (_failure_reason == NULL) {
3681     // Record the first failure reason.
3682     _failure_reason = reason;
3683   }
3684 
3685   if (!C-&gt;failure_reason_is(C2Compiler::retry_no_subsuming_loads())) {
3686     C-&gt;print_method(PHASE_FAILURE);
3687   }
3688   _root = NULL;  // flush the graph, too
3689 }
3690 
3691 Compile::TracePhase::TracePhase(const char* name, elapsedTimer* accumulator)
3692   : TraceTime(name, accumulator, CITime, CITimeVerbose),
3693     _phase_name(name), _dolog(CITimeVerbose)
3694 {
3695   if (_dolog) {
3696     C = Compile::current();
3697     _log = C-&gt;log();
3698   } else {
3699     C = NULL;
3700     _log = NULL;
3701   }
3702   if (_log != NULL) {
3703     _log-&gt;begin_head("phase name='%s' nodes='%d' live='%d'", _phase_name, C-&gt;unique(), C-&gt;live_nodes());
3704     _log-&gt;stamp();
3705     _log-&gt;end_head();
3706   }
3707 }
3708 
3709 Compile::TracePhase::~TracePhase() {
3710 
3711   C = Compile::current();
3712   if (_dolog) {
3713     _log = C-&gt;log();
3714   } else {
3715     _log = NULL;
3716   }
3717 
3718 #ifdef ASSERT
3719   if (PrintIdealNodeCount) {
3720     tty-&gt;print_cr("phase name='%s' nodes='%d' live='%d' live_graph_walk='%d'",
3721                   _phase_name, C-&gt;unique(), C-&gt;live_nodes(), C-&gt;count_live_nodes_by_graph_walk());
3722   }
3723 
3724   if (VerifyIdealNodeCount) {
3725     Compile::current()-&gt;print_missing_nodes();
3726   }
3727 #endif
3728 
3729   if (_log != NULL) {
3730     _log-&gt;done("phase name='%s' nodes='%d' live='%d'", _phase_name, C-&gt;unique(), C-&gt;live_nodes());
3731   }
3732 }
3733 
3734 //=============================================================================
3735 // Two Constant's are equal when the type and the value are equal.
3736 bool Compile::Constant::operator==(const Constant&amp; other) {
3737   if (type()          != other.type()         )  return false;
3738   if (can_be_reused() != other.can_be_reused())  return false;
3739   // For floating point values we compare the bit pattern.
3740   switch (type()) {
3741   case T_FLOAT:   return (_v._value.i == other._v._value.i);
3742   case T_LONG:
3743   case T_DOUBLE:  return (_v._value.j == other._v._value.j);
3744   case T_OBJECT:
3745   case T_ADDRESS: return (_v._value.l == other._v._value.l);
3746   case T_VOID:    return (_v._value.l == other._v._value.l);  // jump-table entries
3747   case T_METADATA: return (_v._metadata == other._v._metadata);
3748   default: ShouldNotReachHere();
3749   }
3750   return false;
3751 }
3752 
3753 static int type_to_size_in_bytes(BasicType t) {
3754   switch (t) {
3755   case T_LONG:    return sizeof(jlong  );
3756   case T_FLOAT:   return sizeof(jfloat );
3757   case T_DOUBLE:  return sizeof(jdouble);
3758   case T_METADATA: return sizeof(Metadata*);
3759     // We use T_VOID as marker for jump-table entries (labels) which
3760     // need an internal word relocation.
3761   case T_VOID:
3762   case T_ADDRESS:
3763   case T_OBJECT:  return sizeof(jobject);
3764   }
3765 
3766   ShouldNotReachHere();
3767   return -1;
3768 }
3769 
3770 int Compile::ConstantTable::qsort_comparator(Constant* a, Constant* b) {
3771   // sort descending
3772   if (a-&gt;freq() &gt; b-&gt;freq())  return -1;
3773   if (a-&gt;freq() &lt; b-&gt;freq())  return  1;
3774   return 0;
3775 }
3776 
3777 void Compile::ConstantTable::calculate_offsets_and_size() {
3778   // First, sort the array by frequencies.
3779   _constants.sort(qsort_comparator);
3780 
3781 #ifdef ASSERT
3782   // Make sure all jump-table entries were sorted to the end of the
3783   // array (they have a negative frequency).
3784   bool found_void = false;
3785   for (int i = 0; i &lt; _constants.length(); i++) {
3786     Constant con = _constants.at(i);
3787     if (con.type() == T_VOID)
3788       found_void = true;  // jump-tables
3789     else
3790       assert(!found_void, "wrong sorting");
3791   }
3792 #endif
3793 
3794   int offset = 0;
3795   for (int i = 0; i &lt; _constants.length(); i++) {
3796     Constant* con = _constants.adr_at(i);
3797 
3798     // Align offset for type.
3799     int typesize = type_to_size_in_bytes(con-&gt;type());
3800     offset = align_size_up(offset, typesize);
3801     con-&gt;set_offset(offset);   // set constant's offset
3802 
3803     if (con-&gt;type() == T_VOID) {
3804       MachConstantNode* n = (MachConstantNode*) con-&gt;get_jobject();
3805       offset = offset + typesize * n-&gt;outcnt();  // expand jump-table
3806     } else {
3807       offset = offset + typesize;
3808     }
3809   }
3810 
3811   // Align size up to the next section start (which is insts; see
3812   // CodeBuffer::align_at_start).
3813   assert(_size == -1, "already set?");
3814   _size = align_size_up(offset, CodeEntryAlignment);
3815 }
3816 
3817 void Compile::ConstantTable::emit(CodeBuffer&amp; cb) {
3818   MacroAssembler _masm(&amp;cb);
3819   for (int i = 0; i &lt; _constants.length(); i++) {
3820     Constant con = _constants.at(i);
3821     address constant_addr = NULL;
3822     switch (con.type()) {
3823     case T_LONG:   constant_addr = _masm.long_constant(  con.get_jlong()  ); break;
3824     case T_FLOAT:  constant_addr = _masm.float_constant( con.get_jfloat() ); break;
3825     case T_DOUBLE: constant_addr = _masm.double_constant(con.get_jdouble()); break;
3826     case T_OBJECT: {
3827       jobject obj = con.get_jobject();
3828       int oop_index = _masm.oop_recorder()-&gt;find_index(obj);
3829       constant_addr = _masm.address_constant((address) obj, oop_Relocation::spec(oop_index));
3830       break;
3831     }
3832     case T_ADDRESS: {
3833       address addr = (address) con.get_jobject();
3834       constant_addr = _masm.address_constant(addr);
3835       break;
3836     }
3837     // We use T_VOID as marker for jump-table entries (labels) which
3838     // need an internal word relocation.
3839     case T_VOID: {
3840       MachConstantNode* n = (MachConstantNode*) con.get_jobject();
3841       // Fill the jump-table with a dummy word.  The real value is
3842       // filled in later in fill_jump_table.
3843       address dummy = (address) n;
3844       constant_addr = _masm.address_constant(dummy);
3845       // Expand jump-table
3846       for (uint i = 1; i &lt; n-&gt;outcnt(); i++) {
3847         address temp_addr = _masm.address_constant(dummy + i);
3848         assert(temp_addr, "consts section too small");
3849       }
3850       break;
3851     }
3852     case T_METADATA: {
3853       Metadata* obj = con.get_metadata();
3854       int metadata_index = _masm.oop_recorder()-&gt;find_index(obj);
3855       constant_addr = _masm.address_constant((address) obj, metadata_Relocation::spec(metadata_index));
3856       break;
3857     }
3858     default: ShouldNotReachHere();
3859     }
3860     assert(constant_addr, "consts section too small");
3861     assert((constant_addr - _masm.code()-&gt;consts()-&gt;start()) == con.offset(),
3862             "must be: %d == %d", (int) (constant_addr - _masm.code()-&gt;consts()-&gt;start()), (int)(con.offset()));
3863   }
3864 }
3865 
3866 int Compile::ConstantTable::find_offset(Constant&amp; con) const {
3867   int idx = _constants.find(con);
3868   assert(idx != -1, "constant must be in constant table");
3869   int offset = _constants.at(idx).offset();
3870   assert(offset != -1, "constant table not emitted yet?");
3871   return offset;
3872 }
3873 
3874 void Compile::ConstantTable::add(Constant&amp; con) {
3875   if (con.can_be_reused()) {
3876     int idx = _constants.find(con);
3877     if (idx != -1 &amp;&amp; _constants.at(idx).can_be_reused()) {
3878       _constants.adr_at(idx)-&gt;inc_freq(con.freq());  // increase the frequency by the current value
3879       return;
3880     }
3881   }
3882   (void) _constants.append(con);
3883 }
3884 
3885 Compile::Constant Compile::ConstantTable::add(MachConstantNode* n, BasicType type, jvalue value) {
3886   Block* b = Compile::current()-&gt;cfg()-&gt;get_block_for_node(n);
3887   Constant con(type, value, b-&gt;_freq);
3888   add(con);
3889   return con;
3890 }
3891 
3892 Compile::Constant Compile::ConstantTable::add(Metadata* metadata) {
3893   Constant con(metadata);
3894   add(con);
3895   return con;
3896 }
3897 
3898 Compile::Constant Compile::ConstantTable::add(MachConstantNode* n, MachOper* oper) {
3899   jvalue value;
3900   BasicType type = oper-&gt;type()-&gt;basic_type();
3901   switch (type) {
3902   case T_LONG:    value.j = oper-&gt;constantL(); break;
3903   case T_FLOAT:   value.f = oper-&gt;constantF(); break;
3904   case T_DOUBLE:  value.d = oper-&gt;constantD(); break;
3905   case T_OBJECT:
3906   case T_ADDRESS: value.l = (jobject) oper-&gt;constant(); break;
3907   case T_METADATA: return add((Metadata*)oper-&gt;constant()); break;
3908   default: guarantee(false, "unhandled type: %s", type2name(type));
3909   }
3910   return add(n, type, value);
3911 }
3912 
3913 Compile::Constant Compile::ConstantTable::add_jump_table(MachConstantNode* n) {
3914   jvalue value;
3915   // We can use the node pointer here to identify the right jump-table
3916   // as this method is called from Compile::Fill_buffer right before
3917   // the MachNodes are emitted and the jump-table is filled (means the
3918   // MachNode pointers do not change anymore).
3919   value.l = (jobject) n;
3920   Constant con(T_VOID, value, next_jump_table_freq(), false);  // Labels of a jump-table cannot be reused.
3921   add(con);
3922   return con;
3923 }
3924 
3925 void Compile::ConstantTable::fill_jump_table(CodeBuffer&amp; cb, MachConstantNode* n, GrowableArray&lt;Label*&gt; labels) const {
3926   // If called from Compile::scratch_emit_size do nothing.
3927   if (Compile::current()-&gt;in_scratch_emit_size())  return;
3928 
3929   assert(labels.is_nonempty(), "must be");
3930   assert((uint) labels.length() == n-&gt;outcnt(), "must be equal: %d == %d", labels.length(), n-&gt;outcnt());
3931 
3932   // Since MachConstantNode::constant_offset() also contains
3933   // table_base_offset() we need to subtract the table_base_offset()
3934   // to get the plain offset into the constant table.
3935   int offset = n-&gt;constant_offset() - table_base_offset();
3936 
3937   MacroAssembler _masm(&amp;cb);
3938   address* jump_table_base = (address*) (_masm.code()-&gt;consts()-&gt;start() + offset);
3939 
3940   for (uint i = 0; i &lt; n-&gt;outcnt(); i++) {
3941     address* constant_addr = &amp;jump_table_base[i];
3942     assert(*constant_addr == (((address) n) + i), "all jump-table entries must contain adjusted node pointer: " INTPTR_FORMAT " == " INTPTR_FORMAT, p2i(*constant_addr), p2i(((address) n) + i));
3943     *constant_addr = cb.consts()-&gt;target(*labels.at(i), (address) constant_addr);
3944     cb.consts()-&gt;relocate((address) constant_addr, relocInfo::internal_word_type);
3945   }
3946 }
3947 
3948 //----------------------------static_subtype_check-----------------------------
3949 // Shortcut important common cases when superklass is exact:
3950 // (0) superklass is java.lang.Object (can occur in reflective code)
3951 // (1) subklass is already limited to a subtype of superklass =&gt; always ok
3952 // (2) subklass does not overlap with superklass =&gt; always fail
3953 // (3) superklass has NO subtypes and we can check with a simple compare.
3954 int Compile::static_subtype_check(ciKlass* superk, ciKlass* subk) {
3955   if (StressReflectiveCode) {
3956     return SSC_full_test;       // Let caller generate the general case.
3957   }
3958 
3959   if (superk == env()-&gt;Object_klass()) {
3960     return SSC_always_true;     // (0) this test cannot fail
3961   }
3962 
3963   ciType* superelem = superk;
3964   if (superelem-&gt;is_array_klass())
3965     superelem = superelem-&gt;as_array_klass()-&gt;base_element_type();
3966 
3967   if (!subk-&gt;is_interface()) {  // cannot trust static interface types yet
3968     if (subk-&gt;is_subtype_of(superk)) {
3969       return SSC_always_true;   // (1) false path dead; no dynamic test needed
3970     }
3971     if (!(superelem-&gt;is_klass() &amp;&amp; superelem-&gt;as_klass()-&gt;is_interface()) &amp;&amp;
3972         !superk-&gt;is_subtype_of(subk)) {
3973       return SSC_always_false;
3974     }
3975   }
3976 
3977   // If casting to an instance klass, it must have no subtypes
3978   if (superk-&gt;is_interface()) {
3979     // Cannot trust interfaces yet.
3980     // %%% S.B. superk-&gt;nof_implementors() == 1
3981   } else if (superelem-&gt;is_instance_klass()) {
3982     ciInstanceKlass* ik = superelem-&gt;as_instance_klass();
3983     if (!ik-&gt;has_subklass() &amp;&amp; !ik-&gt;is_interface()) {
3984       if (!ik-&gt;is_final()) {
3985         // Add a dependency if there is a chance of a later subclass.
3986         dependencies()-&gt;assert_leaf_type(ik);
3987       }
3988       return SSC_easy_test;     // (3) caller can do a simple ptr comparison
3989     }
3990   } else {
3991     // A primitive array type has no subtypes.
3992     return SSC_easy_test;       // (3) caller can do a simple ptr comparison
3993   }
3994 
3995   return SSC_full_test;
3996 }
3997 
3998 Node* Compile::conv_I2X_index(PhaseGVN* phase, Node* idx, const TypeInt* sizetype, Node* ctrl) {
3999 #ifdef _LP64
4000   // The scaled index operand to AddP must be a clean 64-bit value.
4001   // Java allows a 32-bit int to be incremented to a negative
4002   // value, which appears in a 64-bit register as a large
4003   // positive number.  Using that large positive number as an
4004   // operand in pointer arithmetic has bad consequences.
4005   // On the other hand, 32-bit overflow is rare, and the possibility
4006   // can often be excluded, if we annotate the ConvI2L node with
4007   // a type assertion that its value is known to be a small positive
4008   // number.  (The prior range check has ensured this.)
4009   // This assertion is used by ConvI2LNode::Ideal.
4010   int index_max = max_jint - 1;  // array size is max_jint, index is one less
4011   if (sizetype != NULL) index_max = sizetype-&gt;_hi - 1;
4012   const TypeInt* iidxtype = TypeInt::make(0, index_max, Type::WidenMax);
4013   idx = constrained_convI2L(phase, idx, iidxtype, ctrl);
4014 #endif
4015   return idx;
4016 }
4017 
4018 // Convert integer value to a narrowed long type dependent on ctrl (for example, a range check)
4019 Node* Compile::constrained_convI2L(PhaseGVN* phase, Node* value, const TypeInt* itype, Node* ctrl) {
4020   if (ctrl != NULL) {
4021     // Express control dependency by a CastII node with a narrow type.
4022     value = new CastIINode(value, itype, false, true /* range check dependency */);
4023     // Make the CastII node dependent on the control input to prevent the narrowed ConvI2L
4024     // node from floating above the range check during loop optimizations. Otherwise, the
4025     // ConvI2L node may be eliminated independently of the range check, causing the data path
4026     // to become TOP while the control path is still there (although it's unreachable).
4027     value-&gt;set_req(0, ctrl);
4028     // Save CastII node to remove it after loop optimizations.
4029     phase-&gt;C-&gt;add_range_check_cast(value);
4030     value = phase-&gt;transform(value);
4031   }
4032   const TypeLong* ltype = TypeLong::make(itype-&gt;_lo, itype-&gt;_hi, itype-&gt;_widen);
4033   return phase-&gt;transform(new ConvI2LNode(value, ltype));
4034 }
4035 
4036 // The message about the current inlining is accumulated in
4037 // _print_inlining_stream and transfered into the _print_inlining_list
4038 // once we know whether inlining succeeds or not. For regular
4039 // inlining, messages are appended to the buffer pointed by
4040 // _print_inlining_idx in the _print_inlining_list. For late inlining,
4041 // a new buffer is added after _print_inlining_idx in the list. This
4042 // way we can update the inlining message for late inlining call site
4043 // when the inlining is attempted again.
4044 void Compile::print_inlining_init() {
4045   if (print_inlining() || print_intrinsics()) {
4046     _print_inlining_stream = new stringStream();
4047     _print_inlining_list = new (comp_arena())GrowableArray&lt;PrintInliningBuffer&gt;(comp_arena(), 1, 1, PrintInliningBuffer());
4048   }
4049 }
4050 
4051 void Compile::print_inlining_reinit() {
4052   if (print_inlining() || print_intrinsics()) {
4053     // Re allocate buffer when we change ResourceMark
4054     _print_inlining_stream = new stringStream();
4055   }
4056 }
4057 
4058 void Compile::print_inlining_reset() {
4059   _print_inlining_stream-&gt;reset();
4060 }
4061 
4062 void Compile::print_inlining_commit() {
4063   assert(print_inlining() || print_intrinsics(), "PrintInlining off?");
4064   // Transfer the message from _print_inlining_stream to the current
4065   // _print_inlining_list buffer and clear _print_inlining_stream.
4066   _print_inlining_list-&gt;at(_print_inlining_idx).ss()-&gt;write(_print_inlining_stream-&gt;as_string(), _print_inlining_stream-&gt;size());
4067   print_inlining_reset();
4068 }
4069 
4070 void Compile::print_inlining_push() {
4071   // Add new buffer to the _print_inlining_list at current position
4072   _print_inlining_idx++;
4073   _print_inlining_list-&gt;insert_before(_print_inlining_idx, PrintInliningBuffer());
4074 }
4075 
4076 Compile::PrintInliningBuffer&amp; Compile::print_inlining_current() {
4077   return _print_inlining_list-&gt;at(_print_inlining_idx);
4078 }
4079 
4080 void Compile::print_inlining_update(CallGenerator* cg) {
4081   if (print_inlining() || print_intrinsics()) {
4082     if (!cg-&gt;is_late_inline()) {
4083       if (print_inlining_current().cg() != NULL) {
4084         print_inlining_push();
4085       }
4086       print_inlining_commit();
4087     } else {
4088       if (print_inlining_current().cg() != cg &amp;&amp;
4089           (print_inlining_current().cg() != NULL ||
4090            print_inlining_current().ss()-&gt;size() != 0)) {
4091         print_inlining_push();
4092       }
4093       print_inlining_commit();
4094       print_inlining_current().set_cg(cg);
4095     }
4096   }
4097 }
4098 
4099 void Compile::print_inlining_move_to(CallGenerator* cg) {
4100   // We resume inlining at a late inlining call site. Locate the
4101   // corresponding inlining buffer so that we can update it.
4102   if (print_inlining()) {
4103     for (int i = 0; i &lt; _print_inlining_list-&gt;length(); i++) {
4104       if (_print_inlining_list-&gt;adr_at(i)-&gt;cg() == cg) {
4105         _print_inlining_idx = i;
4106         return;
4107       }
4108     }
4109     ShouldNotReachHere();
4110   }
4111 }
4112 
4113 void Compile::print_inlining_update_delayed(CallGenerator* cg) {
4114   if (print_inlining()) {
4115     assert(_print_inlining_stream-&gt;size() &gt; 0, "missing inlining msg");
4116     assert(print_inlining_current().cg() == cg, "wrong entry");
4117     // replace message with new message
4118     _print_inlining_list-&gt;at_put(_print_inlining_idx, PrintInliningBuffer());
4119     print_inlining_commit();
4120     print_inlining_current().set_cg(cg);
4121   }
4122 }
4123 
4124 void Compile::print_inlining_assert_ready() {
4125   assert(!_print_inlining || _print_inlining_stream-&gt;size() == 0, "loosing data");
4126 }
4127 
4128 void Compile::process_print_inlining() {
4129   bool do_print_inlining = print_inlining() || print_intrinsics();
4130   if (do_print_inlining || log() != NULL) {
4131     // Print inlining message for candidates that we couldn't inline
4132     // for lack of space
4133     for (int i = 0; i &lt; _late_inlines.length(); i++) {
4134       CallGenerator* cg = _late_inlines.at(i);
4135       if (!cg-&gt;is_mh_late_inline()) {
4136         const char* msg = "live nodes &gt; LiveNodeCountInliningCutoff";
4137         if (do_print_inlining) {
4138           cg-&gt;print_inlining_late(msg);
4139         }
4140         log_late_inline_failure(cg, msg);
4141       }
4142     }
4143   }
4144   if (do_print_inlining) {
4145     ResourceMark rm;
4146     stringStream ss;
4147     for (int i = 0; i &lt; _print_inlining_list-&gt;length(); i++) {
4148       ss.print("%s", _print_inlining_list-&gt;adr_at(i)-&gt;ss()-&gt;as_string());
4149     }
4150     size_t end = ss.size();
4151     _print_inlining_output = NEW_ARENA_ARRAY(comp_arena(), char, end+1);
4152     strncpy(_print_inlining_output, ss.base(), end+1);
4153     _print_inlining_output[end] = 0;
4154   }
4155 }
4156 
4157 void Compile::dump_print_inlining() {
4158   if (_print_inlining_output != NULL) {
4159     tty-&gt;print_raw(_print_inlining_output);
4160   }
4161 }
4162 
4163 void Compile::log_late_inline(CallGenerator* cg) {
4164   if (log() != NULL) {
4165     log()-&gt;head("late_inline method='%d'  inline_id='" JLONG_FORMAT "'", log()-&gt;identify(cg-&gt;method()),
4166                 cg-&gt;unique_id());
4167     JVMState* p = cg-&gt;call_node()-&gt;jvms();
4168     while (p != NULL) {
4169       log()-&gt;elem("jvms bci='%d' method='%d'", p-&gt;bci(), log()-&gt;identify(p-&gt;method()));
4170       p = p-&gt;caller();
4171     }
4172     log()-&gt;tail("late_inline");
4173   }
4174 }
4175 
4176 void Compile::log_late_inline_failure(CallGenerator* cg, const char* msg) {
4177   log_late_inline(cg);
4178   if (log() != NULL) {
4179     log()-&gt;inline_fail(msg);
4180   }
4181 }
4182 
4183 void Compile::log_inline_id(CallGenerator* cg) {
4184   if (log() != NULL) {
4185     // The LogCompilation tool needs a unique way to identify late
4186     // inline call sites. This id must be unique for this call site in
4187     // this compilation. Try to have it unique across compilations as
4188     // well because it can be convenient when grepping through the log
4189     // file.
4190     // Distinguish OSR compilations from others in case CICountOSR is
4191     // on.
4192     jlong id = ((jlong)unique()) + (((jlong)compile_id()) &lt;&lt; 33) + (CICountOSR &amp;&amp; is_osr_compilation() ? ((jlong)1) &lt;&lt; 32 : 0);
4193     cg-&gt;set_unique_id(id);
4194     log()-&gt;elem("inline_id id='" JLONG_FORMAT "'", id);
4195   }
4196 }
4197 
4198 void Compile::log_inline_failure(const char* msg) {
4199   if (C-&gt;log() != NULL) {
4200     C-&gt;log()-&gt;inline_fail(msg);
4201   }
4202 }
4203 
4204 
4205 // Dump inlining replay data to the stream.
4206 // Don't change thread state and acquire any locks.
4207 void Compile::dump_inline_data(outputStream* out) {
4208   InlineTree* inl_tree = ilt();
4209   if (inl_tree != NULL) {
4210     out-&gt;print(" inline %d", inl_tree-&gt;count());
4211     inl_tree-&gt;dump_replay_data(out);
4212   }
4213 }
4214 
4215 int Compile::cmp_expensive_nodes(Node* n1, Node* n2) {
4216   if (n1-&gt;Opcode() &lt; n2-&gt;Opcode())      return -1;
4217   else if (n1-&gt;Opcode() &gt; n2-&gt;Opcode()) return 1;
4218 
4219   assert(n1-&gt;req() == n2-&gt;req(), "can't compare %s nodes: n1-&gt;req() = %d, n2-&gt;req() = %d", NodeClassNames[n1-&gt;Opcode()], n1-&gt;req(), n2-&gt;req());
4220   for (uint i = 1; i &lt; n1-&gt;req(); i++) {
4221     if (n1-&gt;in(i) &lt; n2-&gt;in(i))      return -1;
4222     else if (n1-&gt;in(i) &gt; n2-&gt;in(i)) return 1;
4223   }
4224 
4225   return 0;
4226 }
4227 
4228 int Compile::cmp_expensive_nodes(Node** n1p, Node** n2p) {
4229   Node* n1 = *n1p;
4230   Node* n2 = *n2p;
4231 
4232   return cmp_expensive_nodes(n1, n2);
4233 }
4234 
4235 void Compile::sort_expensive_nodes() {
4236   if (!expensive_nodes_sorted()) {
4237     _expensive_nodes-&gt;sort(cmp_expensive_nodes);
4238   }
4239 }
4240 
4241 bool Compile::expensive_nodes_sorted() const {
4242   for (int i = 1; i &lt; _expensive_nodes-&gt;length(); i++) {
4243     if (cmp_expensive_nodes(_expensive_nodes-&gt;adr_at(i), _expensive_nodes-&gt;adr_at(i-1)) &lt; 0) {
4244       return false;
4245     }
4246   }
4247   return true;
4248 }
4249 
4250 bool Compile::should_optimize_expensive_nodes(PhaseIterGVN &amp;igvn) {
4251   if (_expensive_nodes-&gt;length() == 0) {
4252     return false;
4253   }
4254 
4255   assert(OptimizeExpensiveOps, "optimization off?");
4256 
4257   // Take this opportunity to remove dead nodes from the list
4258   int j = 0;
4259   for (int i = 0; i &lt; _expensive_nodes-&gt;length(); i++) {
4260     Node* n = _expensive_nodes-&gt;at(i);
4261     if (!n-&gt;is_unreachable(igvn)) {
4262       assert(n-&gt;is_expensive(), "should be expensive");
4263       _expensive_nodes-&gt;at_put(j, n);
4264       j++;
4265     }
4266   }
4267   _expensive_nodes-&gt;trunc_to(j);
4268 
4269   // Then sort the list so that similar nodes are next to each other
4270   // and check for at least two nodes of identical kind with same data
4271   // inputs.
4272   sort_expensive_nodes();
4273 
4274   for (int i = 0; i &lt; _expensive_nodes-&gt;length()-1; i++) {
4275     if (cmp_expensive_nodes(_expensive_nodes-&gt;adr_at(i), _expensive_nodes-&gt;adr_at(i+1)) == 0) {
4276       return true;
4277     }
4278   }
4279 
4280   return false;
4281 }
4282 
4283 void Compile::cleanup_expensive_nodes(PhaseIterGVN &amp;igvn) {
4284   if (_expensive_nodes-&gt;length() == 0) {
4285     return;
4286   }
4287 
4288   assert(OptimizeExpensiveOps, "optimization off?");
4289 
4290   // Sort to bring similar nodes next to each other and clear the
4291   // control input of nodes for which there's only a single copy.
4292   sort_expensive_nodes();
4293 
4294   int j = 0;
4295   int identical = 0;
4296   int i = 0;
4297   bool modified = false;
4298   for (; i &lt; _expensive_nodes-&gt;length()-1; i++) {
4299     assert(j &lt;= i, "can't write beyond current index");
4300     if (_expensive_nodes-&gt;at(i)-&gt;Opcode() == _expensive_nodes-&gt;at(i+1)-&gt;Opcode()) {
4301       identical++;
4302       _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4303       continue;
4304     }
4305     if (identical &gt; 0) {
4306       _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4307       identical = 0;
4308     } else {
4309       Node* n = _expensive_nodes-&gt;at(i);
4310       igvn.replace_input_of(n, 0, NULL);
4311       igvn.hash_insert(n);
4312       modified = true;
4313     }
4314   }
4315   if (identical &gt; 0) {
4316     _expensive_nodes-&gt;at_put(j++, _expensive_nodes-&gt;at(i));
4317   } else if (_expensive_nodes-&gt;length() &gt;= 1) {
4318     Node* n = _expensive_nodes-&gt;at(i);
4319     igvn.replace_input_of(n, 0, NULL);
4320     igvn.hash_insert(n);
4321     modified = true;
4322   }
4323   _expensive_nodes-&gt;trunc_to(j);
4324   if (modified) {
4325     igvn.optimize();
4326   }
4327 }
4328 
4329 void Compile::add_expensive_node(Node * n) {
4330   assert(!_expensive_nodes-&gt;contains(n), "duplicate entry in expensive list");
4331   assert(n-&gt;is_expensive(), "expensive nodes with non-null control here only");
4332   assert(!n-&gt;is_CFG() &amp;&amp; !n-&gt;is_Mem(), "no cfg or memory nodes here");
4333   if (OptimizeExpensiveOps) {
4334     _expensive_nodes-&gt;append(n);
4335   } else {
4336     // Clear control input and let IGVN optimize expensive nodes if
4337     // OptimizeExpensiveOps is off.
4338     n-&gt;set_req(0, NULL);
4339   }
4340 }
4341 
4342 /**
4343  * Remove the speculative part of types and clean up the graph
4344  */
4345 void Compile::remove_speculative_types(PhaseIterGVN &amp;igvn) {
4346   if (UseTypeSpeculation) {
4347     Unique_Node_List worklist;
4348     worklist.push(root());
4349     int modified = 0;
4350     // Go over all type nodes that carry a speculative type, drop the
4351     // speculative part of the type and enqueue the node for an igvn
4352     // which may optimize it out.
4353     for (uint next = 0; next &lt; worklist.size(); ++next) {
4354       Node *n  = worklist.at(next);
4355       if (n-&gt;is_Type()) {
4356         TypeNode* tn = n-&gt;as_Type();
4357         const Type* t = tn-&gt;type();
4358         const Type* t_no_spec = t-&gt;remove_speculative();
4359         if (t_no_spec != t) {
4360           bool in_hash = igvn.hash_delete(n);
4361           assert(in_hash, "node should be in igvn hash table");
4362           tn-&gt;set_type(t_no_spec);
4363           igvn.hash_insert(n);
4364           igvn._worklist.push(n); // give it a chance to go away
4365           modified++;
4366         }
4367       }
4368       uint max = n-&gt;len();
4369       for( uint i = 0; i &lt; max; ++i ) {
4370         Node *m = n-&gt;in(i);
4371         if (not_a_node(m))  continue;
4372         worklist.push(m);
4373       }
4374     }
4375     // Drop the speculative part of all types in the igvn's type table
4376     igvn.remove_speculative_types();
4377     if (modified &gt; 0) {
4378       igvn.optimize();
4379     }
4380 #ifdef ASSERT
4381     // Verify that after the IGVN is over no speculative type has resurfaced
4382     worklist.clear();
4383     worklist.push(root());
4384     for (uint next = 0; next &lt; worklist.size(); ++next) {
4385       Node *n  = worklist.at(next);
4386       const Type* t = igvn.type_or_null(n);
4387       assert((t == NULL) || (t == t-&gt;remove_speculative()), "no more speculative types");
4388       if (n-&gt;is_Type()) {
4389         t = n-&gt;as_Type()-&gt;type();
4390         assert(t == t-&gt;remove_speculative(), "no more speculative types");
4391       }
4392       uint max = n-&gt;len();
4393       for( uint i = 0; i &lt; max; ++i ) {
4394         Node *m = n-&gt;in(i);
4395         if (not_a_node(m))  continue;
4396         worklist.push(m);
4397       }
4398     }
4399     igvn.check_no_speculative_types();
4400 #endif
4401   }
4402 }
4403 
4404 // Auxiliary method to support randomized stressing/fuzzing.
4405 //
4406 // This method can be called the arbitrary number of times, with current count
4407 // as the argument. The logic allows selecting a single candidate from the
4408 // running list of candidates as follows:
4409 //    int count = 0;
4410 //    Cand* selected = null;
4411 //    while(cand = cand-&gt;next()) {
4412 //      if (randomized_select(++count)) {
4413 //        selected = cand;
4414 //      }
4415 //    }
4416 //
4417 // Including count equalizes the chances any candidate is "selected".
4418 // This is useful when we don't have the complete list of candidates to choose
4419 // from uniformly. In this case, we need to adjust the randomicity of the
4420 // selection, or else we will end up biasing the selection towards the latter
4421 // candidates.
4422 //
4423 // Quick back-envelope calculation shows that for the list of n candidates
4424 // the equal probability for the candidate to persist as "best" can be
4425 // achieved by replacing it with "next" k-th candidate with the probability
4426 // of 1/k. It can be easily shown that by the end of the run, the
4427 // probability for any candidate is converged to 1/n, thus giving the
4428 // uniform distribution among all the candidates.
4429 //
4430 // We don't care about the domain size as long as (RANDOMIZED_DOMAIN / count) is large.
4431 #define RANDOMIZED_DOMAIN_POW 29
4432 #define RANDOMIZED_DOMAIN (1 &lt;&lt; RANDOMIZED_DOMAIN_POW)
4433 #define RANDOMIZED_DOMAIN_MASK ((1 &lt;&lt; (RANDOMIZED_DOMAIN_POW + 1)) - 1)
4434 bool Compile::randomized_select(int count) {
4435   assert(count &gt; 0, "only positive");
4436   return (os::random() &amp; RANDOMIZED_DOMAIN_MASK) &lt; (RANDOMIZED_DOMAIN / count);
4437 }
4438 
4439 CloneMap&amp;     Compile::clone_map()                 { return _clone_map; }
4440 void          Compile::set_clone_map(Dict* d)      { _clone_map._dict = d; }
4441 
4442 void NodeCloneInfo::dump() const {
4443   tty-&gt;print(" {%d:%d} ", idx(), gen());
4444 }
4445 
4446 void CloneMap::clone(Node* old, Node* nnn, int gen) {
4447   uint64_t val = value(old-&gt;_idx);
4448   NodeCloneInfo cio(val);
4449   assert(val != 0, "old node should be in the map");
4450   NodeCloneInfo cin(cio.idx(), gen + cio.gen());
4451   insert(nnn-&gt;_idx, cin.get());
4452 #ifndef PRODUCT
4453   if (is_debug()) {
4454     tty-&gt;print_cr("CloneMap::clone inserted node %d info {%d:%d} into CloneMap", nnn-&gt;_idx, cin.idx(), cin.gen());
4455   }
4456 #endif
4457 }
4458 
4459 void CloneMap::verify_insert_and_clone(Node* old, Node* nnn, int gen) {
4460   NodeCloneInfo cio(value(old-&gt;_idx));
4461   if (cio.get() == 0) {
4462     cio.set(old-&gt;_idx, 0);
4463     insert(old-&gt;_idx, cio.get());
4464 #ifndef PRODUCT
4465     if (is_debug()) {
4466       tty-&gt;print_cr("CloneMap::verify_insert_and_clone inserted node %d info {%d:%d} into CloneMap", old-&gt;_idx, cio.idx(), cio.gen());
4467     }
4468 #endif
4469   }
4470   clone(old, nnn, gen);
4471 }
4472 
4473 int CloneMap::max_gen() const {
4474   int g = 0;
4475   DictI di(_dict);
4476   for(; di.test(); ++di) {
4477     int t = gen(di._key);
4478     if (g &lt; t) {
4479       g = t;
4480 #ifndef PRODUCT
4481       if (is_debug()) {
4482         tty-&gt;print_cr("CloneMap::max_gen() update max=%d from %d", g, _2_node_idx_t(di._key));
4483       }
4484 #endif
4485     }
4486   }
4487   return g;
4488 }
4489 
4490 void CloneMap::dump(node_idx_t key) const {
4491   uint64_t val = value(key);
4492   if (val != 0) {
4493     NodeCloneInfo ni(val);
4494     ni.dump();
4495   }
4496 }
</pre></body></html>
