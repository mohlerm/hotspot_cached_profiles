<?xml version="1.0"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
    "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head><meta charset="utf-8">
<meta http-equiv="cache-control" content="no-cache" />
<meta http-equiv="Pragma" content="no-cache" />
<meta http-equiv="Expires" content="-1" />
<!--
   Note to customizers: the body of the webrev is IDed as SUNWwebrev
   to allow easy overriding by users of webrev via the userContent.css
   mechanism available in some browsers.

   For example, to have all "removed" information be red instead of
   brown, set a rule in your userContent.css file like:

       body#SUNWwebrev span.removed { color: red ! important; }
-->
<style type="text/css" media="screen">
body {
    background-color: #eeeeee;
}
hr {
    border: none 0;
    border-top: 1px solid #aaa;
    height: 1px;
}
div.summary {
    font-size: .8em;
    border-bottom: 1px solid #aaa;
    padding-left: 1em;
    padding-right: 1em;
}
div.summary h2 {
    margin-bottom: 0.3em;
}
div.summary table th {
    text-align: right;
    vertical-align: top;
    white-space: nowrap;
}
span.lineschanged {
    font-size: 0.7em;
}
span.oldmarker {
    color: red;
    font-size: large;
    font-weight: bold;
}
span.newmarker {
    color: green;
    font-size: large;
    font-weight: bold;
}
span.removed {
    color: brown;
}
span.changed {
    color: blue;
}
span.new {
    color: blue;
    font-weight: bold;
}
a.print { font-size: x-small; }

</style>

<style type="text/css" media="print">
pre { font-size: 0.8em; font-family: courier, monospace; }
span.removed { color: #444; font-style: italic }
span.changed { font-weight: bold; }
span.new { font-weight: bold; }
span.newmarker { font-size: 1.2em; font-weight: bold; }
span.oldmarker { font-size: 1.2em; font-weight: bold; }
a.print {display: none}
hr { border: none 0; border-top: 1px solid #aaa; height: 1px; }
</style>

<title>New src/share/vm/opto/library_call.cpp</title>
<body id="SUNWwebrev">
<pre>
   1 /*
   2  * Copyright (c) 1999, 2016, Oracle and/or its affiliates. All rights reserved.
   3  * DO NOT ALTER OR REMOVE COPYRIGHT NOTICES OR THIS FILE HEADER.
   4  *
   5  * This code is free software; you can redistribute it and/or modify it
   6  * under the terms of the GNU General Public License version 2 only, as
   7  * published by the Free Software Foundation.
   8  *
   9  * This code is distributed in the hope that it will be useful, but WITHOUT
  10  * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
  11  * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
  12  * version 2 for more details (a copy is included in the LICENSE file that
  13  * accompanied this code).
  14  *
  15  * You should have received a copy of the GNU General Public License version
  16  * 2 along with this work; if not, write to the Free Software Foundation,
  17  * Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA.
  18  *
  19  * Please contact Oracle, 500 Oracle Parkway, Redwood Shores, CA 94065 USA
  20  * or visit www.oracle.com if you need additional information or have any
  21  * questions.
  22  *
  23  */
  24 
  25 #include "precompiled.hpp"
  26 #include "asm/macroAssembler.hpp"
  27 #include "classfile/systemDictionary.hpp"
  28 #include "classfile/vmSymbols.hpp"
  29 #include "compiler/compileBroker.hpp"
  30 #include "compiler/compileLog.hpp"
  31 #include "oops/objArrayKlass.hpp"
  32 #include "opto/addnode.hpp"
  33 #include "opto/arraycopynode.hpp"
  34 #include "opto/c2compiler.hpp"
  35 #include "opto/callGenerator.hpp"
  36 #include "opto/castnode.hpp"
  37 #include "opto/cfgnode.hpp"
  38 #include "opto/convertnode.hpp"
  39 #include "opto/countbitsnode.hpp"
  40 #include "opto/intrinsicnode.hpp"
  41 #include "opto/idealKit.hpp"
  42 #include "opto/mathexactnode.hpp"
  43 #include "opto/movenode.hpp"
  44 #include "opto/mulnode.hpp"
  45 #include "opto/narrowptrnode.hpp"
  46 #include "opto/opaquenode.hpp"
  47 #include "opto/parse.hpp"
  48 #include "opto/runtime.hpp"
  49 #include "opto/subnode.hpp"
  50 #include "prims/nativeLookup.hpp"
  51 #include "runtime/sharedRuntime.hpp"
  52 #ifdef TRACE_HAVE_INTRINSICS
  53 #include "trace/traceMacros.hpp"
  54 #endif
  55 
  56 class LibraryIntrinsic : public InlineCallGenerator {
  57   // Extend the set of intrinsics known to the runtime:
  58  public:
  59  private:
  60   bool             _is_virtual;
  61   bool             _does_virtual_dispatch;
  62   int8_t           _predicates_count;  // Intrinsic is predicated by several conditions
  63   int8_t           _last_predicate; // Last generated predicate
  64   vmIntrinsics::ID _intrinsic_id;
  65 
  66  public:
  67   LibraryIntrinsic(ciMethod* m, bool is_virtual, int predicates_count, bool does_virtual_dispatch, vmIntrinsics::ID id)
  68     : InlineCallGenerator(m),
  69       _is_virtual(is_virtual),
  70       _does_virtual_dispatch(does_virtual_dispatch),
  71       _predicates_count((int8_t)predicates_count),
  72       _last_predicate((int8_t)-1),
  73       _intrinsic_id(id)
  74   {
  75   }
  76   virtual bool is_intrinsic() const { return true; }
  77   virtual bool is_virtual()   const { return _is_virtual; }
  78   virtual bool is_predicated() const { return _predicates_count &gt; 0; }
  79   virtual int  predicates_count() const { return _predicates_count; }
  80   virtual bool does_virtual_dispatch()   const { return _does_virtual_dispatch; }
  81   virtual JVMState* generate(JVMState* jvms);
  82   virtual Node* generate_predicate(JVMState* jvms, int predicate);
  83   vmIntrinsics::ID intrinsic_id() const { return _intrinsic_id; }
  84 };
  85 
  86 
  87 // Local helper class for LibraryIntrinsic:
  88 class LibraryCallKit : public GraphKit {
  89  private:
  90   LibraryIntrinsic* _intrinsic;     // the library intrinsic being called
  91   Node*             _result;        // the result node, if any
  92   int               _reexecute_sp;  // the stack pointer when bytecode needs to be reexecuted
  93 
  94   const TypeOopPtr* sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr = false);
  95 
  96  public:
  97   LibraryCallKit(JVMState* jvms, LibraryIntrinsic* intrinsic)
  98     : GraphKit(jvms),
  99       _intrinsic(intrinsic),
 100       _result(NULL)
 101   {
 102     // Check if this is a root compile.  In that case we don't have a caller.
 103     if (!jvms-&gt;has_method()) {
 104       _reexecute_sp = sp();
 105     } else {
 106       // Find out how many arguments the interpreter needs when deoptimizing
 107       // and save the stack pointer value so it can used by uncommon_trap.
 108       // We find the argument count by looking at the declared signature.
 109       bool ignored_will_link;
 110       ciSignature* declared_signature = NULL;
 111       ciMethod* ignored_callee = caller()-&gt;get_method_at_bci(bci(), ignored_will_link, &amp;declared_signature);
 112       const int nargs = declared_signature-&gt;arg_size_for_bc(caller()-&gt;java_code_at_bci(bci()));
 113       _reexecute_sp = sp() + nargs;  // "push" arguments back on stack
 114     }
 115   }
 116 
 117   virtual LibraryCallKit* is_LibraryCallKit() const { return (LibraryCallKit*)this; }
 118 
 119   ciMethod*         caller()    const    { return jvms()-&gt;method(); }
 120   int               bci()       const    { return jvms()-&gt;bci(); }
 121   LibraryIntrinsic* intrinsic() const    { return _intrinsic; }
 122   vmIntrinsics::ID  intrinsic_id() const { return _intrinsic-&gt;intrinsic_id(); }
 123   ciMethod*         callee()    const    { return _intrinsic-&gt;method(); }
 124 
 125   bool  try_to_inline(int predicate);
 126   Node* try_to_predicate(int predicate);
 127 
 128   void push_result() {
 129     // Push the result onto the stack.
 130     if (!stopped() &amp;&amp; result() != NULL) {
 131       BasicType bt = result()-&gt;bottom_type()-&gt;basic_type();
 132       push_node(bt, result());
 133     }
 134   }
 135 
 136  private:
 137   void fatal_unexpected_iid(vmIntrinsics::ID iid) {
 138     fatal("unexpected intrinsic %d: %s", iid, vmIntrinsics::name_at(iid));
 139   }
 140 
 141   void  set_result(Node* n) { assert(_result == NULL, "only set once"); _result = n; }
 142   void  set_result(RegionNode* region, PhiNode* value);
 143   Node*     result() { return _result; }
 144 
 145   virtual int reexecute_sp() { return _reexecute_sp; }
 146 
 147   // Helper functions to inline natives
 148   Node* generate_guard(Node* test, RegionNode* region, float true_prob);
 149   Node* generate_slow_guard(Node* test, RegionNode* region);
 150   Node* generate_fair_guard(Node* test, RegionNode* region);
 151   Node* generate_negative_guard(Node* index, RegionNode* region,
 152                                 // resulting CastII of index:
 153                                 Node* *pos_index = NULL);
 154   Node* generate_limit_guard(Node* offset, Node* subseq_length,
 155                              Node* array_length,
 156                              RegionNode* region);
 157   void  generate_string_range_check(Node* array, Node* offset,
 158                                     Node* length, bool char_count);
 159   Node* generate_current_thread(Node* &amp;tls_output);
 160   Node* load_mirror_from_klass(Node* klass);
 161   Node* load_klass_from_mirror_common(Node* mirror, bool never_see_null,
 162                                       RegionNode* region, int null_path,
 163                                       int offset);
 164   Node* load_klass_from_mirror(Node* mirror, bool never_see_null,
 165                                RegionNode* region, int null_path) {
 166     int offset = java_lang_Class::klass_offset_in_bytes();
 167     return load_klass_from_mirror_common(mirror, never_see_null,
 168                                          region, null_path,
 169                                          offset);
 170   }
 171   Node* load_array_klass_from_mirror(Node* mirror, bool never_see_null,
 172                                      RegionNode* region, int null_path) {
 173     int offset = java_lang_Class::array_klass_offset_in_bytes();
 174     return load_klass_from_mirror_common(mirror, never_see_null,
 175                                          region, null_path,
 176                                          offset);
 177   }
 178   Node* generate_access_flags_guard(Node* kls,
 179                                     int modifier_mask, int modifier_bits,
 180                                     RegionNode* region);
 181   Node* generate_interface_guard(Node* kls, RegionNode* region);
 182   Node* generate_array_guard(Node* kls, RegionNode* region) {
 183     return generate_array_guard_common(kls, region, false, false);
 184   }
 185   Node* generate_non_array_guard(Node* kls, RegionNode* region) {
 186     return generate_array_guard_common(kls, region, false, true);
 187   }
 188   Node* generate_objArray_guard(Node* kls, RegionNode* region) {
 189     return generate_array_guard_common(kls, region, true, false);
 190   }
 191   Node* generate_non_objArray_guard(Node* kls, RegionNode* region) {
 192     return generate_array_guard_common(kls, region, true, true);
 193   }
 194   Node* generate_array_guard_common(Node* kls, RegionNode* region,
 195                                     bool obj_array, bool not_array);
 196   Node* generate_virtual_guard(Node* obj_klass, RegionNode* slow_region);
 197   CallJavaNode* generate_method_call(vmIntrinsics::ID method_id,
 198                                      bool is_virtual = false, bool is_static = false);
 199   CallJavaNode* generate_method_call_static(vmIntrinsics::ID method_id) {
 200     return generate_method_call(method_id, false, true);
 201   }
 202   CallJavaNode* generate_method_call_virtual(vmIntrinsics::ID method_id) {
 203     return generate_method_call(method_id, true, false);
 204   }
 205   Node * load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 206   Node * field_address_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString, bool is_exact, bool is_static, ciInstanceKlass * fromKls);
 207 
 208   Node* make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2, StrIntrinsicNode::ArgEnc ae);
 209   bool inline_string_compareTo(StrIntrinsicNode::ArgEnc ae);
 210   bool inline_string_indexOf(StrIntrinsicNode::ArgEnc ae);
 211   bool inline_string_indexOfI(StrIntrinsicNode::ArgEnc ae);
 212   Node* make_indexOf_node(Node* src_start, Node* src_count, Node* tgt_start, Node* tgt_count,
 213                           RegionNode* region, Node* phi, StrIntrinsicNode::ArgEnc ae);
 214   bool inline_string_indexOfChar();
 215   bool inline_string_equals(StrIntrinsicNode::ArgEnc ae);
 216   bool inline_string_toBytesU();
 217   bool inline_string_getCharsU();
 218   bool inline_string_copy(bool compress);
 219   bool inline_string_char_access(bool is_store);
 220   Node* round_double_node(Node* n);
 221   bool runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName);
 222   bool inline_math_native(vmIntrinsics::ID id);
 223   bool inline_trig(vmIntrinsics::ID id);
 224   bool inline_math(vmIntrinsics::ID id);
 225   template &lt;typename OverflowOp&gt;
 226   bool inline_math_overflow(Node* arg1, Node* arg2);
 227   void inline_math_mathExact(Node* math, Node* test);
 228   bool inline_math_addExactI(bool is_increment);
 229   bool inline_math_addExactL(bool is_increment);
 230   bool inline_math_multiplyExactI();
 231   bool inline_math_multiplyExactL();
 232   bool inline_math_negateExactI();
 233   bool inline_math_negateExactL();
 234   bool inline_math_subtractExactI(bool is_decrement);
 235   bool inline_math_subtractExactL(bool is_decrement);
 236   bool inline_min_max(vmIntrinsics::ID id);
 237   bool inline_notify(vmIntrinsics::ID id);
 238   Node* generate_min_max(vmIntrinsics::ID id, Node* x, Node* y);
 239   // This returns Type::AnyPtr, RawPtr, or OopPtr.
 240   int classify_unsafe_addr(Node* &amp;base, Node* &amp;offset);
 241   Node* make_unsafe_address(Node* base, Node* offset);
 242   // Helper for inline_unsafe_access.
 243   // Generates the guards that check whether the result of
 244   // Unsafe.getObject should be recorded in an SATB log buffer.
 245   void insert_pre_barrier(Node* base_oop, Node* offset, Node* pre_val, bool need_mem_bar);
 246 
 247   typedef enum { Relaxed, Opaque, Volatile, Acquire, Release } AccessKind;
 248   bool inline_unsafe_access(bool is_native_ptr, bool is_store, BasicType type, AccessKind kind, bool is_unaligned);
 249   static bool klass_needs_init_guard(Node* kls);
 250   bool inline_unsafe_allocate();
 251   bool inline_unsafe_copyMemory();
 252   bool inline_native_currentThread();
 253 
 254   bool inline_native_time_funcs(address method, const char* funcName);
 255   bool inline_native_isInterrupted();
 256   bool inline_native_Class_query(vmIntrinsics::ID id);
 257   bool inline_native_subtype_check();
 258 
 259   bool inline_native_newArray();
 260   bool inline_native_getLength();
 261   bool inline_array_copyOf(bool is_copyOfRange);
 262   bool inline_array_equals(StrIntrinsicNode::ArgEnc ae);
 263   bool inline_objects_checkIndex();
 264   void copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark);
 265   bool inline_native_clone(bool is_virtual);
 266   bool inline_native_Reflection_getCallerClass();
 267   // Helper function for inlining native object hash method
 268   bool inline_native_hashcode(bool is_virtual, bool is_static);
 269   bool inline_native_getClass();
 270 
 271   // Helper functions for inlining arraycopy
 272   bool inline_arraycopy();
 273   AllocateArrayNode* tightly_coupled_allocation(Node* ptr,
 274                                                 RegionNode* slow_region);
 275   JVMState* arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp);
 276   void arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp);
 277 
 278   typedef enum { LS_get_add, LS_get_set, LS_cmp_swap, LS_cmp_swap_weak, LS_cmp_exchange } LoadStoreKind;
 279   MemNode::MemOrd access_kind_to_memord_LS(AccessKind access_kind, bool is_store);
 280   MemNode::MemOrd access_kind_to_memord(AccessKind access_kind);
 281   bool inline_unsafe_load_store(BasicType type,  LoadStoreKind kind, AccessKind access_kind);
 282   bool inline_unsafe_fence(vmIntrinsics::ID id);
 283   bool inline_fp_conversions(vmIntrinsics::ID id);
 284   bool inline_number_methods(vmIntrinsics::ID id);
 285   bool inline_reference_get();
 286   bool inline_Class_cast();
 287   bool inline_aescrypt_Block(vmIntrinsics::ID id);
 288   bool inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id);
 289   bool inline_counterMode_AESCrypt(vmIntrinsics::ID id);
 290   Node* inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting);
 291   Node* inline_counterMode_AESCrypt_predicate();
 292   Node* get_key_start_from_aescrypt_object(Node* aescrypt_object);
 293   Node* get_original_key_start_from_aescrypt_object(Node* aescrypt_object);
 294   bool inline_ghash_processBlocks();
 295   bool inline_sha_implCompress(vmIntrinsics::ID id);
 296   bool inline_digestBase_implCompressMB(int predicate);
 297   bool inline_sha_implCompressMB(Node* digestBaseObj, ciInstanceKlass* instklass_SHA,
 298                                  bool long_state, address stubAddr, const char *stubName,
 299                                  Node* src_start, Node* ofs, Node* limit);
 300   Node* get_state_from_sha_object(Node *sha_object);
 301   Node* get_state_from_sha5_object(Node *sha_object);
 302   Node* inline_digestBase_implCompressMB_predicate(int predicate);
 303   bool inline_encodeISOArray();
 304   bool inline_updateCRC32();
 305   bool inline_updateBytesCRC32();
 306   bool inline_updateByteBufferCRC32();
 307   Node* get_table_from_crc32c_class(ciInstanceKlass *crc32c_class);
 308   bool inline_updateBytesCRC32C();
 309   bool inline_updateDirectByteBufferCRC32C();
 310   bool inline_updateBytesAdler32();
 311   bool inline_updateByteBufferAdler32();
 312   bool inline_multiplyToLen();
 313   bool inline_hasNegatives();
 314   bool inline_squareToLen();
 315   bool inline_mulAdd();
 316   bool inline_montgomeryMultiply();
 317   bool inline_montgomerySquare();
 318   bool inline_vectorizedMismatch();
 319 
 320   bool inline_profileBoolean();
 321   bool inline_isCompileConstant();
 322 
 323   bool inline_deoptimize();
 324 };
 325 
 326 //---------------------------make_vm_intrinsic----------------------------
 327 CallGenerator* Compile::make_vm_intrinsic(ciMethod* m, bool is_virtual) {
 328   vmIntrinsics::ID id = m-&gt;intrinsic_id();
 329   assert(id != vmIntrinsics::_none, "must be a VM intrinsic");
 330 
 331   if (!m-&gt;is_loaded()) {
 332     // Do not attempt to inline unloaded methods.
 333     return NULL;
 334   }
 335 
 336   C2Compiler* compiler = (C2Compiler*)CompileBroker::compiler(CompLevel_full_optimization);
 337   bool is_available = false;
 338 
 339   {
 340     // For calling is_intrinsic_supported and is_intrinsic_disabled_by_flag
 341     // the compiler must transition to '_thread_in_vm' state because both
 342     // methods access VM-internal data.
 343     VM_ENTRY_MARK;
 344     methodHandle mh(THREAD, m-&gt;get_Method());
 345     is_available = compiler-&gt;is_intrinsic_supported(mh, is_virtual) &amp;&amp;
 346                    !C-&gt;directive()-&gt;is_intrinsic_disabled(mh) &amp;&amp;
 347                    !vmIntrinsics::is_disabled_by_flags(mh);
 348 
 349   }
 350 
 351   if (is_available) {
 352     assert(id &lt;= vmIntrinsics::LAST_COMPILER_INLINE, "caller responsibility");
 353     assert(id != vmIntrinsics::_Object_init &amp;&amp; id != vmIntrinsics::_invoke, "enum out of order?");
 354     return new LibraryIntrinsic(m, is_virtual,
 355                                 vmIntrinsics::predicates_needed(id),
 356                                 vmIntrinsics::does_virtual_dispatch(id),
 357                                 (vmIntrinsics::ID) id);
 358   } else {
 359     return NULL;
 360   }
 361 }
 362 
 363 //----------------------register_library_intrinsics-----------------------
 364 // Initialize this file's data structures, for each Compile instance.
 365 void Compile::register_library_intrinsics() {
 366   // Nothing to do here.
 367 }
 368 
 369 JVMState* LibraryIntrinsic::generate(JVMState* jvms) {
 370   LibraryCallKit kit(jvms, this);
 371   Compile* C = kit.C;
 372   int nodes = C-&gt;unique();
 373 #ifndef PRODUCT
 374   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 375     char buf[1000];
 376     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 377     tty-&gt;print_cr("Intrinsic %s", str);
 378   }
 379 #endif
 380   ciMethod* callee = kit.callee();
 381   const int bci    = kit.bci();
 382 
 383   // Try to inline the intrinsic.
 384   if ((CheckIntrinsics ? callee-&gt;intrinsic_candidate() : true) &amp;&amp;
 385       kit.try_to_inline(_last_predicate)) {
 386     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 387       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual)" : "(intrinsic)");
 388     }
 389     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 390     if (C-&gt;log()) {
 391       C-&gt;log()-&gt;elem("intrinsic id='%s'%s nodes='%d'",
 392                      vmIntrinsics::name_at(intrinsic_id()),
 393                      (is_virtual() ? " virtual='1'" : ""),
 394                      C-&gt;unique() - nodes);
 395     }
 396     // Push the result from the inlined method onto the stack.
 397     kit.push_result();
 398     C-&gt;print_inlining_update(this);
 399     return kit.transfer_exceptions_into_jvms();
 400   }
 401 
 402   // The intrinsic bailed out
 403   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 404     if (jvms-&gt;has_method()) {
 405       // Not a root compile.
 406       const char* msg;
 407       if (callee-&gt;intrinsic_candidate()) {
 408         msg = is_virtual() ? "failed to inline (intrinsic, virtual)" : "failed to inline (intrinsic)";
 409       } else {
 410         msg = is_virtual() ? "failed to inline (intrinsic, virtual), method not annotated"
 411                            : "failed to inline (intrinsic), method not annotated";
 412       }
 413       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, msg);
 414     } else {
 415       // Root compile
 416       tty-&gt;print("Did not generate intrinsic %s%s at bci:%d in",
 417                vmIntrinsics::name_at(intrinsic_id()),
 418                (is_virtual() ? " (virtual)" : ""), bci);
 419     }
 420   }
 421   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 422   C-&gt;print_inlining_update(this);
 423   return NULL;
 424 }
 425 
 426 Node* LibraryIntrinsic::generate_predicate(JVMState* jvms, int predicate) {
 427   LibraryCallKit kit(jvms, this);
 428   Compile* C = kit.C;
 429   int nodes = C-&gt;unique();
 430   _last_predicate = predicate;
 431 #ifndef PRODUCT
 432   assert(is_predicated() &amp;&amp; predicate &lt; predicates_count(), "sanity");
 433   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
 434     char buf[1000];
 435     const char* str = vmIntrinsics::short_name_as_C_string(intrinsic_id(), buf, sizeof(buf));
 436     tty-&gt;print_cr("Predicate for intrinsic %s", str);
 437   }
 438 #endif
 439   ciMethod* callee = kit.callee();
 440   const int bci    = kit.bci();
 441 
 442   Node* slow_ctl = kit.try_to_predicate(predicate);
 443   if (!kit.failing()) {
 444     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 445       C-&gt;print_inlining(callee, jvms-&gt;depth() - 1, bci, is_virtual() ? "(intrinsic, virtual, predicate)" : "(intrinsic, predicate)");
 446     }
 447     C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_worked);
 448     if (C-&gt;log()) {
 449       C-&gt;log()-&gt;elem("predicate_intrinsic id='%s'%s nodes='%d'",
 450                      vmIntrinsics::name_at(intrinsic_id()),
 451                      (is_virtual() ? " virtual='1'" : ""),
 452                      C-&gt;unique() - nodes);
 453     }
 454     return slow_ctl; // Could be NULL if the check folds.
 455   }
 456 
 457   // The intrinsic bailed out
 458   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
 459     if (jvms-&gt;has_method()) {
 460       // Not a root compile.
 461       const char* msg = "failed to generate predicate for intrinsic";
 462       C-&gt;print_inlining(kit.callee(), jvms-&gt;depth() - 1, bci, msg);
 463     } else {
 464       // Root compile
 465       C-&gt;print_inlining_stream()-&gt;print("Did not generate predicate for intrinsic %s%s at bci:%d in",
 466                                         vmIntrinsics::name_at(intrinsic_id()),
 467                                         (is_virtual() ? " (virtual)" : ""), bci);
 468     }
 469   }
 470   C-&gt;gather_intrinsic_statistics(intrinsic_id(), is_virtual(), Compile::_intrinsic_failed);
 471   return NULL;
 472 }
 473 
 474 bool LibraryCallKit::try_to_inline(int predicate) {
 475   // Handle symbolic names for otherwise undistinguished boolean switches:
 476   const bool is_store       = true;
 477   const bool is_compress    = true;
 478   const bool is_native_ptr  = true;
 479   const bool is_static      = true;
 480   const bool is_volatile    = true;
 481 
 482   if (!jvms()-&gt;has_method()) {
 483     // Root JVMState has a null method.
 484     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 485     // Insert the memory aliasing node
 486     set_all_memory(reset_memory());
 487   }
 488   assert(merged_memory(), "");
 489 
 490 
 491   switch (intrinsic_id()) {
 492   case vmIntrinsics::_hashCode:                 return inline_native_hashcode(intrinsic()-&gt;is_virtual(), !is_static);
 493   case vmIntrinsics::_identityHashCode:         return inline_native_hashcode(/*!virtual*/ false,         is_static);
 494   case vmIntrinsics::_getClass:                 return inline_native_getClass();
 495 
 496   case vmIntrinsics::_dsin:
 497   case vmIntrinsics::_dcos:
 498   case vmIntrinsics::_dtan:
 499   case vmIntrinsics::_dabs:
 500   case vmIntrinsics::_datan2:
 501   case vmIntrinsics::_dsqrt:
 502   case vmIntrinsics::_dexp:
 503   case vmIntrinsics::_dlog:
 504   case vmIntrinsics::_dlog10:
 505   case vmIntrinsics::_dpow:                     return inline_math_native(intrinsic_id());
 506 
 507   case vmIntrinsics::_min:
 508   case vmIntrinsics::_max:                      return inline_min_max(intrinsic_id());
 509 
 510   case vmIntrinsics::_notify:
 511   case vmIntrinsics::_notifyAll:
 512     if (InlineNotify) {
 513       return inline_notify(intrinsic_id());
 514     }
 515     return false;
 516 
 517   case vmIntrinsics::_addExactI:                return inline_math_addExactI(false /* add */);
 518   case vmIntrinsics::_addExactL:                return inline_math_addExactL(false /* add */);
 519   case vmIntrinsics::_decrementExactI:          return inline_math_subtractExactI(true /* decrement */);
 520   case vmIntrinsics::_decrementExactL:          return inline_math_subtractExactL(true /* decrement */);
 521   case vmIntrinsics::_incrementExactI:          return inline_math_addExactI(true /* increment */);
 522   case vmIntrinsics::_incrementExactL:          return inline_math_addExactL(true /* increment */);
 523   case vmIntrinsics::_multiplyExactI:           return inline_math_multiplyExactI();
 524   case vmIntrinsics::_multiplyExactL:           return inline_math_multiplyExactL();
 525   case vmIntrinsics::_negateExactI:             return inline_math_negateExactI();
 526   case vmIntrinsics::_negateExactL:             return inline_math_negateExactL();
 527   case vmIntrinsics::_subtractExactI:           return inline_math_subtractExactI(false /* subtract */);
 528   case vmIntrinsics::_subtractExactL:           return inline_math_subtractExactL(false /* subtract */);
 529 
 530   case vmIntrinsics::_arraycopy:                return inline_arraycopy();
 531 
 532   case vmIntrinsics::_compareToL:               return inline_string_compareTo(StrIntrinsicNode::LL);
 533   case vmIntrinsics::_compareToU:               return inline_string_compareTo(StrIntrinsicNode::UU);
 534   case vmIntrinsics::_compareToLU:              return inline_string_compareTo(StrIntrinsicNode::LU);
 535   case vmIntrinsics::_compareToUL:              return inline_string_compareTo(StrIntrinsicNode::UL);
 536 
 537   case vmIntrinsics::_indexOfL:                 return inline_string_indexOf(StrIntrinsicNode::LL);
 538   case vmIntrinsics::_indexOfU:                 return inline_string_indexOf(StrIntrinsicNode::UU);
 539   case vmIntrinsics::_indexOfUL:                return inline_string_indexOf(StrIntrinsicNode::UL);
 540   case vmIntrinsics::_indexOfIL:                return inline_string_indexOfI(StrIntrinsicNode::LL);
 541   case vmIntrinsics::_indexOfIU:                return inline_string_indexOfI(StrIntrinsicNode::UU);
 542   case vmIntrinsics::_indexOfIUL:               return inline_string_indexOfI(StrIntrinsicNode::UL);
 543   case vmIntrinsics::_indexOfU_char:            return inline_string_indexOfChar();
 544 
 545   case vmIntrinsics::_equalsL:                  return inline_string_equals(StrIntrinsicNode::LL);
 546   case vmIntrinsics::_equalsU:                  return inline_string_equals(StrIntrinsicNode::UU);
 547 
 548   case vmIntrinsics::_toBytesStringU:           return inline_string_toBytesU();
 549   case vmIntrinsics::_getCharsStringU:          return inline_string_getCharsU();
 550   case vmIntrinsics::_getCharStringU:           return inline_string_char_access(!is_store);
 551   case vmIntrinsics::_putCharStringU:           return inline_string_char_access( is_store);
 552 
 553   case vmIntrinsics::_compressStringC:
 554   case vmIntrinsics::_compressStringB:          return inline_string_copy( is_compress);
 555   case vmIntrinsics::_inflateStringC:
 556   case vmIntrinsics::_inflateStringB:           return inline_string_copy(!is_compress);
 557 
 558   case vmIntrinsics::_getObject:                return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   Relaxed, false);
 559   case vmIntrinsics::_getBoolean:               return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  Relaxed, false);
 560   case vmIntrinsics::_getByte:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     Relaxed, false);
 561   case vmIntrinsics::_getShort:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    Relaxed, false);
 562   case vmIntrinsics::_getChar:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     Relaxed, false);
 563   case vmIntrinsics::_getInt:                   return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      Relaxed, false);
 564   case vmIntrinsics::_getLong:                  return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     Relaxed, false);
 565   case vmIntrinsics::_getFloat:                 return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    Relaxed, false);
 566   case vmIntrinsics::_getDouble:                return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   Relaxed, false);
 567 
 568   case vmIntrinsics::_putObject:                return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   Relaxed, false);
 569   case vmIntrinsics::_putBoolean:               return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  Relaxed, false);
 570   case vmIntrinsics::_putByte:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     Relaxed, false);
 571   case vmIntrinsics::_putShort:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    Relaxed, false);
 572   case vmIntrinsics::_putChar:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     Relaxed, false);
 573   case vmIntrinsics::_putInt:                   return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      Relaxed, false);
 574   case vmIntrinsics::_putLong:                  return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     Relaxed, false);
 575   case vmIntrinsics::_putFloat:                 return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    Relaxed, false);
 576   case vmIntrinsics::_putDouble:                return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   Relaxed, false);
 577 
 578   case vmIntrinsics::_getByte_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_BYTE,     Relaxed, false);
 579   case vmIntrinsics::_getShort_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_SHORT,    Relaxed, false);
 580   case vmIntrinsics::_getChar_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_CHAR,     Relaxed, false);
 581   case vmIntrinsics::_getInt_raw:               return inline_unsafe_access( is_native_ptr, !is_store, T_INT,      Relaxed, false);
 582   case vmIntrinsics::_getLong_raw:              return inline_unsafe_access( is_native_ptr, !is_store, T_LONG,     Relaxed, false);
 583   case vmIntrinsics::_getFloat_raw:             return inline_unsafe_access( is_native_ptr, !is_store, T_FLOAT,    Relaxed, false);
 584   case vmIntrinsics::_getDouble_raw:            return inline_unsafe_access( is_native_ptr, !is_store, T_DOUBLE,   Relaxed, false);
 585   case vmIntrinsics::_getAddress_raw:           return inline_unsafe_access( is_native_ptr, !is_store, T_ADDRESS,  Relaxed, false);
 586 
 587   case vmIntrinsics::_putByte_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_BYTE,     Relaxed, false);
 588   case vmIntrinsics::_putShort_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_SHORT,    Relaxed, false);
 589   case vmIntrinsics::_putChar_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_CHAR,     Relaxed, false);
 590   case vmIntrinsics::_putInt_raw:               return inline_unsafe_access( is_native_ptr,  is_store, T_INT,      Relaxed, false);
 591   case vmIntrinsics::_putLong_raw:              return inline_unsafe_access( is_native_ptr,  is_store, T_LONG,     Relaxed, false);
 592   case vmIntrinsics::_putFloat_raw:             return inline_unsafe_access( is_native_ptr,  is_store, T_FLOAT,    Relaxed, false);
 593   case vmIntrinsics::_putDouble_raw:            return inline_unsafe_access( is_native_ptr,  is_store, T_DOUBLE,   Relaxed, false);
 594   case vmIntrinsics::_putAddress_raw:           return inline_unsafe_access( is_native_ptr,  is_store, T_ADDRESS,  Relaxed, false);
 595 
 596   case vmIntrinsics::_getObjectVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   Volatile, false);
 597   case vmIntrinsics::_getBooleanVolatile:       return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  Volatile, false);
 598   case vmIntrinsics::_getByteVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     Volatile, false);
 599   case vmIntrinsics::_getShortVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    Volatile, false);
 600   case vmIntrinsics::_getCharVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     Volatile, false);
 601   case vmIntrinsics::_getIntVolatile:           return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      Volatile, false);
 602   case vmIntrinsics::_getLongVolatile:          return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     Volatile, false);
 603   case vmIntrinsics::_getFloatVolatile:         return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    Volatile, false);
 604   case vmIntrinsics::_getDoubleVolatile:        return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   Volatile, false);
 605 
 606   case vmIntrinsics::_putObjectVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   Volatile, false);
 607   case vmIntrinsics::_putBooleanVolatile:       return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  Volatile, false);
 608   case vmIntrinsics::_putByteVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     Volatile, false);
 609   case vmIntrinsics::_putShortVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    Volatile, false);
 610   case vmIntrinsics::_putCharVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     Volatile, false);
 611   case vmIntrinsics::_putIntVolatile:           return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      Volatile, false);
 612   case vmIntrinsics::_putLongVolatile:          return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     Volatile, false);
 613   case vmIntrinsics::_putFloatVolatile:         return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    Volatile, false);
 614   case vmIntrinsics::_putDoubleVolatile:        return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   Volatile, false);
 615 
 616   case vmIntrinsics::_getShortUnaligned:        return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    Relaxed, true);
 617   case vmIntrinsics::_getCharUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     Relaxed, true);
 618   case vmIntrinsics::_getIntUnaligned:          return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      Relaxed, true);
 619   case vmIntrinsics::_getLongUnaligned:         return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     Relaxed, true);
 620 
 621   case vmIntrinsics::_putShortUnaligned:        return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    Relaxed, true);
 622   case vmIntrinsics::_putCharUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     Relaxed, true);
 623   case vmIntrinsics::_putIntUnaligned:          return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      Relaxed, true);
 624   case vmIntrinsics::_putLongUnaligned:         return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     Relaxed, true);
 625 
 626   case vmIntrinsics::_putOrderedObject:         return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   Release, false);
 627   case vmIntrinsics::_putOrderedInt:            return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      Release, false);
 628   case vmIntrinsics::_putOrderedLong:           return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     Release, false);
 629 
 630   case vmIntrinsics::_getObjectAcquire:         return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   Acquire, false);
 631   case vmIntrinsics::_getBooleanAcquire:        return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  Acquire, false);
 632   case vmIntrinsics::_getByteAcquire:           return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     Acquire, false);
 633   case vmIntrinsics::_getShortAcquire:          return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    Acquire, false);
 634   case vmIntrinsics::_getCharAcquire:           return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     Acquire, false);
 635   case vmIntrinsics::_getIntAcquire:            return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      Acquire, false);
 636   case vmIntrinsics::_getLongAcquire:           return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     Acquire, false);
 637   case vmIntrinsics::_getFloatAcquire:          return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    Acquire, false);
 638   case vmIntrinsics::_getDoubleAcquire:         return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   Acquire, false);
 639 
 640   case vmIntrinsics::_putObjectRelease:         return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   Release, false);
 641   case vmIntrinsics::_putBooleanRelease:        return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  Release, false);
 642   case vmIntrinsics::_putByteRelease:           return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     Release, false);
 643   case vmIntrinsics::_putShortRelease:          return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    Release, false);
 644   case vmIntrinsics::_putCharRelease:           return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     Release, false);
 645   case vmIntrinsics::_putIntRelease:            return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      Release, false);
 646   case vmIntrinsics::_putLongRelease:           return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     Release, false);
 647   case vmIntrinsics::_putFloatRelease:          return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    Release, false);
 648   case vmIntrinsics::_putDoubleRelease:         return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   Release, false);
 649 
 650   case vmIntrinsics::_getObjectOpaque:          return inline_unsafe_access(!is_native_ptr, !is_store, T_OBJECT,   Opaque, false);
 651   case vmIntrinsics::_getBooleanOpaque:         return inline_unsafe_access(!is_native_ptr, !is_store, T_BOOLEAN,  Opaque, false);
 652   case vmIntrinsics::_getByteOpaque:            return inline_unsafe_access(!is_native_ptr, !is_store, T_BYTE,     Opaque, false);
 653   case vmIntrinsics::_getShortOpaque:           return inline_unsafe_access(!is_native_ptr, !is_store, T_SHORT,    Opaque, false);
 654   case vmIntrinsics::_getCharOpaque:            return inline_unsafe_access(!is_native_ptr, !is_store, T_CHAR,     Opaque, false);
 655   case vmIntrinsics::_getIntOpaque:             return inline_unsafe_access(!is_native_ptr, !is_store, T_INT,      Opaque, false);
 656   case vmIntrinsics::_getLongOpaque:            return inline_unsafe_access(!is_native_ptr, !is_store, T_LONG,     Opaque, false);
 657   case vmIntrinsics::_getFloatOpaque:           return inline_unsafe_access(!is_native_ptr, !is_store, T_FLOAT,    Opaque, false);
 658   case vmIntrinsics::_getDoubleOpaque:          return inline_unsafe_access(!is_native_ptr, !is_store, T_DOUBLE,   Opaque, false);
 659 
 660   case vmIntrinsics::_putObjectOpaque:          return inline_unsafe_access(!is_native_ptr,  is_store, T_OBJECT,   Opaque, false);
 661   case vmIntrinsics::_putBooleanOpaque:         return inline_unsafe_access(!is_native_ptr,  is_store, T_BOOLEAN,  Opaque, false);
 662   case vmIntrinsics::_putByteOpaque:            return inline_unsafe_access(!is_native_ptr,  is_store, T_BYTE,     Opaque, false);
 663   case vmIntrinsics::_putShortOpaque:           return inline_unsafe_access(!is_native_ptr,  is_store, T_SHORT,    Opaque, false);
 664   case vmIntrinsics::_putCharOpaque:            return inline_unsafe_access(!is_native_ptr,  is_store, T_CHAR,     Opaque, false);
 665   case vmIntrinsics::_putIntOpaque:             return inline_unsafe_access(!is_native_ptr,  is_store, T_INT,      Opaque, false);
 666   case vmIntrinsics::_putLongOpaque:            return inline_unsafe_access(!is_native_ptr,  is_store, T_LONG,     Opaque, false);
 667   case vmIntrinsics::_putFloatOpaque:           return inline_unsafe_access(!is_native_ptr,  is_store, T_FLOAT,    Opaque, false);
 668   case vmIntrinsics::_putDoubleOpaque:          return inline_unsafe_access(!is_native_ptr,  is_store, T_DOUBLE,   Opaque, false);
 669 
 670   case vmIntrinsics::_compareAndSwapObject:             return inline_unsafe_load_store(T_OBJECT, LS_cmp_swap,      Volatile);
 671   case vmIntrinsics::_compareAndSwapInt:                return inline_unsafe_load_store(T_INT,    LS_cmp_swap,      Volatile);
 672   case vmIntrinsics::_compareAndSwapLong:               return inline_unsafe_load_store(T_LONG,   LS_cmp_swap,      Volatile);
 673 
 674   case vmIntrinsics::_weakCompareAndSwapObject:         return inline_unsafe_load_store(T_OBJECT, LS_cmp_swap_weak, Relaxed);
 675   case vmIntrinsics::_weakCompareAndSwapObjectAcquire:  return inline_unsafe_load_store(T_OBJECT, LS_cmp_swap_weak, Acquire);
 676   case vmIntrinsics::_weakCompareAndSwapObjectRelease:  return inline_unsafe_load_store(T_OBJECT, LS_cmp_swap_weak, Release);
 677   case vmIntrinsics::_weakCompareAndSwapInt:            return inline_unsafe_load_store(T_INT,    LS_cmp_swap_weak, Relaxed);
 678   case vmIntrinsics::_weakCompareAndSwapIntAcquire:     return inline_unsafe_load_store(T_INT,    LS_cmp_swap_weak, Acquire);
 679   case vmIntrinsics::_weakCompareAndSwapIntRelease:     return inline_unsafe_load_store(T_INT,    LS_cmp_swap_weak, Release);
 680   case vmIntrinsics::_weakCompareAndSwapLong:           return inline_unsafe_load_store(T_LONG,   LS_cmp_swap_weak, Relaxed);
 681   case vmIntrinsics::_weakCompareAndSwapLongAcquire:    return inline_unsafe_load_store(T_LONG,   LS_cmp_swap_weak, Acquire);
 682   case vmIntrinsics::_weakCompareAndSwapLongRelease:    return inline_unsafe_load_store(T_LONG,   LS_cmp_swap_weak, Release);
 683 
 684   case vmIntrinsics::_compareAndExchangeObjectVolatile: return inline_unsafe_load_store(T_OBJECT, LS_cmp_exchange,  Volatile);
 685   case vmIntrinsics::_compareAndExchangeObjectAcquire:  return inline_unsafe_load_store(T_OBJECT, LS_cmp_exchange,  Acquire);
 686   case vmIntrinsics::_compareAndExchangeObjectRelease:  return inline_unsafe_load_store(T_OBJECT, LS_cmp_exchange,  Release);
 687   case vmIntrinsics::_compareAndExchangeIntVolatile:    return inline_unsafe_load_store(T_INT,    LS_cmp_exchange,  Volatile);
 688   case vmIntrinsics::_compareAndExchangeIntAcquire:     return inline_unsafe_load_store(T_INT,    LS_cmp_exchange,  Acquire);
 689   case vmIntrinsics::_compareAndExchangeIntRelease:     return inline_unsafe_load_store(T_INT,    LS_cmp_exchange,  Release);
 690   case vmIntrinsics::_compareAndExchangeLongVolatile:   return inline_unsafe_load_store(T_LONG,   LS_cmp_exchange,  Volatile);
 691   case vmIntrinsics::_compareAndExchangeLongAcquire:    return inline_unsafe_load_store(T_LONG,   LS_cmp_exchange,  Acquire);
 692   case vmIntrinsics::_compareAndExchangeLongRelease:    return inline_unsafe_load_store(T_LONG,   LS_cmp_exchange,  Release);
 693 
 694   case vmIntrinsics::_getAndAddInt:                     return inline_unsafe_load_store(T_INT,    LS_get_add,       Volatile);
 695   case vmIntrinsics::_getAndAddLong:                    return inline_unsafe_load_store(T_LONG,   LS_get_add,       Volatile);
 696   case vmIntrinsics::_getAndSetInt:                     return inline_unsafe_load_store(T_INT,    LS_get_set,       Volatile);
 697   case vmIntrinsics::_getAndSetLong:                    return inline_unsafe_load_store(T_LONG,   LS_get_set,       Volatile);
 698   case vmIntrinsics::_getAndSetObject:                  return inline_unsafe_load_store(T_OBJECT, LS_get_set,       Volatile);
 699 
 700   case vmIntrinsics::_loadFence:
 701   case vmIntrinsics::_storeFence:
 702   case vmIntrinsics::_fullFence:                return inline_unsafe_fence(intrinsic_id());
 703 
 704   case vmIntrinsics::_currentThread:            return inline_native_currentThread();
 705   case vmIntrinsics::_isInterrupted:            return inline_native_isInterrupted();
 706 
 707 #ifdef TRACE_HAVE_INTRINSICS
 708   case vmIntrinsics::_counterTime:              return inline_native_time_funcs(CAST_FROM_FN_PTR(address, TRACE_TIME_METHOD), "counterTime");
 709 #endif
 710   case vmIntrinsics::_currentTimeMillis:        return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeMillis), "currentTimeMillis");
 711   case vmIntrinsics::_nanoTime:                 return inline_native_time_funcs(CAST_FROM_FN_PTR(address, os::javaTimeNanos), "nanoTime");
 712   case vmIntrinsics::_allocateInstance:         return inline_unsafe_allocate();
 713   case vmIntrinsics::_copyMemory:               return inline_unsafe_copyMemory();
 714   case vmIntrinsics::_newArray:                 return inline_native_newArray();
 715   case vmIntrinsics::_getLength:                return inline_native_getLength();
 716   case vmIntrinsics::_copyOf:                   return inline_array_copyOf(false);
 717   case vmIntrinsics::_copyOfRange:              return inline_array_copyOf(true);
 718   case vmIntrinsics::_equalsB:                  return inline_array_equals(StrIntrinsicNode::LL);
 719   case vmIntrinsics::_equalsC:                  return inline_array_equals(StrIntrinsicNode::UU);
 720   case vmIntrinsics::_Objects_checkIndex:       return inline_objects_checkIndex();
 721   case vmIntrinsics::_clone:                    return inline_native_clone(intrinsic()-&gt;is_virtual());
 722 
 723   case vmIntrinsics::_isAssignableFrom:         return inline_native_subtype_check();
 724 
 725   case vmIntrinsics::_isInstance:
 726   case vmIntrinsics::_getModifiers:
 727   case vmIntrinsics::_isInterface:
 728   case vmIntrinsics::_isArray:
 729   case vmIntrinsics::_isPrimitive:
 730   case vmIntrinsics::_getSuperclass:
 731   case vmIntrinsics::_getClassAccessFlags:      return inline_native_Class_query(intrinsic_id());
 732 
 733   case vmIntrinsics::_floatToRawIntBits:
 734   case vmIntrinsics::_floatToIntBits:
 735   case vmIntrinsics::_intBitsToFloat:
 736   case vmIntrinsics::_doubleToRawLongBits:
 737   case vmIntrinsics::_doubleToLongBits:
 738   case vmIntrinsics::_longBitsToDouble:         return inline_fp_conversions(intrinsic_id());
 739 
 740   case vmIntrinsics::_numberOfLeadingZeros_i:
 741   case vmIntrinsics::_numberOfLeadingZeros_l:
 742   case vmIntrinsics::_numberOfTrailingZeros_i:
 743   case vmIntrinsics::_numberOfTrailingZeros_l:
 744   case vmIntrinsics::_bitCount_i:
 745   case vmIntrinsics::_bitCount_l:
 746   case vmIntrinsics::_reverseBytes_i:
 747   case vmIntrinsics::_reverseBytes_l:
 748   case vmIntrinsics::_reverseBytes_s:
 749   case vmIntrinsics::_reverseBytes_c:           return inline_number_methods(intrinsic_id());
 750 
 751   case vmIntrinsics::_getCallerClass:           return inline_native_Reflection_getCallerClass();
 752 
 753   case vmIntrinsics::_Reference_get:            return inline_reference_get();
 754 
 755   case vmIntrinsics::_Class_cast:               return inline_Class_cast();
 756 
 757   case vmIntrinsics::_aescrypt_encryptBlock:
 758   case vmIntrinsics::_aescrypt_decryptBlock:    return inline_aescrypt_Block(intrinsic_id());
 759 
 760   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 761   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 762     return inline_cipherBlockChaining_AESCrypt(intrinsic_id());
 763 
 764   case vmIntrinsics::_counterMode_AESCrypt:
 765     return inline_counterMode_AESCrypt(intrinsic_id());
 766 
 767   case vmIntrinsics::_sha_implCompress:
 768   case vmIntrinsics::_sha2_implCompress:
 769   case vmIntrinsics::_sha5_implCompress:
 770     return inline_sha_implCompress(intrinsic_id());
 771 
 772   case vmIntrinsics::_digestBase_implCompressMB:
 773     return inline_digestBase_implCompressMB(predicate);
 774 
 775   case vmIntrinsics::_multiplyToLen:
 776     return inline_multiplyToLen();
 777 
 778   case vmIntrinsics::_squareToLen:
 779     return inline_squareToLen();
 780 
 781   case vmIntrinsics::_mulAdd:
 782     return inline_mulAdd();
 783 
 784   case vmIntrinsics::_montgomeryMultiply:
 785     return inline_montgomeryMultiply();
 786   case vmIntrinsics::_montgomerySquare:
 787     return inline_montgomerySquare();
 788 
 789   case vmIntrinsics::_vectorizedMismatch:
 790     return inline_vectorizedMismatch();
 791 
 792   case vmIntrinsics::_ghash_processBlocks:
 793     return inline_ghash_processBlocks();
 794 
 795   case vmIntrinsics::_encodeISOArray:
 796   case vmIntrinsics::_encodeByteISOArray:
 797     return inline_encodeISOArray();
 798 
 799   case vmIntrinsics::_updateCRC32:
 800     return inline_updateCRC32();
 801   case vmIntrinsics::_updateBytesCRC32:
 802     return inline_updateBytesCRC32();
 803   case vmIntrinsics::_updateByteBufferCRC32:
 804     return inline_updateByteBufferCRC32();
 805 
 806   case vmIntrinsics::_updateBytesCRC32C:
 807     return inline_updateBytesCRC32C();
 808   case vmIntrinsics::_updateDirectByteBufferCRC32C:
 809     return inline_updateDirectByteBufferCRC32C();
 810 
 811   case vmIntrinsics::_updateBytesAdler32:
 812     return inline_updateBytesAdler32();
 813   case vmIntrinsics::_updateByteBufferAdler32:
 814     return inline_updateByteBufferAdler32();
 815 
 816   case vmIntrinsics::_profileBoolean:
 817     return inline_profileBoolean();
 818   case vmIntrinsics::_isCompileConstant:
 819     return inline_isCompileConstant();
 820 
 821   case vmIntrinsics::_hasNegatives:
 822     return inline_hasNegatives();
 823 
 824   case vmIntrinsics::_deoptimize:
 825     return inline_deoptimize();
 826 
 827   default:
 828     // If you get here, it may be that someone has added a new intrinsic
 829     // to the list in vmSymbols.hpp without implementing it here.
 830 #ifndef PRODUCT
 831     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 832       tty-&gt;print_cr("*** Warning: Unimplemented intrinsic %s(%d)",
 833                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 834     }
 835 #endif
 836     return false;
 837   }
 838 }
 839 
 840 Node* LibraryCallKit::try_to_predicate(int predicate) {
 841   if (!jvms()-&gt;has_method()) {
 842     // Root JVMState has a null method.
 843     assert(map()-&gt;memory()-&gt;Opcode() == Op_Parm, "");
 844     // Insert the memory aliasing node
 845     set_all_memory(reset_memory());
 846   }
 847   assert(merged_memory(), "");
 848 
 849   switch (intrinsic_id()) {
 850   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
 851     return inline_cipherBlockChaining_AESCrypt_predicate(false);
 852   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
 853     return inline_cipherBlockChaining_AESCrypt_predicate(true);
 854   case vmIntrinsics::_counterMode_AESCrypt:
 855     return inline_counterMode_AESCrypt_predicate();
 856   case vmIntrinsics::_digestBase_implCompressMB:
 857     return inline_digestBase_implCompressMB_predicate(predicate);
 858 
 859   default:
 860     // If you get here, it may be that someone has added a new intrinsic
 861     // to the list in vmSymbols.hpp without implementing it here.
 862 #ifndef PRODUCT
 863     if ((PrintMiscellaneous &amp;&amp; (Verbose || WizardMode)) || PrintOpto) {
 864       tty-&gt;print_cr("*** Warning: Unimplemented predicate for intrinsic %s(%d)",
 865                     vmIntrinsics::name_at(intrinsic_id()), intrinsic_id());
 866     }
 867 #endif
 868     Node* slow_ctl = control();
 869     set_control(top()); // No fast path instrinsic
 870     return slow_ctl;
 871   }
 872 }
 873 
 874 //------------------------------set_result-------------------------------
 875 // Helper function for finishing intrinsics.
 876 void LibraryCallKit::set_result(RegionNode* region, PhiNode* value) {
 877   record_for_igvn(region);
 878   set_control(_gvn.transform(region));
 879   set_result( _gvn.transform(value));
 880   assert(value-&gt;type()-&gt;basic_type() == result()-&gt;bottom_type()-&gt;basic_type(), "sanity");
 881 }
 882 
 883 //------------------------------generate_guard---------------------------
 884 // Helper function for generating guarded fast-slow graph structures.
 885 // The given 'test', if true, guards a slow path.  If the test fails
 886 // then a fast path can be taken.  (We generally hope it fails.)
 887 // In all cases, GraphKit::control() is updated to the fast path.
 888 // The returned value represents the control for the slow path.
 889 // The return value is never 'top'; it is either a valid control
 890 // or NULL if it is obvious that the slow path can never be taken.
 891 // Also, if region and the slow control are not NULL, the slow edge
 892 // is appended to the region.
 893 Node* LibraryCallKit::generate_guard(Node* test, RegionNode* region, float true_prob) {
 894   if (stopped()) {
 895     // Already short circuited.
 896     return NULL;
 897   }
 898 
 899   // Build an if node and its projections.
 900   // If test is true we take the slow path, which we assume is uncommon.
 901   if (_gvn.type(test) == TypeInt::ZERO) {
 902     // The slow branch is never taken.  No need to build this guard.
 903     return NULL;
 904   }
 905 
 906   IfNode* iff = create_and_map_if(control(), test, true_prob, COUNT_UNKNOWN);
 907 
 908   Node* if_slow = _gvn.transform(new IfTrueNode(iff));
 909   if (if_slow == top()) {
 910     // The slow branch is never taken.  No need to build this guard.
 911     return NULL;
 912   }
 913 
 914   if (region != NULL)
 915     region-&gt;add_req(if_slow);
 916 
 917   Node* if_fast = _gvn.transform(new IfFalseNode(iff));
 918   set_control(if_fast);
 919 
 920   return if_slow;
 921 }
 922 
 923 inline Node* LibraryCallKit::generate_slow_guard(Node* test, RegionNode* region) {
 924   return generate_guard(test, region, PROB_UNLIKELY_MAG(3));
 925 }
 926 inline Node* LibraryCallKit::generate_fair_guard(Node* test, RegionNode* region) {
 927   return generate_guard(test, region, PROB_FAIR);
 928 }
 929 
 930 inline Node* LibraryCallKit::generate_negative_guard(Node* index, RegionNode* region,
 931                                                      Node* *pos_index) {
 932   if (stopped())
 933     return NULL;                // already stopped
 934   if (_gvn.type(index)-&gt;higher_equal(TypeInt::POS)) // [0,maxint]
 935     return NULL;                // index is already adequately typed
 936   Node* cmp_lt = _gvn.transform(new CmpINode(index, intcon(0)));
 937   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
 938   Node* is_neg = generate_guard(bol_lt, region, PROB_MIN);
 939   if (is_neg != NULL &amp;&amp; pos_index != NULL) {
 940     // Emulate effect of Parse::adjust_map_after_if.
 941     Node* ccast = new CastIINode(index, TypeInt::POS);
 942     ccast-&gt;set_req(0, control());
 943     (*pos_index) = _gvn.transform(ccast);
 944   }
 945   return is_neg;
 946 }
 947 
 948 // Make sure that 'position' is a valid limit index, in [0..length].
 949 // There are two equivalent plans for checking this:
 950 //   A. (offset + copyLength)  unsigned&lt;=  arrayLength
 951 //   B. offset  &lt;=  (arrayLength - copyLength)
 952 // We require that all of the values above, except for the sum and
 953 // difference, are already known to be non-negative.
 954 // Plan A is robust in the face of overflow, if offset and copyLength
 955 // are both hugely positive.
 956 //
 957 // Plan B is less direct and intuitive, but it does not overflow at
 958 // all, since the difference of two non-negatives is always
 959 // representable.  Whenever Java methods must perform the equivalent
 960 // check they generally use Plan B instead of Plan A.
 961 // For the moment we use Plan A.
 962 inline Node* LibraryCallKit::generate_limit_guard(Node* offset,
 963                                                   Node* subseq_length,
 964                                                   Node* array_length,
 965                                                   RegionNode* region) {
 966   if (stopped())
 967     return NULL;                // already stopped
 968   bool zero_offset = _gvn.type(offset) == TypeInt::ZERO;
 969   if (zero_offset &amp;&amp; subseq_length-&gt;eqv_uncast(array_length))
 970     return NULL;                // common case of whole-array copy
 971   Node* last = subseq_length;
 972   if (!zero_offset)             // last += offset
 973     last = _gvn.transform(new AddINode(last, offset));
 974   Node* cmp_lt = _gvn.transform(new CmpUNode(array_length, last));
 975   Node* bol_lt = _gvn.transform(new BoolNode(cmp_lt, BoolTest::lt));
 976   Node* is_over = generate_guard(bol_lt, region, PROB_MIN);
 977   return is_over;
 978 }
 979 
 980 // Emit range checks for the given String.value byte array
 981 void LibraryCallKit::generate_string_range_check(Node* array, Node* offset, Node* count, bool char_count) {
 982   if (stopped()) {
 983     return; // already stopped
 984   }
 985   RegionNode* bailout = new RegionNode(1);
 986   record_for_igvn(bailout);
 987   if (char_count) {
 988     // Convert char count to byte count
 989     count = _gvn.transform(new LShiftINode(count, intcon(1)));
 990   }
 991 
 992   // Offset and count must not be negative
 993   generate_negative_guard(offset, bailout);
 994   generate_negative_guard(count, bailout);
 995   // Offset + count must not exceed length of array
 996   generate_limit_guard(offset, count, load_array_length(array), bailout);
 997 
 998   if (bailout-&gt;req() &gt; 1) {
 999     PreserveJVMState pjvms(this);
1000     set_control(_gvn.transform(bailout));
1001     uncommon_trap(Deoptimization::Reason_intrinsic,
1002                   Deoptimization::Action_maybe_recompile);
1003   }
1004 }
1005 
1006 //--------------------------generate_current_thread--------------------
1007 Node* LibraryCallKit::generate_current_thread(Node* &amp;tls_output) {
1008   ciKlass*    thread_klass = env()-&gt;Thread_klass();
1009   const Type* thread_type  = TypeOopPtr::make_from_klass(thread_klass)-&gt;cast_to_ptr_type(TypePtr::NotNull);
1010   Node* thread = _gvn.transform(new ThreadLocalNode());
1011   Node* p = basic_plus_adr(top()/*!oop*/, thread, in_bytes(JavaThread::threadObj_offset()));
1012   Node* threadObj = make_load(NULL, p, thread_type, T_OBJECT, MemNode::unordered);
1013   tls_output = thread;
1014   return threadObj;
1015 }
1016 
1017 
1018 //------------------------------make_string_method_node------------------------
1019 // Helper method for String intrinsic functions. This version is called with
1020 // str1 and str2 pointing to byte[] nodes containing Latin1 or UTF16 encoded
1021 // characters (depending on 'is_byte'). cnt1 and cnt2 are pointing to Int nodes
1022 // containing the lengths of str1 and str2.
1023 Node* LibraryCallKit::make_string_method_node(int opcode, Node* str1_start, Node* cnt1, Node* str2_start, Node* cnt2, StrIntrinsicNode::ArgEnc ae) {
1024   Node* result = NULL;
1025   switch (opcode) {
1026   case Op_StrIndexOf:
1027     result = new StrIndexOfNode(control(), memory(TypeAryPtr::BYTES),
1028                                 str1_start, cnt1, str2_start, cnt2, ae);
1029     break;
1030   case Op_StrComp:
1031     result = new StrCompNode(control(), memory(TypeAryPtr::BYTES),
1032                              str1_start, cnt1, str2_start, cnt2, ae);
1033     break;
1034   case Op_StrEquals:
1035     // We already know that cnt1 == cnt2 here (checked in 'inline_string_equals').
1036     // Use the constant length if there is one because optimized match rule may exist.
1037     result = new StrEqualsNode(control(), memory(TypeAryPtr::BYTES),
1038                                str1_start, str2_start, cnt2-&gt;is_Con() ? cnt2 : cnt1, ae);
1039     break;
1040   default:
1041     ShouldNotReachHere();
1042     return NULL;
1043   }
1044 
1045   // All these intrinsics have checks.
1046   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1047 
1048   return _gvn.transform(result);
1049 }
1050 
1051 //------------------------------inline_string_compareTo------------------------
1052 bool LibraryCallKit::inline_string_compareTo(StrIntrinsicNode::ArgEnc ae) {
1053   Node* arg1 = argument(0);
1054   Node* arg2 = argument(1);
1055 
1056   // Get start addr and length of first argument
1057   Node* arg1_start  = array_element_address(arg1, intcon(0), T_BYTE);
1058   Node* arg1_cnt    = load_array_length(arg1);
1059 
1060   // Get start addr and length of second argument
1061   Node* arg2_start  = array_element_address(arg2, intcon(0), T_BYTE);
1062   Node* arg2_cnt    = load_array_length(arg2);
1063 
1064   Node* result = make_string_method_node(Op_StrComp, arg1_start, arg1_cnt, arg2_start, arg2_cnt, ae);
1065   set_result(result);
1066   return true;
1067 }
1068 
1069 //------------------------------inline_string_equals------------------------
1070 bool LibraryCallKit::inline_string_equals(StrIntrinsicNode::ArgEnc ae) {
1071   Node* arg1 = argument(0);
1072   Node* arg2 = argument(1);
1073 
1074   // paths (plus control) merge
1075   RegionNode* region = new RegionNode(3);
1076   Node* phi = new PhiNode(region, TypeInt::BOOL);
1077 
1078   if (!stopped()) {
1079     // Get start addr and length of first argument
1080     Node* arg1_start  = array_element_address(arg1, intcon(0), T_BYTE);
1081     Node* arg1_cnt    = load_array_length(arg1);
1082 
1083     // Get start addr and length of second argument
1084     Node* arg2_start  = array_element_address(arg2, intcon(0), T_BYTE);
1085     Node* arg2_cnt    = load_array_length(arg2);
1086 
1087     // Check for arg1_cnt != arg2_cnt
1088     Node* cmp = _gvn.transform(new CmpINode(arg1_cnt, arg2_cnt));
1089     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
1090     Node* if_ne = generate_slow_guard(bol, NULL);
1091     if (if_ne != NULL) {
1092       phi-&gt;init_req(2, intcon(0));
1093       region-&gt;init_req(2, if_ne);
1094     }
1095 
1096     // Check for count == 0 is done by assembler code for StrEquals.
1097 
1098     if (!stopped()) {
1099       Node* equals = make_string_method_node(Op_StrEquals, arg1_start, arg1_cnt, arg2_start, arg2_cnt, ae);
1100       phi-&gt;init_req(1, equals);
1101       region-&gt;init_req(1, control());
1102     }
1103   }
1104 
1105   // post merge
1106   set_control(_gvn.transform(region));
1107   record_for_igvn(region);
1108 
1109   set_result(_gvn.transform(phi));
1110   return true;
1111 }
1112 
1113 //------------------------------inline_array_equals----------------------------
1114 bool LibraryCallKit::inline_array_equals(StrIntrinsicNode::ArgEnc ae) {
1115   assert(ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::LL, "unsupported array types");
1116   Node* arg1 = argument(0);
1117   Node* arg2 = argument(1);
1118 
1119   const TypeAryPtr* mtype = (ae == StrIntrinsicNode::UU) ? TypeAryPtr::CHARS : TypeAryPtr::BYTES;
1120   set_result(_gvn.transform(new AryEqNode(control(), memory(mtype), arg1, arg2, ae)));
1121   return true;
1122 }
1123 
1124 //------------------------------inline_hasNegatives------------------------------
1125 bool LibraryCallKit::inline_hasNegatives() {
1126   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1127     return false;
1128   }
1129 
1130   assert(callee()-&gt;signature()-&gt;size() == 3, "hasNegatives has 3 parameters");
1131   // no receiver since it is static method
1132   Node* ba         = argument(0);
1133   Node* offset     = argument(1);
1134   Node* len        = argument(2);
1135 
1136   // Range checks
1137   generate_string_range_check(ba, offset, len, false);
1138   if (stopped()) {
1139     return true;
1140   }
1141   Node* ba_start = array_element_address(ba, offset, T_BYTE);
1142   Node* result = new HasNegativesNode(control(), memory(TypeAryPtr::BYTES), ba_start, len);
1143   set_result(_gvn.transform(result));
1144   return true;
1145 }
1146 
1147 bool LibraryCallKit::inline_objects_checkIndex() {
1148   Node* index = argument(0);
1149   Node* length = argument(1);
1150   if (too_many_traps(Deoptimization::Reason_intrinsic) || too_many_traps(Deoptimization::Reason_range_check)) {
1151     return false;
1152   }
1153 
1154   Node* len_pos_cmp = _gvn.transform(new CmpINode(length, intcon(0)));
1155   Node* len_pos_bol = _gvn.transform(new BoolNode(len_pos_cmp, BoolTest::ge));
1156 
1157   {
1158     BuildCutout unless(this, len_pos_bol, PROB_MAX);
1159     uncommon_trap(Deoptimization::Reason_intrinsic,
1160                   Deoptimization::Action_make_not_entrant);
1161   }
1162 
1163   if (stopped()) {
1164     return false;
1165   }
1166 
1167   Node* rc_cmp = _gvn.transform(new CmpUNode(index, length));
1168   BoolTest::mask btest = BoolTest::lt;
1169   Node* rc_bool = _gvn.transform(new BoolNode(rc_cmp, btest));
1170   RangeCheckNode* rc = new RangeCheckNode(control(), rc_bool, PROB_MAX, COUNT_UNKNOWN);
1171   _gvn.set_type(rc, rc-&gt;Value(&amp;_gvn));
1172   if (!rc_bool-&gt;is_Con()) {
1173     record_for_igvn(rc);
1174   }
1175   set_control(_gvn.transform(new IfTrueNode(rc)));
1176   {
1177     PreserveJVMState pjvms(this);
1178     set_control(_gvn.transform(new IfFalseNode(rc)));
1179     uncommon_trap(Deoptimization::Reason_range_check,
1180                   Deoptimization::Action_make_not_entrant);
1181   }
1182 
1183   if (stopped()) {
1184     return false;
1185   }
1186 
1187   Node* result = new CastIINode(index, TypeInt::make(0, _gvn.type(length)-&gt;is_int()-&gt;_hi, Type::WidenMax));
1188   result-&gt;set_req(0, control());
1189   result = _gvn.transform(result);
1190   set_result(result);
1191   replace_in_map(index, result);
1192   return true;
1193 }
1194 
1195 //------------------------------inline_string_indexOf------------------------
1196 bool LibraryCallKit::inline_string_indexOf(StrIntrinsicNode::ArgEnc ae) {
1197   if (!Matcher::match_rule_supported(Op_StrIndexOf)) {
1198     return false;
1199   }
1200   Node* src = argument(0);
1201   Node* tgt = argument(1);
1202 
1203   // Make the merge point
1204   RegionNode* result_rgn = new RegionNode(4);
1205   Node*       result_phi = new PhiNode(result_rgn, TypeInt::INT);
1206 
1207   // Get start addr and length of source string
1208   Node* src_start = array_element_address(src, intcon(0), T_BYTE);
1209   Node* src_count = load_array_length(src);
1210 
1211   // Get start addr and length of substring
1212   Node* tgt_start = array_element_address(tgt, intcon(0), T_BYTE);
1213   Node* tgt_count = load_array_length(tgt);
1214 
1215   if (ae == StrIntrinsicNode::UU || ae == StrIntrinsicNode::UL) {
1216     // Divide src size by 2 if String is UTF16 encoded
1217     src_count = _gvn.transform(new RShiftINode(src_count, intcon(1)));
1218   }
1219   if (ae == StrIntrinsicNode::UU) {
1220     // Divide substring size by 2 if String is UTF16 encoded
1221     tgt_count = _gvn.transform(new RShiftINode(tgt_count, intcon(1)));
1222   }
1223 
1224   Node* result = make_indexOf_node(src_start, src_count, tgt_start, tgt_count, result_rgn, result_phi, ae);
1225   if (result != NULL) {
1226     result_phi-&gt;init_req(3, result);
1227     result_rgn-&gt;init_req(3, control());
1228   }
1229   set_control(_gvn.transform(result_rgn));
1230   record_for_igvn(result_rgn);
1231   set_result(_gvn.transform(result_phi));
1232 
1233   return true;
1234 }
1235 
1236 //-----------------------------inline_string_indexOf-----------------------
1237 bool LibraryCallKit::inline_string_indexOfI(StrIntrinsicNode::ArgEnc ae) {
1238   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1239     return false;
1240   }
1241   if (!Matcher::match_rule_supported(Op_StrIndexOf)) {
1242     return false;
1243   }
1244   assert(callee()-&gt;signature()-&gt;size() == 5, "String.indexOf() has 5 arguments");
1245   Node* src         = argument(0); // byte[]
1246   Node* src_count   = argument(1); // char count
1247   Node* tgt         = argument(2); // byte[]
1248   Node* tgt_count   = argument(3); // char count
1249   Node* from_index  = argument(4); // char index
1250 
1251   // Multiply byte array index by 2 if String is UTF16 encoded
1252   Node* src_offset = (ae == StrIntrinsicNode::LL) ? from_index : _gvn.transform(new LShiftINode(from_index, intcon(1)));
1253   src_count = _gvn.transform(new SubINode(src_count, from_index));
1254   Node* src_start = array_element_address(src, src_offset, T_BYTE);
1255   Node* tgt_start = array_element_address(tgt, intcon(0), T_BYTE);
1256 
1257   // Range checks
1258   generate_string_range_check(src, src_offset, src_count, ae != StrIntrinsicNode::LL);
1259   generate_string_range_check(tgt, intcon(0), tgt_count, ae == StrIntrinsicNode::UU);
1260   if (stopped()) {
1261     return true;
1262   }
1263 
1264   RegionNode* region = new RegionNode(5);
1265   Node* phi = new PhiNode(region, TypeInt::INT);
1266 
1267   Node* result = make_indexOf_node(src_start, src_count, tgt_start, tgt_count, region, phi, ae);
1268   if (result != NULL) {
1269     // The result is index relative to from_index if substring was found, -1 otherwise.
1270     // Generate code which will fold into cmove.
1271     Node* cmp = _gvn.transform(new CmpINode(result, intcon(0)));
1272     Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::lt));
1273 
1274     Node* if_lt = generate_slow_guard(bol, NULL);
1275     if (if_lt != NULL) {
1276       // result == -1
1277       phi-&gt;init_req(3, result);
1278       region-&gt;init_req(3, if_lt);
1279     }
1280     if (!stopped()) {
1281       result = _gvn.transform(new AddINode(result, from_index));
1282       phi-&gt;init_req(4, result);
1283       region-&gt;init_req(4, control());
1284     }
1285   }
1286 
1287   set_control(_gvn.transform(region));
1288   record_for_igvn(region);
1289   set_result(_gvn.transform(phi));
1290 
1291   return true;
1292 }
1293 
1294 // Create StrIndexOfNode with fast path checks
1295 Node* LibraryCallKit::make_indexOf_node(Node* src_start, Node* src_count, Node* tgt_start, Node* tgt_count,
1296                                         RegionNode* region, Node* phi, StrIntrinsicNode::ArgEnc ae) {
1297   // Check for substr count &gt; string count
1298   Node* cmp = _gvn.transform(new CmpINode(tgt_count, src_count));
1299   Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::gt));
1300   Node* if_gt = generate_slow_guard(bol, NULL);
1301   if (if_gt != NULL) {
1302     phi-&gt;init_req(1, intcon(-1));
1303     region-&gt;init_req(1, if_gt);
1304   }
1305   if (!stopped()) {
1306     // Check for substr count == 0
1307     cmp = _gvn.transform(new CmpINode(tgt_count, intcon(0)));
1308     bol = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
1309     Node* if_zero = generate_slow_guard(bol, NULL);
1310     if (if_zero != NULL) {
1311       phi-&gt;init_req(2, intcon(0));
1312       region-&gt;init_req(2, if_zero);
1313     }
1314   }
1315   if (!stopped()) {
1316     return make_string_method_node(Op_StrIndexOf, src_start, src_count, tgt_start, tgt_count, ae);
1317   }
1318   return NULL;
1319 }
1320 
1321 //-----------------------------inline_string_indexOfChar-----------------------
1322 bool LibraryCallKit::inline_string_indexOfChar() {
1323   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1324     return false;
1325   }
1326   if (!Matcher::match_rule_supported(Op_StrIndexOfChar)) {
1327     return false;
1328   }
1329   assert(callee()-&gt;signature()-&gt;size() == 4, "String.indexOfChar() has 4 arguments");
1330   Node* src         = argument(0); // byte[]
1331   Node* tgt         = argument(1); // tgt is int ch
1332   Node* from_index  = argument(2);
1333   Node* max         = argument(3);
1334 
1335   Node* src_offset = _gvn.transform(new LShiftINode(from_index, intcon(1)));
1336   Node* src_start = array_element_address(src, src_offset, T_BYTE);
1337   Node* src_count = _gvn.transform(new SubINode(max, from_index));
1338 
1339   // Range checks
1340   generate_string_range_check(src, src_offset, src_count, true);
1341   if (stopped()) {
1342     return true;
1343   }
1344 
1345   RegionNode* region = new RegionNode(3);
1346   Node* phi = new PhiNode(region, TypeInt::INT);
1347 
1348   Node* result = new StrIndexOfCharNode(control(), memory(TypeAryPtr::BYTES), src_start, src_count, tgt, StrIntrinsicNode::none);
1349   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1350   _gvn.transform(result);
1351 
1352   Node* cmp = _gvn.transform(new CmpINode(result, intcon(0)));
1353   Node* bol = _gvn.transform(new BoolNode(cmp, BoolTest::lt));
1354 
1355   Node* if_lt = generate_slow_guard(bol, NULL);
1356   if (if_lt != NULL) {
1357     // result == -1
1358     phi-&gt;init_req(2, result);
1359     region-&gt;init_req(2, if_lt);
1360   }
1361   if (!stopped()) {
1362     result = _gvn.transform(new AddINode(result, from_index));
1363     phi-&gt;init_req(1, result);
1364     region-&gt;init_req(1, control());
1365   }
1366   set_control(_gvn.transform(region));
1367   record_for_igvn(region);
1368   set_result(_gvn.transform(phi));
1369 
1370   return true;
1371 }
1372 //---------------------------inline_string_copy---------------------
1373 // compressIt == true --&gt; generate a compressed copy operation (compress char[]/byte[] to byte[])
1374 //   int StringUTF16.compress(char[] src, int srcOff, byte[] dst, int dstOff, int len)
1375 //   int StringUTF16.compress(byte[] src, int srcOff, byte[] dst, int dstOff, int len)
1376 // compressIt == false --&gt; generate an inflated copy operation (inflate byte[] to char[]/byte[])
1377 //   void StringLatin1.inflate(byte[] src, int srcOff, char[] dst, int dstOff, int len)
1378 //   void StringLatin1.inflate(byte[] src, int srcOff, byte[] dst, int dstOff, int len)
1379 bool LibraryCallKit::inline_string_copy(bool compress) {
1380   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1381     return false;
1382   }
1383   int nargs = 5;  // 2 oops, 3 ints
1384   assert(callee()-&gt;signature()-&gt;size() == nargs, "string copy has 5 arguments");
1385 
1386   Node* src         = argument(0);
1387   Node* src_offset  = argument(1);
1388   Node* dst         = argument(2);
1389   Node* dst_offset  = argument(3);
1390   Node* length      = argument(4);
1391 
1392   // Check for allocation before we add nodes that would confuse
1393   // tightly_coupled_allocation()
1394   AllocateArrayNode* alloc = tightly_coupled_allocation(dst, NULL);
1395 
1396   // Figure out the size and type of the elements we will be copying.
1397   const Type* src_type = src-&gt;Value(&amp;_gvn);
1398   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
1399   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
1400   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
1401   assert((compress &amp;&amp; dst_elem == T_BYTE &amp;&amp; (src_elem == T_BYTE || src_elem == T_CHAR)) ||
1402          (!compress &amp;&amp; src_elem == T_BYTE &amp;&amp; (dst_elem == T_BYTE || dst_elem == T_CHAR)),
1403          "Unsupported array types for inline_string_copy");
1404 
1405   // Range checks
1406   generate_string_range_check(src, src_offset, length, compress &amp;&amp; src_elem == T_BYTE);
1407   generate_string_range_check(dst, dst_offset, length, !compress &amp;&amp; dst_elem == T_BYTE);
1408   if (stopped()) {
1409     return true;
1410   }
1411 
1412   // Convert char[] offsets to byte[] offsets
1413   if (compress &amp;&amp; src_elem == T_BYTE) {
1414     src_offset = _gvn.transform(new LShiftINode(src_offset, intcon(1)));
1415   } else if (!compress &amp;&amp; dst_elem == T_BYTE) {
1416     dst_offset = _gvn.transform(new LShiftINode(dst_offset, intcon(1)));
1417   }
1418 
1419   Node* src_start = array_element_address(src, src_offset, src_elem);
1420   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
1421   // 'src_start' points to src array + scaled offset
1422   // 'dst_start' points to dst array + scaled offset
1423   Node* count = NULL;
1424   if (compress) {
1425     count = compress_string(src_start, TypeAryPtr::get_array_body_type(src_elem), dst_start, length);
1426   } else {
1427     inflate_string(src_start, dst_start, TypeAryPtr::get_array_body_type(dst_elem), length);
1428   }
1429 
1430   if (alloc != NULL) {
1431     if (alloc-&gt;maybe_set_complete(&amp;_gvn)) {
1432       // "You break it, you buy it."
1433       InitializeNode* init = alloc-&gt;initialization();
1434       assert(init-&gt;is_complete(), "we just did this");
1435       init-&gt;set_complete_with_arraycopy();
1436       assert(dst-&gt;is_CheckCastPP(), "sanity");
1437       assert(dst-&gt;in(0)-&gt;in(0) == init, "dest pinned");
1438     }
1439     // Do not let stores that initialize this object be reordered with
1440     // a subsequent store that would make this object accessible by
1441     // other threads.
1442     // Record what AllocateNode this StoreStore protects so that
1443     // escape analysis can go from the MemBarStoreStoreNode to the
1444     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
1445     // based on the escape status of the AllocateNode.
1446     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
1447   }
1448   if (compress) {
1449     set_result(_gvn.transform(count));
1450   }
1451   return true;
1452 }
1453 
1454 #ifdef _LP64
1455 #define XTOP ,top() /*additional argument*/
1456 #else  //_LP64
1457 #define XTOP        /*no additional argument*/
1458 #endif //_LP64
1459 
1460 //------------------------inline_string_toBytesU--------------------------
1461 // public static byte[] StringUTF16.toBytes(char[] value, int off, int len)
1462 bool LibraryCallKit::inline_string_toBytesU() {
1463   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1464     return false;
1465   }
1466   // Get the arguments.
1467   Node* value     = argument(0);
1468   Node* offset    = argument(1);
1469   Node* length    = argument(2);
1470 
1471   Node* newcopy = NULL;
1472 
1473   // Set the original stack and the reexecute bit for the interpreter to reexecute
1474   // the bytecode that invokes StringUTF16.toBytes() if deoptimization happens.
1475   { PreserveReexecuteState preexecs(this);
1476     jvms()-&gt;set_should_reexecute(true);
1477 
1478     // Check if a null path was taken unconditionally.
1479     value = null_check(value);
1480 
1481     RegionNode* bailout = new RegionNode(1);
1482     record_for_igvn(bailout);
1483 
1484     // Range checks
1485     generate_negative_guard(offset, bailout);
1486     generate_negative_guard(length, bailout);
1487     generate_limit_guard(offset, length, load_array_length(value), bailout);
1488     // Make sure that resulting byte[] length does not overflow Integer.MAX_VALUE
1489     generate_limit_guard(length, intcon(0), intcon(max_jint/2), bailout);
1490 
1491     if (bailout-&gt;req() &gt; 1) {
1492       PreserveJVMState pjvms(this);
1493       set_control(_gvn.transform(bailout));
1494       uncommon_trap(Deoptimization::Reason_intrinsic,
1495                     Deoptimization::Action_maybe_recompile);
1496     }
1497     if (stopped()) {
1498       return true;
1499     }
1500 
1501     Node* size = _gvn.transform(new LShiftINode(length, intcon(1)));
1502     Node* klass_node = makecon(TypeKlassPtr::make(ciTypeArrayKlass::make(T_BYTE)));
1503     newcopy = new_array(klass_node, size, 0);  // no arguments to push
1504     AllocateArrayNode* alloc = tightly_coupled_allocation(newcopy, NULL);
1505 
1506     // Calculate starting addresses.
1507     Node* src_start = array_element_address(value, offset, T_CHAR);
1508     Node* dst_start = basic_plus_adr(newcopy, arrayOopDesc::base_offset_in_bytes(T_BYTE));
1509 
1510     // Check if src array address is aligned to HeapWordSize (dst is always aligned)
1511     const TypeInt* toffset = gvn().type(offset)-&gt;is_int();
1512     bool aligned = toffset-&gt;is_con() &amp;&amp; ((toffset-&gt;get_con() * type2aelembytes(T_CHAR)) % HeapWordSize == 0);
1513 
1514     // Figure out which arraycopy runtime method to call (disjoint, uninitialized).
1515     const char* copyfunc_name = "arraycopy";
1516     address     copyfunc_addr = StubRoutines::select_arraycopy_function(T_CHAR, aligned, true, copyfunc_name, true);
1517     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
1518                       OptoRuntime::fast_arraycopy_Type(),
1519                       copyfunc_addr, copyfunc_name, TypeRawPtr::BOTTOM,
1520                       src_start, dst_start, ConvI2X(length) XTOP);
1521     // Do not let reads from the cloned object float above the arraycopy.
1522     if (alloc != NULL) {
1523       if (alloc-&gt;maybe_set_complete(&amp;_gvn)) {
1524         // "You break it, you buy it."
1525         InitializeNode* init = alloc-&gt;initialization();
1526         assert(init-&gt;is_complete(), "we just did this");
1527         init-&gt;set_complete_with_arraycopy();
1528         assert(newcopy-&gt;is_CheckCastPP(), "sanity");
1529         assert(newcopy-&gt;in(0)-&gt;in(0) == init, "dest pinned");
1530       }
1531       // Do not let stores that initialize this object be reordered with
1532       // a subsequent store that would make this object accessible by
1533       // other threads.
1534       // Record what AllocateNode this StoreStore protects so that
1535       // escape analysis can go from the MemBarStoreStoreNode to the
1536       // AllocateNode and eliminate the MemBarStoreStoreNode if possible
1537       // based on the escape status of the AllocateNode.
1538       insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
1539     } else {
1540       insert_mem_bar(Op_MemBarCPUOrder);
1541     }
1542   } // original reexecute is set back here
1543 
1544   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1545   if (!stopped()) {
1546     set_result(newcopy);
1547   }
1548   return true;
1549 }
1550 
1551 //------------------------inline_string_getCharsU--------------------------
1552 // public void StringUTF16.getChars(byte[] src, int srcBegin, int srcEnd, char dst[], int dstBegin)
1553 bool LibraryCallKit::inline_string_getCharsU() {
1554   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
1555     return false;
1556   }
1557 
1558   // Get the arguments.
1559   Node* src       = argument(0);
1560   Node* src_begin = argument(1);
1561   Node* src_end   = argument(2); // exclusive offset (i &lt; src_end)
1562   Node* dst       = argument(3);
1563   Node* dst_begin = argument(4);
1564 
1565   // Check for allocation before we add nodes that would confuse
1566   // tightly_coupled_allocation()
1567   AllocateArrayNode* alloc = tightly_coupled_allocation(dst, NULL);
1568 
1569   // Check if a null path was taken unconditionally.
1570   src = null_check(src);
1571   dst = null_check(dst);
1572   if (stopped()) {
1573     return true;
1574   }
1575 
1576   // Get length and convert char[] offset to byte[] offset
1577   Node* length = _gvn.transform(new SubINode(src_end, src_begin));
1578   src_begin = _gvn.transform(new LShiftINode(src_begin, intcon(1)));
1579 
1580   // Range checks
1581   generate_string_range_check(src, src_begin, length, true);
1582   generate_string_range_check(dst, dst_begin, length, false);
1583   if (stopped()) {
1584     return true;
1585   }
1586 
1587   if (!stopped()) {
1588     // Calculate starting addresses.
1589     Node* src_start = array_element_address(src, src_begin, T_BYTE);
1590     Node* dst_start = array_element_address(dst, dst_begin, T_CHAR);
1591 
1592     // Check if array addresses are aligned to HeapWordSize
1593     const TypeInt* tsrc = gvn().type(src_begin)-&gt;is_int();
1594     const TypeInt* tdst = gvn().type(dst_begin)-&gt;is_int();
1595     bool aligned = tsrc-&gt;is_con() &amp;&amp; ((tsrc-&gt;get_con() * type2aelembytes(T_BYTE)) % HeapWordSize == 0) &amp;&amp;
1596                    tdst-&gt;is_con() &amp;&amp; ((tdst-&gt;get_con() * type2aelembytes(T_CHAR)) % HeapWordSize == 0);
1597 
1598     // Figure out which arraycopy runtime method to call (disjoint, uninitialized).
1599     const char* copyfunc_name = "arraycopy";
1600     address     copyfunc_addr = StubRoutines::select_arraycopy_function(T_CHAR, aligned, true, copyfunc_name, true);
1601     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
1602                       OptoRuntime::fast_arraycopy_Type(),
1603                       copyfunc_addr, copyfunc_name, TypeRawPtr::BOTTOM,
1604                       src_start, dst_start, ConvI2X(length) XTOP);
1605     // Do not let reads from the cloned object float above the arraycopy.
1606     if (alloc != NULL) {
1607       if (alloc-&gt;maybe_set_complete(&amp;_gvn)) {
1608         // "You break it, you buy it."
1609         InitializeNode* init = alloc-&gt;initialization();
1610         assert(init-&gt;is_complete(), "we just did this");
1611         init-&gt;set_complete_with_arraycopy();
1612         assert(dst-&gt;is_CheckCastPP(), "sanity");
1613         assert(dst-&gt;in(0)-&gt;in(0) == init, "dest pinned");
1614       }
1615       // Do not let stores that initialize this object be reordered with
1616       // a subsequent store that would make this object accessible by
1617       // other threads.
1618       // Record what AllocateNode this StoreStore protects so that
1619       // escape analysis can go from the MemBarStoreStoreNode to the
1620       // AllocateNode and eliminate the MemBarStoreStoreNode if possible
1621       // based on the escape status of the AllocateNode.
1622       insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
1623     } else {
1624       insert_mem_bar(Op_MemBarCPUOrder);
1625     }
1626   }
1627 
1628   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1629   return true;
1630 }
1631 
1632 //----------------------inline_string_char_access----------------------------
1633 // Store/Load char to/from byte[] array.
1634 // static void StringUTF16.putChar(byte[] val, int index, int c)
1635 // static char StringUTF16.getChar(byte[] val, int index)
1636 bool LibraryCallKit::inline_string_char_access(bool is_store) {
1637   Node* value  = argument(0);
1638   Node* index  = argument(1);
1639   Node* ch = is_store ? argument(2) : NULL;
1640 
1641   // This intrinsic accesses byte[] array as char[] array. Computing the offsets
1642   // correctly requires matched array shapes.
1643   assert (arrayOopDesc::base_offset_in_bytes(T_CHAR) == arrayOopDesc::base_offset_in_bytes(T_BYTE),
1644           "sanity: byte[] and char[] bases agree");
1645   assert (type2aelembytes(T_CHAR) == type2aelembytes(T_BYTE)*2,
1646           "sanity: byte[] and char[] scales agree");
1647 
1648   // Bail when getChar over constants is requested: constant folding would
1649   // reject folding mismatched char access over byte[]. A normal inlining for getChar
1650   // Java method would constant fold nicely instead.
1651   if (!is_store &amp;&amp; value-&gt;is_Con() &amp;&amp; index-&gt;is_Con()) {
1652     return false;
1653   }
1654 
1655   Node* adr = array_element_address(value, index, T_CHAR);
1656   if (is_store) {
1657     (void) store_to_memory(control(), adr, ch, T_CHAR, TypeAryPtr::BYTES, MemNode::unordered,
1658                            false, false, true /* mismatched */);
1659   } else {
1660     ch = make_load(control(), adr, TypeInt::CHAR, T_CHAR, TypeAryPtr::BYTES, MemNode::unordered,
1661                    LoadNode::DependsOnlyOnTest, false, false, true /* mismatched */);
1662     set_result(ch);
1663   }
1664   return true;
1665 }
1666 
1667 //--------------------------round_double_node--------------------------------
1668 // Round a double node if necessary.
1669 Node* LibraryCallKit::round_double_node(Node* n) {
1670   if (Matcher::strict_fp_requires_explicit_rounding &amp;&amp; UseSSE &lt;= 1)
1671     n = _gvn.transform(new RoundDoubleNode(0, n));
1672   return n;
1673 }
1674 
1675 //------------------------------inline_math-----------------------------------
1676 // public static double Math.abs(double)
1677 // public static double Math.sqrt(double)
1678 // public static double Math.log(double)
1679 // public static double Math.log10(double)
1680 bool LibraryCallKit::inline_math(vmIntrinsics::ID id) {
1681   Node* arg = round_double_node(argument(0));
1682   Node* n = NULL;
1683   switch (id) {
1684   case vmIntrinsics::_dabs:   n = new AbsDNode(                arg);  break;
1685   case vmIntrinsics::_dsqrt:  n = new SqrtDNode(C, control(),  arg);  break;
1686   case vmIntrinsics::_dlog10: n = new Log10DNode(C, control(), arg);  break;
1687   default:  fatal_unexpected_iid(id);  break;
1688   }
1689   set_result(_gvn.transform(n));
1690   return true;
1691 }
1692 
1693 //------------------------------inline_trig----------------------------------
1694 // Inline sin/cos/tan instructions, if possible.  If rounding is required, do
1695 // argument reduction which will turn into a fast/slow diamond.
1696 bool LibraryCallKit::inline_trig(vmIntrinsics::ID id) {
1697   Node* arg = round_double_node(argument(0));
1698   Node* n = NULL;
1699 
1700   switch (id) {
1701   case vmIntrinsics::_dtan:  n = new TanDNode(C, control(), arg);  break;
1702   default:  fatal_unexpected_iid(id);  break;
1703   }
1704   n = _gvn.transform(n);
1705 
1706   // Rounding required?  Check for argument reduction!
1707   if (Matcher::strict_fp_requires_explicit_rounding) {
1708     static const double     pi_4 =  0.7853981633974483;
1709     static const double neg_pi_4 = -0.7853981633974483;
1710     // pi/2 in 80-bit extended precision
1711     // static const unsigned char pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0x3f,0x00,0x00,0x00,0x00,0x00,0x00};
1712     // -pi/2 in 80-bit extended precision
1713     // static const unsigned char neg_pi_2_bits_x[] = {0x35,0xc2,0x68,0x21,0xa2,0xda,0x0f,0xc9,0xff,0xbf,0x00,0x00,0x00,0x00,0x00,0x00};
1714     // Cutoff value for using this argument reduction technique
1715     //static const double    pi_2_minus_epsilon =  1.564660403643354;
1716     //static const double neg_pi_2_plus_epsilon = -1.564660403643354;
1717 
1718     // Pseudocode for sin:
1719     // if (x &lt;= Math.PI / 4.0) {
1720     //   if (x &gt;= -Math.PI / 4.0) return  fsin(x);
1721     //   if (x &gt;= -Math.PI / 2.0) return -fcos(x + Math.PI / 2.0);
1722     // } else {
1723     //   if (x &lt;=  Math.PI / 2.0) return  fcos(x - Math.PI / 2.0);
1724     // }
1725     // return StrictMath.sin(x);
1726 
1727     // Pseudocode for cos:
1728     // if (x &lt;= Math.PI / 4.0) {
1729     //   if (x &gt;= -Math.PI / 4.0) return  fcos(x);
1730     //   if (x &gt;= -Math.PI / 2.0) return  fsin(x + Math.PI / 2.0);
1731     // } else {
1732     //   if (x &lt;=  Math.PI / 2.0) return -fsin(x - Math.PI / 2.0);
1733     // }
1734     // return StrictMath.cos(x);
1735 
1736     // Actually, sticking in an 80-bit Intel value into C2 will be tough; it
1737     // requires a special machine instruction to load it.  Instead we'll try
1738     // the 'easy' case.  If we really need the extra range +/- PI/2 we'll
1739     // probably do the math inside the SIN encoding.
1740 
1741     // Make the merge point
1742     RegionNode* r = new RegionNode(3);
1743     Node* phi = new PhiNode(r, Type::DOUBLE);
1744 
1745     // Flatten arg so we need only 1 test
1746     Node *abs = _gvn.transform(new AbsDNode(arg));
1747     // Node for PI/4 constant
1748     Node *pi4 = makecon(TypeD::make(pi_4));
1749     // Check PI/4 : abs(arg)
1750     Node *cmp = _gvn.transform(new CmpDNode(pi4,abs));
1751     // Check: If PI/4 &lt; abs(arg) then go slow
1752     Node *bol = _gvn.transform(new BoolNode( cmp, BoolTest::lt ));
1753     // Branch either way
1754     IfNode *iff = create_and_xform_if(control(),bol, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
1755     set_control(opt_iff(r,iff));
1756 
1757     // Set fast path result
1758     phi-&gt;init_req(2, n);
1759 
1760     // Slow path - non-blocking leaf call
1761     Node* call = NULL;
1762     switch (id) {
1763     case vmIntrinsics::_dtan:
1764       call = make_runtime_call(RC_LEAF, OptoRuntime::Math_D_D_Type(),
1765                                CAST_FROM_FN_PTR(address, SharedRuntime::dtan),
1766                                "Tan", NULL, arg, top());
1767       break;
1768     }
1769     assert(control()-&gt;in(0) == call, "");
1770     Node* slow_result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
1771     r-&gt;init_req(1, control());
1772     phi-&gt;init_req(1, slow_result);
1773 
1774     // Post-merge
1775     set_control(_gvn.transform(r));
1776     record_for_igvn(r);
1777     n = _gvn.transform(phi);
1778 
1779     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
1780   }
1781   set_result(n);
1782   return true;
1783 }
1784 
1785 //------------------------------runtime_math-----------------------------
1786 bool LibraryCallKit::runtime_math(const TypeFunc* call_type, address funcAddr, const char* funcName) {
1787   assert(call_type == OptoRuntime::Math_DD_D_Type() || call_type == OptoRuntime::Math_D_D_Type(),
1788          "must be (DD)D or (D)D type");
1789 
1790   // Inputs
1791   Node* a = round_double_node(argument(0));
1792   Node* b = (call_type == OptoRuntime::Math_DD_D_Type()) ? round_double_node(argument(2)) : NULL;
1793 
1794   const TypePtr* no_memory_effects = NULL;
1795   Node* trig = make_runtime_call(RC_LEAF, call_type, funcAddr, funcName,
1796                                  no_memory_effects,
1797                                  a, top(), b, b ? top() : NULL);
1798   Node* value = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+0));
1799 #ifdef ASSERT
1800   Node* value_top = _gvn.transform(new ProjNode(trig, TypeFunc::Parms+1));
1801   assert(value_top == top(), "second value must be top");
1802 #endif
1803 
1804   set_result(value);
1805   return true;
1806 }
1807 
1808 //------------------------------inline_math_native-----------------------------
1809 bool LibraryCallKit::inline_math_native(vmIntrinsics::ID id) {
1810 #define FN_PTR(f) CAST_FROM_FN_PTR(address, f)
1811   switch (id) {
1812     // These intrinsics are not properly supported on all hardware
1813   case vmIntrinsics::_dsin:
1814     return StubRoutines::dsin() != NULL ?
1815       runtime_math(OptoRuntime::Math_D_D_Type(), StubRoutines::dsin(), "dsin") :
1816       runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dsin),   "SIN");
1817   case vmIntrinsics::_dcos:
1818     return StubRoutines::dcos() != NULL ?
1819       runtime_math(OptoRuntime::Math_D_D_Type(), StubRoutines::dcos(), "dcos") :
1820       runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dcos),   "COS");
1821   case vmIntrinsics::_dtan:   return Matcher::has_match_rule(Op_TanD)   ? inline_trig(id) :
1822     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dtan),   "TAN");
1823 
1824   case vmIntrinsics::_dlog:
1825     return StubRoutines::dlog() != NULL ?
1826       runtime_math(OptoRuntime::Math_D_D_Type(), StubRoutines::dlog(), "dlog") :
1827       runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog),   "LOG");
1828   case vmIntrinsics::_dlog10: return Matcher::has_match_rule(Op_Log10D) ? inline_math(id) :
1829     runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dlog10), "LOG10");
1830 
1831     // These intrinsics are supported on all hardware
1832   case vmIntrinsics::_dsqrt:  return Matcher::match_rule_supported(Op_SqrtD) ? inline_math(id) : false;
1833   case vmIntrinsics::_dabs:   return Matcher::has_match_rule(Op_AbsD)   ? inline_math(id) : false;
1834 
1835   case vmIntrinsics::_dexp:
1836     return StubRoutines::dexp() != NULL ?
1837       runtime_math(OptoRuntime::Math_D_D_Type(), StubRoutines::dexp(),  "dexp") :
1838       runtime_math(OptoRuntime::Math_D_D_Type(), FN_PTR(SharedRuntime::dexp),  "EXP");
1839   case vmIntrinsics::_dpow:
1840     return StubRoutines::dpow() != NULL ?
1841       runtime_math(OptoRuntime::Math_DD_D_Type(), StubRoutines::dpow(), "dpow") :
1842       runtime_math(OptoRuntime::Math_DD_D_Type(), FN_PTR(SharedRuntime::dpow),  "POW");
1843 #undef FN_PTR
1844 
1845    // These intrinsics are not yet correctly implemented
1846   case vmIntrinsics::_datan2:
1847     return false;
1848 
1849   default:
1850     fatal_unexpected_iid(id);
1851     return false;
1852   }
1853 }
1854 
1855 static bool is_simple_name(Node* n) {
1856   return (n-&gt;req() == 1         // constant
1857           || (n-&gt;is_Type() &amp;&amp; n-&gt;as_Type()-&gt;type()-&gt;singleton())
1858           || n-&gt;is_Proj()       // parameter or return value
1859           || n-&gt;is_Phi()        // local of some sort
1860           );
1861 }
1862 
1863 //----------------------------inline_notify-----------------------------------*
1864 bool LibraryCallKit::inline_notify(vmIntrinsics::ID id) {
1865   const TypeFunc* ftype = OptoRuntime::monitor_notify_Type();
1866   address func;
1867   if (id == vmIntrinsics::_notify) {
1868     func = OptoRuntime::monitor_notify_Java();
1869   } else {
1870     func = OptoRuntime::monitor_notifyAll_Java();
1871   }
1872   Node* call = make_runtime_call(RC_NO_LEAF, ftype, func, NULL, TypeRawPtr::BOTTOM, argument(0));
1873   make_slow_call_ex(call, env()-&gt;Throwable_klass(), false);
1874   return true;
1875 }
1876 
1877 
1878 //----------------------------inline_min_max-----------------------------------
1879 bool LibraryCallKit::inline_min_max(vmIntrinsics::ID id) {
1880   set_result(generate_min_max(id, argument(0), argument(1)));
1881   return true;
1882 }
1883 
1884 void LibraryCallKit::inline_math_mathExact(Node* math, Node *test) {
1885   Node* bol = _gvn.transform( new BoolNode(test, BoolTest::overflow) );
1886   IfNode* check = create_and_map_if(control(), bol, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
1887   Node* fast_path = _gvn.transform( new IfFalseNode(check));
1888   Node* slow_path = _gvn.transform( new IfTrueNode(check) );
1889 
1890   {
1891     PreserveJVMState pjvms(this);
1892     PreserveReexecuteState preexecs(this);
1893     jvms()-&gt;set_should_reexecute(true);
1894 
1895     set_control(slow_path);
1896     set_i_o(i_o());
1897 
1898     uncommon_trap(Deoptimization::Reason_intrinsic,
1899                   Deoptimization::Action_none);
1900   }
1901 
1902   set_control(fast_path);
1903   set_result(math);
1904 }
1905 
1906 template &lt;typename OverflowOp&gt;
1907 bool LibraryCallKit::inline_math_overflow(Node* arg1, Node* arg2) {
1908   typedef typename OverflowOp::MathOp MathOp;
1909 
1910   MathOp* mathOp = new MathOp(arg1, arg2);
1911   Node* operation = _gvn.transform( mathOp );
1912   Node* ofcheck = _gvn.transform( new OverflowOp(arg1, arg2) );
1913   inline_math_mathExact(operation, ofcheck);
1914   return true;
1915 }
1916 
1917 bool LibraryCallKit::inline_math_addExactI(bool is_increment) {
1918   return inline_math_overflow&lt;OverflowAddINode&gt;(argument(0), is_increment ? intcon(1) : argument(1));
1919 }
1920 
1921 bool LibraryCallKit::inline_math_addExactL(bool is_increment) {
1922   return inline_math_overflow&lt;OverflowAddLNode&gt;(argument(0), is_increment ? longcon(1) : argument(2));
1923 }
1924 
1925 bool LibraryCallKit::inline_math_subtractExactI(bool is_decrement) {
1926   return inline_math_overflow&lt;OverflowSubINode&gt;(argument(0), is_decrement ? intcon(1) : argument(1));
1927 }
1928 
1929 bool LibraryCallKit::inline_math_subtractExactL(bool is_decrement) {
1930   return inline_math_overflow&lt;OverflowSubLNode&gt;(argument(0), is_decrement ? longcon(1) : argument(2));
1931 }
1932 
1933 bool LibraryCallKit::inline_math_negateExactI() {
1934   return inline_math_overflow&lt;OverflowSubINode&gt;(intcon(0), argument(0));
1935 }
1936 
1937 bool LibraryCallKit::inline_math_negateExactL() {
1938   return inline_math_overflow&lt;OverflowSubLNode&gt;(longcon(0), argument(0));
1939 }
1940 
1941 bool LibraryCallKit::inline_math_multiplyExactI() {
1942   return inline_math_overflow&lt;OverflowMulINode&gt;(argument(0), argument(1));
1943 }
1944 
1945 bool LibraryCallKit::inline_math_multiplyExactL() {
1946   return inline_math_overflow&lt;OverflowMulLNode&gt;(argument(0), argument(2));
1947 }
1948 
1949 Node*
1950 LibraryCallKit::generate_min_max(vmIntrinsics::ID id, Node* x0, Node* y0) {
1951   // These are the candidate return value:
1952   Node* xvalue = x0;
1953   Node* yvalue = y0;
1954 
1955   if (xvalue == yvalue) {
1956     return xvalue;
1957   }
1958 
1959   bool want_max = (id == vmIntrinsics::_max);
1960 
1961   const TypeInt* txvalue = _gvn.type(xvalue)-&gt;isa_int();
1962   const TypeInt* tyvalue = _gvn.type(yvalue)-&gt;isa_int();
1963   if (txvalue == NULL || tyvalue == NULL)  return top();
1964   // This is not really necessary, but it is consistent with a
1965   // hypothetical MaxINode::Value method:
1966   int widen = MAX2(txvalue-&gt;_widen, tyvalue-&gt;_widen);
1967 
1968   // %%% This folding logic should (ideally) be in a different place.
1969   // Some should be inside IfNode, and there to be a more reliable
1970   // transformation of ?: style patterns into cmoves.  We also want
1971   // more powerful optimizations around cmove and min/max.
1972 
1973   // Try to find a dominating comparison of these guys.
1974   // It can simplify the index computation for Arrays.copyOf
1975   // and similar uses of System.arraycopy.
1976   // First, compute the normalized version of CmpI(x, y).
1977   int   cmp_op = Op_CmpI;
1978   Node* xkey = xvalue;
1979   Node* ykey = yvalue;
1980   Node* ideal_cmpxy = _gvn.transform(new CmpINode(xkey, ykey));
1981   if (ideal_cmpxy-&gt;is_Cmp()) {
1982     // E.g., if we have CmpI(length - offset, count),
1983     // it might idealize to CmpI(length, count + offset)
1984     cmp_op = ideal_cmpxy-&gt;Opcode();
1985     xkey = ideal_cmpxy-&gt;in(1);
1986     ykey = ideal_cmpxy-&gt;in(2);
1987   }
1988 
1989   // Start by locating any relevant comparisons.
1990   Node* start_from = (xkey-&gt;outcnt() &lt; ykey-&gt;outcnt()) ? xkey : ykey;
1991   Node* cmpxy = NULL;
1992   Node* cmpyx = NULL;
1993   for (DUIterator_Fast kmax, k = start_from-&gt;fast_outs(kmax); k &lt; kmax; k++) {
1994     Node* cmp = start_from-&gt;fast_out(k);
1995     if (cmp-&gt;outcnt() &gt; 0 &amp;&amp;            // must have prior uses
1996         cmp-&gt;in(0) == NULL &amp;&amp;           // must be context-independent
1997         cmp-&gt;Opcode() == cmp_op) {      // right kind of compare
1998       if (cmp-&gt;in(1) == xkey &amp;&amp; cmp-&gt;in(2) == ykey)  cmpxy = cmp;
1999       if (cmp-&gt;in(1) == ykey &amp;&amp; cmp-&gt;in(2) == xkey)  cmpyx = cmp;
2000     }
2001   }
2002 
2003   const int NCMPS = 2;
2004   Node* cmps[NCMPS] = { cmpxy, cmpyx };
2005   int cmpn;
2006   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2007     if (cmps[cmpn] != NULL)  break;     // find a result
2008   }
2009   if (cmpn &lt; NCMPS) {
2010     // Look for a dominating test that tells us the min and max.
2011     int depth = 0;                // Limit search depth for speed
2012     Node* dom = control();
2013     for (; dom != NULL; dom = IfNode::up_one_dom(dom, true)) {
2014       if (++depth &gt;= 100)  break;
2015       Node* ifproj = dom;
2016       if (!ifproj-&gt;is_Proj())  continue;
2017       Node* iff = ifproj-&gt;in(0);
2018       if (!iff-&gt;is_If())  continue;
2019       Node* bol = iff-&gt;in(1);
2020       if (!bol-&gt;is_Bool())  continue;
2021       Node* cmp = bol-&gt;in(1);
2022       if (cmp == NULL)  continue;
2023       for (cmpn = 0; cmpn &lt; NCMPS; cmpn++)
2024         if (cmps[cmpn] == cmp)  break;
2025       if (cmpn == NCMPS)  continue;
2026       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2027       if (ifproj-&gt;is_IfFalse())  btest = BoolTest(btest).negate();
2028       if (cmp-&gt;in(1) == ykey)    btest = BoolTest(btest).commute();
2029       // At this point, we know that 'x btest y' is true.
2030       switch (btest) {
2031       case BoolTest::eq:
2032         // They are proven equal, so we can collapse the min/max.
2033         // Either value is the answer.  Choose the simpler.
2034         if (is_simple_name(yvalue) &amp;&amp; !is_simple_name(xvalue))
2035           return yvalue;
2036         return xvalue;
2037       case BoolTest::lt:          // x &lt; y
2038       case BoolTest::le:          // x &lt;= y
2039         return (want_max ? yvalue : xvalue);
2040       case BoolTest::gt:          // x &gt; y
2041       case BoolTest::ge:          // x &gt;= y
2042         return (want_max ? xvalue : yvalue);
2043       }
2044     }
2045   }
2046 
2047   // We failed to find a dominating test.
2048   // Let's pick a test that might GVN with prior tests.
2049   Node*          best_bol   = NULL;
2050   BoolTest::mask best_btest = BoolTest::illegal;
2051   for (cmpn = 0; cmpn &lt; NCMPS; cmpn++) {
2052     Node* cmp = cmps[cmpn];
2053     if (cmp == NULL)  continue;
2054     for (DUIterator_Fast jmax, j = cmp-&gt;fast_outs(jmax); j &lt; jmax; j++) {
2055       Node* bol = cmp-&gt;fast_out(j);
2056       if (!bol-&gt;is_Bool())  continue;
2057       BoolTest::mask btest = bol-&gt;as_Bool()-&gt;_test._test;
2058       if (btest == BoolTest::eq || btest == BoolTest::ne)  continue;
2059       if (cmp-&gt;in(1) == ykey)   btest = BoolTest(btest).commute();
2060       if (bol-&gt;outcnt() &gt; (best_bol == NULL ? 0 : best_bol-&gt;outcnt())) {
2061         best_bol   = bol-&gt;as_Bool();
2062         best_btest = btest;
2063       }
2064     }
2065   }
2066 
2067   Node* answer_if_true  = NULL;
2068   Node* answer_if_false = NULL;
2069   switch (best_btest) {
2070   default:
2071     if (cmpxy == NULL)
2072       cmpxy = ideal_cmpxy;
2073     best_bol = _gvn.transform(new BoolNode(cmpxy, BoolTest::lt));
2074     // and fall through:
2075   case BoolTest::lt:          // x &lt; y
2076   case BoolTest::le:          // x &lt;= y
2077     answer_if_true  = (want_max ? yvalue : xvalue);
2078     answer_if_false = (want_max ? xvalue : yvalue);
2079     break;
2080   case BoolTest::gt:          // x &gt; y
2081   case BoolTest::ge:          // x &gt;= y
2082     answer_if_true  = (want_max ? xvalue : yvalue);
2083     answer_if_false = (want_max ? yvalue : xvalue);
2084     break;
2085   }
2086 
2087   jint hi, lo;
2088   if (want_max) {
2089     // We can sharpen the minimum.
2090     hi = MAX2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2091     lo = MAX2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2092   } else {
2093     // We can sharpen the maximum.
2094     hi = MIN2(txvalue-&gt;_hi, tyvalue-&gt;_hi);
2095     lo = MIN2(txvalue-&gt;_lo, tyvalue-&gt;_lo);
2096   }
2097 
2098   // Use a flow-free graph structure, to avoid creating excess control edges
2099   // which could hinder other optimizations.
2100   // Since Math.min/max is often used with arraycopy, we want
2101   // tightly_coupled_allocation to be able to see beyond min/max expressions.
2102   Node* cmov = CMoveNode::make(NULL, best_bol,
2103                                answer_if_false, answer_if_true,
2104                                TypeInt::make(lo, hi, widen));
2105 
2106   return _gvn.transform(cmov);
2107 
2108   /*
2109   // This is not as desirable as it may seem, since Min and Max
2110   // nodes do not have a full set of optimizations.
2111   // And they would interfere, anyway, with 'if' optimizations
2112   // and with CMoveI canonical forms.
2113   switch (id) {
2114   case vmIntrinsics::_min:
2115     result_val = _gvn.transform(new (C, 3) MinINode(x,y)); break;
2116   case vmIntrinsics::_max:
2117     result_val = _gvn.transform(new (C, 3) MaxINode(x,y)); break;
2118   default:
2119     ShouldNotReachHere();
2120   }
2121   */
2122 }
2123 
2124 inline int
2125 LibraryCallKit::classify_unsafe_addr(Node* &amp;base, Node* &amp;offset) {
2126   const TypePtr* base_type = TypePtr::NULL_PTR;
2127   if (base != NULL)  base_type = _gvn.type(base)-&gt;isa_ptr();
2128   if (base_type == NULL) {
2129     // Unknown type.
2130     return Type::AnyPtr;
2131   } else if (base_type == TypePtr::NULL_PTR) {
2132     // Since this is a NULL+long form, we have to switch to a rawptr.
2133     base   = _gvn.transform(new CastX2PNode(offset));
2134     offset = MakeConX(0);
2135     return Type::RawPtr;
2136   } else if (base_type-&gt;base() == Type::RawPtr) {
2137     return Type::RawPtr;
2138   } else if (base_type-&gt;isa_oopptr()) {
2139     // Base is never null =&gt; always a heap address.
2140     if (base_type-&gt;ptr() == TypePtr::NotNull) {
2141       return Type::OopPtr;
2142     }
2143     // Offset is small =&gt; always a heap address.
2144     const TypeX* offset_type = _gvn.type(offset)-&gt;isa_intptr_t();
2145     if (offset_type != NULL &amp;&amp;
2146         base_type-&gt;offset() == 0 &amp;&amp;     // (should always be?)
2147         offset_type-&gt;_lo &gt;= 0 &amp;&amp;
2148         !MacroAssembler::needs_explicit_null_check(offset_type-&gt;_hi)) {
2149       return Type::OopPtr;
2150     }
2151     // Otherwise, it might either be oop+off or NULL+addr.
2152     return Type::AnyPtr;
2153   } else {
2154     // No information:
2155     return Type::AnyPtr;
2156   }
2157 }
2158 
2159 inline Node* LibraryCallKit::make_unsafe_address(Node* base, Node* offset) {
2160   int kind = classify_unsafe_addr(base, offset);
2161   if (kind == Type::RawPtr) {
2162     return basic_plus_adr(top(), base, offset);
2163   } else {
2164     return basic_plus_adr(base, offset);
2165   }
2166 }
2167 
2168 //--------------------------inline_number_methods-----------------------------
2169 // inline int     Integer.numberOfLeadingZeros(int)
2170 // inline int        Long.numberOfLeadingZeros(long)
2171 //
2172 // inline int     Integer.numberOfTrailingZeros(int)
2173 // inline int        Long.numberOfTrailingZeros(long)
2174 //
2175 // inline int     Integer.bitCount(int)
2176 // inline int        Long.bitCount(long)
2177 //
2178 // inline char  Character.reverseBytes(char)
2179 // inline short     Short.reverseBytes(short)
2180 // inline int     Integer.reverseBytes(int)
2181 // inline long       Long.reverseBytes(long)
2182 bool LibraryCallKit::inline_number_methods(vmIntrinsics::ID id) {
2183   Node* arg = argument(0);
2184   Node* n = NULL;
2185   switch (id) {
2186   case vmIntrinsics::_numberOfLeadingZeros_i:   n = new CountLeadingZerosINode( arg);  break;
2187   case vmIntrinsics::_numberOfLeadingZeros_l:   n = new CountLeadingZerosLNode( arg);  break;
2188   case vmIntrinsics::_numberOfTrailingZeros_i:  n = new CountTrailingZerosINode(arg);  break;
2189   case vmIntrinsics::_numberOfTrailingZeros_l:  n = new CountTrailingZerosLNode(arg);  break;
2190   case vmIntrinsics::_bitCount_i:               n = new PopCountINode(          arg);  break;
2191   case vmIntrinsics::_bitCount_l:               n = new PopCountLNode(          arg);  break;
2192   case vmIntrinsics::_reverseBytes_c:           n = new ReverseBytesUSNode(0,   arg);  break;
2193   case vmIntrinsics::_reverseBytes_s:           n = new ReverseBytesSNode( 0,   arg);  break;
2194   case vmIntrinsics::_reverseBytes_i:           n = new ReverseBytesINode( 0,   arg);  break;
2195   case vmIntrinsics::_reverseBytes_l:           n = new ReverseBytesLNode( 0,   arg);  break;
2196   default:  fatal_unexpected_iid(id);  break;
2197   }
2198   set_result(_gvn.transform(n));
2199   return true;
2200 }
2201 
2202 //----------------------------inline_unsafe_access----------------------------
2203 
2204 const static BasicType T_ADDRESS_HOLDER = T_LONG;
2205 
2206 // Helper that guards and inserts a pre-barrier.
2207 void LibraryCallKit::insert_pre_barrier(Node* base_oop, Node* offset,
2208                                         Node* pre_val, bool need_mem_bar) {
2209   // We could be accessing the referent field of a reference object. If so, when G1
2210   // is enabled, we need to log the value in the referent field in an SATB buffer.
2211   // This routine performs some compile time filters and generates suitable
2212   // runtime filters that guard the pre-barrier code.
2213   // Also add memory barrier for non volatile load from the referent field
2214   // to prevent commoning of loads across safepoint.
2215   if (!UseG1GC &amp;&amp; !need_mem_bar)
2216     return;
2217 
2218   // Some compile time checks.
2219 
2220   // If offset is a constant, is it java_lang_ref_Reference::_reference_offset?
2221   const TypeX* otype = offset-&gt;find_intptr_t_type();
2222   if (otype != NULL &amp;&amp; otype-&gt;is_con() &amp;&amp;
2223       otype-&gt;get_con() != java_lang_ref_Reference::referent_offset) {
2224     // Constant offset but not the reference_offset so just return
2225     return;
2226   }
2227 
2228   // We only need to generate the runtime guards for instances.
2229   const TypeOopPtr* btype = base_oop-&gt;bottom_type()-&gt;isa_oopptr();
2230   if (btype != NULL) {
2231     if (btype-&gt;isa_aryptr()) {
2232       // Array type so nothing to do
2233       return;
2234     }
2235 
2236     const TypeInstPtr* itype = btype-&gt;isa_instptr();
2237     if (itype != NULL) {
2238       // Can the klass of base_oop be statically determined to be
2239       // _not_ a sub-class of Reference and _not_ Object?
2240       ciKlass* klass = itype-&gt;klass();
2241       if ( klass-&gt;is_loaded() &amp;&amp;
2242           !klass-&gt;is_subtype_of(env()-&gt;Reference_klass()) &amp;&amp;
2243           !env()-&gt;Object_klass()-&gt;is_subtype_of(klass)) {
2244         return;
2245       }
2246     }
2247   }
2248 
2249   // The compile time filters did not reject base_oop/offset so
2250   // we need to generate the following runtime filters
2251   //
2252   // if (offset == java_lang_ref_Reference::_reference_offset) {
2253   //   if (instance_of(base, java.lang.ref.Reference)) {
2254   //     pre_barrier(_, pre_val, ...);
2255   //   }
2256   // }
2257 
2258   float likely   = PROB_LIKELY(  0.999);
2259   float unlikely = PROB_UNLIKELY(0.999);
2260 
2261   IdealKit ideal(this);
2262 #define __ ideal.
2263 
2264   Node* referent_off = __ ConX(java_lang_ref_Reference::referent_offset);
2265 
2266   __ if_then(offset, BoolTest::eq, referent_off, unlikely); {
2267       // Update graphKit memory and control from IdealKit.
2268       sync_kit(ideal);
2269 
2270       Node* ref_klass_con = makecon(TypeKlassPtr::make(env()-&gt;Reference_klass()));
2271       Node* is_instof = gen_instanceof(base_oop, ref_klass_con);
2272 
2273       // Update IdealKit memory and control from graphKit.
2274       __ sync_kit(this);
2275 
2276       Node* one = __ ConI(1);
2277       // is_instof == 0 if base_oop == NULL
2278       __ if_then(is_instof, BoolTest::eq, one, unlikely); {
2279 
2280         // Update graphKit from IdeakKit.
2281         sync_kit(ideal);
2282 
2283         // Use the pre-barrier to record the value in the referent field
2284         pre_barrier(false /* do_load */,
2285                     __ ctrl(),
2286                     NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
2287                     pre_val /* pre_val */,
2288                     T_OBJECT);
2289         if (need_mem_bar) {
2290           // Add memory barrier to prevent commoning reads from this field
2291           // across safepoint since GC can change its value.
2292           insert_mem_bar(Op_MemBarCPUOrder);
2293         }
2294         // Update IdealKit from graphKit.
2295         __ sync_kit(this);
2296 
2297       } __ end_if(); // _ref_type != ref_none
2298   } __ end_if(); // offset == referent_offset
2299 
2300   // Final sync IdealKit and GraphKit.
2301   final_sync(ideal);
2302 #undef __
2303 }
2304 
2305 
2306 // Interpret Unsafe.fieldOffset cookies correctly:
2307 extern jlong Unsafe_field_offset_to_byte_offset(jlong field_offset);
2308 
2309 const TypeOopPtr* LibraryCallKit::sharpen_unsafe_type(Compile::AliasType* alias_type, const TypePtr *adr_type, bool is_native_ptr) {
2310   // Attempt to infer a sharper value type from the offset and base type.
2311   ciKlass* sharpened_klass = NULL;
2312 
2313   // See if it is an instance field, with an object type.
2314   if (alias_type-&gt;field() != NULL) {
2315     assert(!is_native_ptr, "native pointer op cannot use a java address");
2316     if (alias_type-&gt;field()-&gt;type()-&gt;is_klass()) {
2317       sharpened_klass = alias_type-&gt;field()-&gt;type()-&gt;as_klass();
2318     }
2319   }
2320 
2321   // See if it is a narrow oop array.
2322   if (adr_type-&gt;isa_aryptr()) {
2323     if (adr_type-&gt;offset() &gt;= objArrayOopDesc::base_offset_in_bytes()) {
2324       const TypeOopPtr *elem_type = adr_type-&gt;is_aryptr()-&gt;elem()-&gt;isa_oopptr();
2325       if (elem_type != NULL) {
2326         sharpened_klass = elem_type-&gt;klass();
2327       }
2328     }
2329   }
2330 
2331   // The sharpened class might be unloaded if there is no class loader
2332   // contraint in place.
2333   if (sharpened_klass != NULL &amp;&amp; sharpened_klass-&gt;is_loaded()) {
2334     const TypeOopPtr* tjp = TypeOopPtr::make_from_klass(sharpened_klass);
2335 
2336 #ifndef PRODUCT
2337     if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
2338       tty-&gt;print("  from base type: ");  adr_type-&gt;dump();
2339       tty-&gt;print("  sharpened value: ");  tjp-&gt;dump();
2340     }
2341 #endif
2342     // Sharpen the value type.
2343     return tjp;
2344   }
2345   return NULL;
2346 }
2347 
2348 bool LibraryCallKit::inline_unsafe_access(const bool is_native_ptr, bool is_store, const BasicType type, const AccessKind kind, const bool unaligned) {
2349   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2350   guarantee(!is_store || kind != Acquire, "Acquire accesses can be produced only for loads");
2351   guarantee( is_store || kind != Release, "Release accesses can be produced only for stores");
2352 
2353 #ifndef PRODUCT
2354   {
2355     ResourceMark rm;
2356     // Check the signatures.
2357     ciSignature* sig = callee()-&gt;signature();
2358 #ifdef ASSERT
2359     if (!is_store) {
2360       // Object getObject(Object base, int/long offset), etc.
2361       BasicType rtype = sig-&gt;return_type()-&gt;basic_type();
2362       if (rtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::getAddress_name())
2363           rtype = T_ADDRESS;  // it is really a C void*
2364       assert(rtype == type, "getter must return the expected value");
2365       if (!is_native_ptr) {
2366         assert(sig-&gt;count() == 2, "oop getter has 2 arguments");
2367         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "getter base is object");
2368         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "getter offset is correct");
2369       } else {
2370         assert(sig-&gt;count() == 1, "native getter has 1 argument");
2371         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "getter base is long");
2372       }
2373     } else {
2374       // void putObject(Object base, int/long offset, Object x), etc.
2375       assert(sig-&gt;return_type()-&gt;basic_type() == T_VOID, "putter must not return a value");
2376       if (!is_native_ptr) {
2377         assert(sig-&gt;count() == 3, "oop putter has 3 arguments");
2378         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "putter base is object");
2379         assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "putter offset is correct");
2380       } else {
2381         assert(sig-&gt;count() == 2, "native putter has 2 arguments");
2382         assert(sig-&gt;type_at(0)-&gt;basic_type() == T_LONG, "putter base is long");
2383       }
2384       BasicType vtype = sig-&gt;type_at(sig-&gt;count()-1)-&gt;basic_type();
2385       if (vtype == T_ADDRESS_HOLDER &amp;&amp; callee()-&gt;name() == ciSymbol::putAddress_name())
2386         vtype = T_ADDRESS;  // it is really a C void*
2387       assert(vtype == type, "putter must accept the expected value");
2388     }
2389 #endif // ASSERT
2390  }
2391 #endif //PRODUCT
2392 
2393   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2394 
2395   Node* receiver = argument(0);  // type: oop
2396 
2397   // Build address expression.
2398   Node* adr;
2399   Node* heap_base_oop = top();
2400   Node* offset = top();
2401   Node* val;
2402 
2403   if (!is_native_ptr) {
2404     // The base is either a Java object or a value produced by Unsafe.staticFieldBase
2405     Node* base = argument(1);  // type: oop
2406     // The offset is a value produced by Unsafe.staticFieldOffset or Unsafe.objectFieldOffset
2407     offset = argument(2);  // type: long
2408     // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2409     // to be plain byte offsets, which are also the same as those accepted
2410     // by oopDesc::field_base.
2411     assert(Unsafe_field_offset_to_byte_offset(11) == 11,
2412            "fieldOffset must be byte-scaled");
2413     // 32-bit machines ignore the high half!
2414     offset = ConvL2X(offset);
2415     adr = make_unsafe_address(base, offset);
2416     heap_base_oop = base;
2417     val = is_store ? argument(4) : NULL;
2418   } else {
2419     Node* ptr = argument(1);  // type: long
2420     ptr = ConvL2X(ptr);  // adjust Java long to machine word
2421     adr = make_unsafe_address(NULL, ptr);
2422     val = is_store ? argument(3) : NULL;
2423   }
2424 
2425   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2426 
2427   // First guess at the value type.
2428   const Type *value_type = Type::get_const_basic_type(type);
2429 
2430   // Try to categorize the address.  If it comes up as TypeJavaPtr::BOTTOM,
2431   // there was not enough information to nail it down.
2432   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2433   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2434 
2435   // We will need memory barriers unless we can determine a unique
2436   // alias category for this reference.  (Note:  If for some reason
2437   // the barriers get omitted and the unsafe reference begins to "pollute"
2438   // the alias analysis of the rest of the graph, either Compile::can_alias
2439   // or Compile::must_alias will throw a diagnostic assert.)
2440   bool need_mem_bar;
2441   switch (kind) {
2442       case Relaxed:
2443           need_mem_bar = (alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM);
2444           break;
2445       case Opaque:
2446           // Opaque uses CPUOrder membars for protection against code movement.
2447       case Acquire:
2448       case Release:
2449       case Volatile:
2450           need_mem_bar = true;
2451           break;
2452       default:
2453           ShouldNotReachHere();
2454   }
2455 
2456   // Some accesses require access atomicity for all types, notably longs and doubles.
2457   // When AlwaysAtomicAccesses is enabled, all accesses are atomic.
2458   bool requires_atomic_access = false;
2459   switch (kind) {
2460       case Relaxed:
2461       case Opaque:
2462           requires_atomic_access = AlwaysAtomicAccesses;
2463           break;
2464       case Acquire:
2465       case Release:
2466       case Volatile:
2467           requires_atomic_access = true;
2468           break;
2469       default:
2470           ShouldNotReachHere();
2471   }
2472 
2473   // Figure out the memory ordering.
2474   // Acquire/Release/Volatile accesses require marking the loads/stores with MemOrd
2475   MemNode::MemOrd mo = access_kind_to_memord_LS(kind, is_store);
2476 
2477   // If we are reading the value of the referent field of a Reference
2478   // object (either by using Unsafe directly or through reflection)
2479   // then, if G1 is enabled, we need to record the referent in an
2480   // SATB log buffer using the pre-barrier mechanism.
2481   // Also we need to add memory barrier to prevent commoning reads
2482   // from this field across safepoint since GC can change its value.
2483   bool need_read_barrier = !is_native_ptr &amp;&amp; !is_store &amp;&amp;
2484                            offset != top() &amp;&amp; heap_base_oop != top();
2485 
2486   if (!is_store &amp;&amp; type == T_OBJECT) {
2487     const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type, is_native_ptr);
2488     if (tjp != NULL) {
2489       value_type = tjp;
2490     }
2491   }
2492 
2493   receiver = null_check(receiver);
2494   if (stopped()) {
2495     return true;
2496   }
2497   // Heap pointers get a null-check from the interpreter,
2498   // as a courtesy.  However, this is not guaranteed by Unsafe,
2499   // and it is not possible to fully distinguish unintended nulls
2500   // from intended ones in this API.
2501 
2502   // We need to emit leading and trailing CPU membars (see below) in
2503   // addition to memory membars for special access modes. This is a little
2504   // too strong, but avoids the need to insert per-alias-type
2505   // volatile membars (for stores; compare Parse::do_put_xxx), which
2506   // we cannot do effectively here because we probably only have a
2507   // rough approximation of type.
2508 
2509   switch(kind) {
2510     case Relaxed:
2511     case Opaque:
2512     case Acquire:
2513       break;
2514     case Release:
2515     case Volatile:
2516       if (is_store) {
2517         insert_mem_bar(Op_MemBarRelease);
2518       } else {
2519         if (support_IRIW_for_not_multiple_copy_atomic_cpu) {
2520           insert_mem_bar(Op_MemBarVolatile);
2521         }
2522       }
2523       break;
2524     default:
2525       ShouldNotReachHere();
2526   }
2527 
2528   // Memory barrier to prevent normal and 'unsafe' accesses from
2529   // bypassing each other.  Happens after null checks, so the
2530   // exception paths do not take memory state from the memory barrier,
2531   // so there's no problems making a strong assert about mixing users
2532   // of safe &amp; unsafe memory.
2533   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2534 
2535   assert(alias_type-&gt;adr_type() == TypeRawPtr::BOTTOM || alias_type-&gt;adr_type() == TypeOopPtr::BOTTOM ||
2536          alias_type-&gt;field() != NULL || alias_type-&gt;element() != NULL, "field, array element or unknown");
2537   bool mismatched = false;
2538   if (alias_type-&gt;element() != NULL || alias_type-&gt;field() != NULL) {
2539     BasicType bt;
2540     if (alias_type-&gt;element() != NULL) {
2541       // Use address type to get the element type. Alias type doesn't provide
2542       // enough information (e.g., doesn't differentiate between byte[] and boolean[]).
2543       const Type* element = adr_type-&gt;is_aryptr()-&gt;elem();
2544       bt = element-&gt;isa_narrowoop() ? T_OBJECT : element-&gt;array_element_basic_type();
2545     } else {
2546       bt = alias_type-&gt;field()-&gt;layout_type();
2547     }
2548     if (bt == T_ARRAY) {
2549       // accessing an array field with getObject is not a mismatch
2550       bt = T_OBJECT;
2551     }
2552     if (bt != type) {
2553       mismatched = true;
2554     }
2555   }
2556   assert(type != T_OBJECT || !unaligned, "unaligned access not supported with object type");
2557 
2558   if (!is_store) {
2559     Node* p = NULL;
2560     // Try to constant fold a load from a constant field
2561     ciField* field = alias_type-&gt;field();
2562     if (heap_base_oop != top() &amp;&amp;
2563         field != NULL &amp;&amp; field-&gt;is_constant() &amp;&amp; !mismatched) {
2564       // final or stable field
2565       const Type* con_type = Type::make_constant(alias_type-&gt;field(), heap_base_oop);
2566       if (con_type != NULL) {
2567         p = makecon(con_type);
2568       }
2569     }
2570     if (p == NULL) {
2571       // To be valid, unsafe loads may depend on other conditions than
2572       // the one that guards them: pin the Load node
2573       p = make_load(control(), adr, value_type, type, adr_type, mo, LoadNode::Pinned, requires_atomic_access, unaligned, mismatched);
2574       // load value
2575       switch (type) {
2576       case T_BOOLEAN:
2577       case T_CHAR:
2578       case T_BYTE:
2579       case T_SHORT:
2580       case T_INT:
2581       case T_LONG:
2582       case T_FLOAT:
2583       case T_DOUBLE:
2584         break;
2585       case T_OBJECT:
2586         if (need_read_barrier) {
2587           // We do not require a mem bar inside pre_barrier if need_mem_bar
2588           // is set: the barriers would be emitted by us.
2589           insert_pre_barrier(heap_base_oop, offset, p, !need_mem_bar);
2590         }
2591         break;
2592       case T_ADDRESS:
2593         // Cast to an int type.
2594         p = _gvn.transform(new CastP2XNode(NULL, p));
2595         p = ConvX2UL(p);
2596         break;
2597       default:
2598         fatal("unexpected type %d: %s", type, type2name(type));
2599         break;
2600       }
2601     }
2602     // The load node has the control of the preceding MemBarCPUOrder.  All
2603     // following nodes will have the control of the MemBarCPUOrder inserted at
2604     // the end of this method.  So, pushing the load onto the stack at a later
2605     // point is fine.
2606     set_result(p);
2607   } else {
2608     // place effect of store into memory
2609     switch (type) {
2610     case T_DOUBLE:
2611       val = dstore_rounding(val);
2612       break;
2613     case T_ADDRESS:
2614       // Repackage the long as a pointer.
2615       val = ConvL2X(val);
2616       val = _gvn.transform(new CastX2PNode(val));
2617       break;
2618     }
2619 
2620     if (type != T_OBJECT) {
2621       (void) store_to_memory(control(), adr, val, type, adr_type, mo, requires_atomic_access, unaligned, mismatched);
2622     } else {
2623       // Possibly an oop being stored to Java heap or native memory
2624       if (!TypePtr::NULL_PTR-&gt;higher_equal(_gvn.type(heap_base_oop))) {
2625         // oop to Java heap.
2626         (void) store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo, mismatched);
2627       } else {
2628         // We can't tell at compile time if we are storing in the Java heap or outside
2629         // of it. So we need to emit code to conditionally do the proper type of
2630         // store.
2631 
2632         IdealKit ideal(this);
2633 #define __ ideal.
2634         // QQQ who knows what probability is here??
2635         __ if_then(heap_base_oop, BoolTest::ne, null(), PROB_UNLIKELY(0.999)); {
2636           // Sync IdealKit and graphKit.
2637           sync_kit(ideal);
2638           Node* st = store_oop_to_unknown(control(), heap_base_oop, adr, adr_type, val, type, mo, mismatched);
2639           // Update IdealKit memory.
2640           __ sync_kit(this);
2641         } __ else_(); {
2642           __ store(__ ctrl(), adr, val, type, alias_type-&gt;index(), mo, requires_atomic_access, mismatched);
2643         } __ end_if();
2644         // Final sync IdealKit and GraphKit.
2645         final_sync(ideal);
2646 #undef __
2647       }
2648     }
2649   }
2650 
2651   switch(kind) {
2652     case Relaxed:
2653     case Opaque:
2654     case Release:
2655       break;
2656     case Acquire:
2657     case Volatile:
2658       if (!is_store) {
2659         insert_mem_bar(Op_MemBarAcquire);
2660       } else {
2661         if (!support_IRIW_for_not_multiple_copy_atomic_cpu) {
2662           insert_mem_bar(Op_MemBarVolatile);
2663         }
2664       }
2665       break;
2666     default:
2667       ShouldNotReachHere();
2668   }
2669 
2670   if (need_mem_bar) insert_mem_bar(Op_MemBarCPUOrder);
2671 
2672   return true;
2673 }
2674 
2675 //----------------------------inline_unsafe_load_store----------------------------
2676 // This method serves a couple of different customers (depending on LoadStoreKind):
2677 //
2678 // LS_cmp_swap:
2679 //
2680 //   boolean compareAndSwapObject(Object o, long offset, Object expected, Object x);
2681 //   boolean compareAndSwapInt(   Object o, long offset, int    expected, int    x);
2682 //   boolean compareAndSwapLong(  Object o, long offset, long   expected, long   x);
2683 //
2684 // LS_cmp_swap_weak:
2685 //
2686 //   boolean weakCompareAndSwapObject(       Object o, long offset, Object expected, Object x);
2687 //   boolean weakCompareAndSwapObjectAcquire(Object o, long offset, Object expected, Object x);
2688 //   boolean weakCompareAndSwapObjectRelease(Object o, long offset, Object expected, Object x);
2689 //
2690 //   boolean weakCompareAndSwapInt(          Object o, long offset, int    expected, int    x);
2691 //   boolean weakCompareAndSwapIntAcquire(   Object o, long offset, int    expected, int    x);
2692 //   boolean weakCompareAndSwapIntRelease(   Object o, long offset, int    expected, int    x);
2693 //
2694 //   boolean weakCompareAndSwapLong(         Object o, long offset, long   expected, long   x);
2695 //   boolean weakCompareAndSwapLongAcquire(  Object o, long offset, long   expected, long   x);
2696 //   boolean weakCompareAndSwapLongRelease(  Object o, long offset, long   expected, long   x);
2697 //
2698 // LS_cmp_exchange:
2699 //
2700 //   Object compareAndExchangeObjectVolatile(Object o, long offset, Object expected, Object x);
2701 //   Object compareAndExchangeObjectAcquire( Object o, long offset, Object expected, Object x);
2702 //   Object compareAndExchangeObjectRelease( Object o, long offset, Object expected, Object x);
2703 //
2704 //   Object compareAndExchangeIntVolatile(   Object o, long offset, Object expected, Object x);
2705 //   Object compareAndExchangeIntAcquire(    Object o, long offset, Object expected, Object x);
2706 //   Object compareAndExchangeIntRelease(    Object o, long offset, Object expected, Object x);
2707 //
2708 //   Object compareAndExchangeLongVolatile(  Object o, long offset, Object expected, Object x);
2709 //   Object compareAndExchangeLongAcquire(   Object o, long offset, Object expected, Object x);
2710 //   Object compareAndExchangeLongRelease(   Object o, long offset, Object expected, Object x);
2711 //
2712 // LS_get_add:
2713 //
2714 //   int  getAndAddInt( Object o, long offset, int  delta)
2715 //   long getAndAddLong(Object o, long offset, long delta)
2716 //
2717 // LS_get_set:
2718 //
2719 //   int    getAndSet(Object o, long offset, int    newValue)
2720 //   long   getAndSet(Object o, long offset, long   newValue)
2721 //   Object getAndSet(Object o, long offset, Object newValue)
2722 //
2723 bool LibraryCallKit::inline_unsafe_load_store(const BasicType type, const LoadStoreKind kind, const AccessKind access_kind) {
2724   // This basic scheme here is the same as inline_unsafe_access, but
2725   // differs in enough details that combining them would make the code
2726   // overly confusing.  (This is a true fact! I originally combined
2727   // them, but even I was confused by it!) As much code/comments as
2728   // possible are retained from inline_unsafe_access though to make
2729   // the correspondences clearer. - dl
2730 
2731   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
2732 
2733 #ifndef PRODUCT
2734   BasicType rtype;
2735   {
2736     ResourceMark rm;
2737     // Check the signatures.
2738     ciSignature* sig = callee()-&gt;signature();
2739     rtype = sig-&gt;return_type()-&gt;basic_type();
2740     switch(kind) {
2741       case LS_get_add:
2742       case LS_get_set: {
2743       // Check the signatures.
2744 #ifdef ASSERT
2745       assert(rtype == type, "get and set must return the expected type");
2746       assert(sig-&gt;count() == 3, "get and set has 3 arguments");
2747       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "get and set base is object");
2748       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "get and set offset is long");
2749       assert(sig-&gt;type_at(2)-&gt;basic_type() == type, "get and set must take expected type as new value/delta");
2750 #endif // ASSERT
2751         break;
2752       }
2753       case LS_cmp_swap:
2754       case LS_cmp_swap_weak: {
2755       // Check the signatures.
2756 #ifdef ASSERT
2757       assert(rtype == T_BOOLEAN, "CAS must return boolean");
2758       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2759       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2760       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2761 #endif // ASSERT
2762         break;
2763       }
2764       case LS_cmp_exchange: {
2765       // Check the signatures.
2766 #ifdef ASSERT
2767       assert(rtype == type, "CAS must return the expected type");
2768       assert(sig-&gt;count() == 4, "CAS has 4 arguments");
2769       assert(sig-&gt;type_at(0)-&gt;basic_type() == T_OBJECT, "CAS base is object");
2770       assert(sig-&gt;type_at(1)-&gt;basic_type() == T_LONG, "CAS offset is long");
2771 #endif // ASSERT
2772         break;
2773       }
2774       default:
2775         ShouldNotReachHere();
2776     }
2777   }
2778 #endif //PRODUCT
2779 
2780   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
2781 
2782   // Get arguments:
2783   Node* receiver = NULL;
2784   Node* base     = NULL;
2785   Node* offset   = NULL;
2786   Node* oldval   = NULL;
2787   Node* newval   = NULL;
2788   switch(kind) {
2789     case LS_cmp_swap:
2790     case LS_cmp_swap_weak:
2791     case LS_cmp_exchange: {
2792       const bool two_slot_type = type2size[type] == 2;
2793       receiver = argument(0);  // type: oop
2794       base     = argument(1);  // type: oop
2795       offset   = argument(2);  // type: long
2796       oldval   = argument(4);  // type: oop, int, or long
2797       newval   = argument(two_slot_type ? 6 : 5);  // type: oop, int, or long
2798       break;
2799     }
2800     case LS_get_add:
2801     case LS_get_set: {
2802       receiver = argument(0);  // type: oop
2803       base     = argument(1);  // type: oop
2804       offset   = argument(2);  // type: long
2805       oldval   = NULL;
2806       newval   = argument(4);  // type: oop, int, or long
2807       break;
2808     }
2809     default:
2810       ShouldNotReachHere();
2811   }
2812 
2813   // Null check receiver.
2814   receiver = null_check(receiver);
2815   if (stopped()) {
2816     return true;
2817   }
2818 
2819   // Build field offset expression.
2820   // We currently rely on the cookies produced by Unsafe.xxxFieldOffset
2821   // to be plain byte offsets, which are also the same as those accepted
2822   // by oopDesc::field_base.
2823   assert(Unsafe_field_offset_to_byte_offset(11) == 11, "fieldOffset must be byte-scaled");
2824   // 32-bit machines ignore the high half of long offsets
2825   offset = ConvL2X(offset);
2826   Node* adr = make_unsafe_address(base, offset);
2827   const TypePtr *adr_type = _gvn.type(adr)-&gt;isa_ptr();
2828 
2829   // For CAS, unlike inline_unsafe_access, there seems no point in
2830   // trying to refine types. Just use the coarse types here.
2831   const Type *value_type = Type::get_const_basic_type(type);
2832   Compile::AliasType* alias_type = C-&gt;alias_type(adr_type);
2833   assert(alias_type-&gt;index() != Compile::AliasIdxBot, "no bare pointers here");
2834 
2835   switch (kind) {
2836     case LS_get_set:
2837     case LS_cmp_exchange: {
2838       if (type == T_OBJECT) {
2839         const TypeOopPtr* tjp = sharpen_unsafe_type(alias_type, adr_type);
2840         if (tjp != NULL) {
2841           value_type = tjp;
2842         }
2843       }
2844       break;
2845     }
2846     case LS_cmp_swap:
2847     case LS_cmp_swap_weak:
2848     case LS_get_add:
2849       break;
2850     default:
2851       ShouldNotReachHere();
2852   }
2853 
2854   int alias_idx = C-&gt;get_alias_index(adr_type);
2855 
2856   // Memory-model-wise, a LoadStore acts like a little synchronized
2857   // block, so needs barriers on each side.  These don't translate
2858   // into actual barriers on most machines, but we still need rest of
2859   // compiler to respect ordering.
2860 
2861   switch (access_kind) {
2862     case Relaxed:
2863     case Acquire:
2864       break;
2865     case Release:
2866     case Volatile:
2867       insert_mem_bar(Op_MemBarRelease);
2868       break;
2869     default:
2870       ShouldNotReachHere();
2871   }
2872   insert_mem_bar(Op_MemBarCPUOrder);
2873 
2874   // Figure out the memory ordering.
2875   MemNode::MemOrd mo = access_kind_to_memord(access_kind);
2876 
2877   // 4984716: MemBars must be inserted before this
2878   //          memory node in order to avoid a false
2879   //          dependency which will confuse the scheduler.
2880   Node *mem = memory(alias_idx);
2881 
2882   // For now, we handle only those cases that actually exist: ints,
2883   // longs, and Object. Adding others should be straightforward.
2884   Node* load_store = NULL;
2885   switch(type) {
2886   case T_INT:
2887     switch(kind) {
2888       case LS_get_add:
2889         load_store = _gvn.transform(new GetAndAddINode(control(), mem, adr, newval, adr_type));
2890         break;
2891       case LS_get_set:
2892         load_store = _gvn.transform(new GetAndSetINode(control(), mem, adr, newval, adr_type));
2893         break;
2894       case LS_cmp_swap_weak:
2895         load_store = _gvn.transform(new WeakCompareAndSwapINode(control(), mem, adr, newval, oldval, mo));
2896         break;
2897       case LS_cmp_swap:
2898         load_store = _gvn.transform(new CompareAndSwapINode(control(), mem, adr, newval, oldval, mo));
2899         break;
2900       case LS_cmp_exchange:
2901         load_store = _gvn.transform(new CompareAndExchangeINode(control(), mem, adr, newval, oldval, adr_type, mo));
2902         break;
2903       default:
2904         ShouldNotReachHere();
2905     }
2906     break;
2907   case T_LONG:
2908     switch(kind) {
2909       case LS_get_add:
2910         load_store = _gvn.transform(new GetAndAddLNode(control(), mem, adr, newval, adr_type));
2911         break;
2912       case LS_get_set:
2913         load_store = _gvn.transform(new GetAndSetLNode(control(), mem, adr, newval, adr_type));
2914         break;
2915       case LS_cmp_swap_weak:
2916         load_store = _gvn.transform(new WeakCompareAndSwapLNode(control(), mem, adr, newval, oldval, mo));
2917         break;
2918       case LS_cmp_swap:
2919         load_store = _gvn.transform(new CompareAndSwapLNode(control(), mem, adr, newval, oldval, mo));
2920         break;
2921       case LS_cmp_exchange:
2922         load_store = _gvn.transform(new CompareAndExchangeLNode(control(), mem, adr, newval, oldval, adr_type, mo));
2923         break;
2924       default:
2925         ShouldNotReachHere();
2926     }
2927     break;
2928   case T_OBJECT:
2929     // Transformation of a value which could be NULL pointer (CastPP #NULL)
2930     // could be delayed during Parse (for example, in adjust_map_after_if()).
2931     // Execute transformation here to avoid barrier generation in such case.
2932     if (_gvn.type(newval) == TypePtr::NULL_PTR)
2933       newval = _gvn.makecon(TypePtr::NULL_PTR);
2934 
2935     // Reference stores need a store barrier.
2936     switch(kind) {
2937       case LS_get_set: {
2938         // If pre-barrier must execute before the oop store, old value will require do_load here.
2939         if (!can_move_pre_barrier()) {
2940           pre_barrier(true /* do_load*/,
2941                       control(), base, adr, alias_idx, newval, value_type-&gt;make_oopptr(),
2942                       NULL /* pre_val*/,
2943                       T_OBJECT);
2944         } // Else move pre_barrier to use load_store value, see below.
2945         break;
2946       }
2947       case LS_cmp_swap_weak:
2948       case LS_cmp_swap:
2949       case LS_cmp_exchange: {
2950         // Same as for newval above:
2951         if (_gvn.type(oldval) == TypePtr::NULL_PTR) {
2952           oldval = _gvn.makecon(TypePtr::NULL_PTR);
2953         }
2954         // The only known value which might get overwritten is oldval.
2955         pre_barrier(false /* do_load */,
2956                     control(), NULL, NULL, max_juint, NULL, NULL,
2957                     oldval /* pre_val */,
2958                     T_OBJECT);
2959         break;
2960       }
2961       default:
2962         ShouldNotReachHere();
2963     }
2964 
2965 #ifdef _LP64
2966     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
2967       Node *newval_enc = _gvn.transform(new EncodePNode(newval, newval-&gt;bottom_type()-&gt;make_narrowoop()));
2968 
2969       switch(kind) {
2970         case LS_get_set:
2971           load_store = _gvn.transform(new GetAndSetNNode(control(), mem, adr, newval_enc, adr_type, value_type-&gt;make_narrowoop()));
2972           break;
2973         case LS_cmp_swap_weak: {
2974           Node *oldval_enc = _gvn.transform(new EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
2975           load_store = _gvn.transform(new WeakCompareAndSwapNNode(control(), mem, adr, newval_enc, oldval_enc, mo));
2976           break;
2977         }
2978         case LS_cmp_swap: {
2979           Node *oldval_enc = _gvn.transform(new EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
2980           load_store = _gvn.transform(new CompareAndSwapNNode(control(), mem, adr, newval_enc, oldval_enc, mo));
2981           break;
2982         }
2983         case LS_cmp_exchange: {
2984           Node *oldval_enc = _gvn.transform(new EncodePNode(oldval, oldval-&gt;bottom_type()-&gt;make_narrowoop()));
2985           load_store = _gvn.transform(new CompareAndExchangeNNode(control(), mem, adr, newval_enc, oldval_enc, adr_type, value_type-&gt;make_narrowoop(), mo));
2986           break;
2987         }
2988         default:
2989           ShouldNotReachHere();
2990       }
2991     } else
2992 #endif
2993     switch (kind) {
2994       case LS_get_set:
2995         load_store = _gvn.transform(new GetAndSetPNode(control(), mem, adr, newval, adr_type, value_type-&gt;is_oopptr()));
2996         break;
2997       case LS_cmp_swap_weak:
2998         load_store = _gvn.transform(new WeakCompareAndSwapPNode(control(), mem, adr, newval, oldval, mo));
2999         break;
3000       case LS_cmp_swap:
3001         load_store = _gvn.transform(new CompareAndSwapPNode(control(), mem, adr, newval, oldval, mo));
3002         break;
3003       case LS_cmp_exchange:
3004         load_store = _gvn.transform(new CompareAndExchangePNode(control(), mem, adr, newval, oldval, adr_type, value_type-&gt;is_oopptr(), mo));
3005         break;
3006       default:
3007         ShouldNotReachHere();
3008     }
3009 
3010     // Emit the post barrier only when the actual store happened. This makes sense
3011     // to check only for LS_cmp_* that can fail to set the value.
3012     // LS_cmp_exchange does not produce any branches by default, so there is no
3013     // boolean result to piggyback on. TODO: When we merge CompareAndSwap with
3014     // CompareAndExchange and move branches here, it would make sense to conditionalize
3015     // post_barriers for LS_cmp_exchange as well.
3016     //
3017     // CAS success path is marked more likely since we anticipate this is a performance
3018     // critical path, while CAS failure path can use the penalty for going through unlikely
3019     // path as backoff. Which is still better than doing a store barrier there.
3020     switch (kind) {
3021       case LS_get_set:
3022       case LS_cmp_exchange: {
3023         post_barrier(control(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
3024         break;
3025       }
3026       case LS_cmp_swap_weak:
3027       case LS_cmp_swap: {
3028         IdealKit ideal(this);
3029         ideal.if_then(load_store, BoolTest::ne, ideal.ConI(0), PROB_STATIC_FREQUENT); {
3030           sync_kit(ideal);
3031           post_barrier(ideal.ctrl(), load_store, base, adr, alias_idx, newval, T_OBJECT, true);
3032           ideal.sync_kit(this);
3033         } ideal.end_if();
3034         final_sync(ideal);
3035         break;
3036       }
3037       default:
3038         ShouldNotReachHere();
3039     }
3040     break;
3041   default:
3042     fatal("unexpected type %d: %s", type, type2name(type));
3043     break;
3044   }
3045 
3046   // SCMemProjNodes represent the memory state of a LoadStore. Their
3047   // main role is to prevent LoadStore nodes from being optimized away
3048   // when their results aren't used.
3049   Node* proj = _gvn.transform(new SCMemProjNode(load_store));
3050   set_memory(proj, alias_idx);
3051 
3052   if (type == T_OBJECT &amp;&amp; (kind == LS_get_set || kind == LS_cmp_exchange)) {
3053 #ifdef _LP64
3054     if (adr-&gt;bottom_type()-&gt;is_ptr_to_narrowoop()) {
3055       load_store = _gvn.transform(new DecodeNNode(load_store, load_store-&gt;get_ptr_type()));
3056     }
3057 #endif
3058     if (can_move_pre_barrier()) {
3059       // Don't need to load pre_val. The old value is returned by load_store.
3060       // The pre_barrier can execute after the xchg as long as no safepoint
3061       // gets inserted between them.
3062       pre_barrier(false /* do_load */,
3063                   control(), NULL, NULL, max_juint, NULL, NULL,
3064                   load_store /* pre_val */,
3065                   T_OBJECT);
3066     }
3067   }
3068 
3069   // Add the trailing membar surrounding the access
3070   insert_mem_bar(Op_MemBarCPUOrder);
3071 
3072   switch (access_kind) {
3073     case Relaxed:
3074     case Release:
3075       break; // do nothing
3076     case Acquire:
3077     case Volatile:
3078       insert_mem_bar(Op_MemBarAcquire);
3079       break;
3080     default:
3081       ShouldNotReachHere();
3082   }
3083 
3084   assert(type2size[load_store-&gt;bottom_type()-&gt;basic_type()] == type2size[rtype], "result type should match");
3085   set_result(load_store);
3086   return true;
3087 }
3088 
3089 MemNode::MemOrd LibraryCallKit::access_kind_to_memord_LS(AccessKind kind, bool is_store) {
3090   MemNode::MemOrd mo = MemNode::unset;
3091   switch(kind) {
3092     case Opaque:
3093     case Relaxed:  mo = MemNode::unordered; break;
3094     case Acquire:  mo = MemNode::acquire;   break;
3095     case Release:  mo = MemNode::release;   break;
3096     case Volatile: mo = is_store ? MemNode::release : MemNode::acquire; break;
3097     default:
3098       ShouldNotReachHere();
3099   }
3100   guarantee(mo != MemNode::unset, "Should select memory ordering");
3101   return mo;
3102 }
3103 
3104 MemNode::MemOrd LibraryCallKit::access_kind_to_memord(AccessKind kind) {
3105   MemNode::MemOrd mo = MemNode::unset;
3106   switch(kind) {
3107     case Opaque:
3108     case Relaxed:  mo = MemNode::unordered; break;
3109     case Acquire:  mo = MemNode::acquire;   break;
3110     case Release:  mo = MemNode::release;   break;
3111     case Volatile: mo = MemNode::seqcst;    break;
3112     default:
3113       ShouldNotReachHere();
3114   }
3115   guarantee(mo != MemNode::unset, "Should select memory ordering");
3116   return mo;
3117 }
3118 
3119 bool LibraryCallKit::inline_unsafe_fence(vmIntrinsics::ID id) {
3120   // Regardless of form, don't allow previous ld/st to move down,
3121   // then issue acquire, release, or volatile mem_bar.
3122   insert_mem_bar(Op_MemBarCPUOrder);
3123   switch(id) {
3124     case vmIntrinsics::_loadFence:
3125       insert_mem_bar(Op_LoadFence);
3126       return true;
3127     case vmIntrinsics::_storeFence:
3128       insert_mem_bar(Op_StoreFence);
3129       return true;
3130     case vmIntrinsics::_fullFence:
3131       insert_mem_bar(Op_MemBarVolatile);
3132       return true;
3133     default:
3134       fatal_unexpected_iid(id);
3135       return false;
3136   }
3137 }
3138 
3139 bool LibraryCallKit::klass_needs_init_guard(Node* kls) {
3140   if (!kls-&gt;is_Con()) {
3141     return true;
3142   }
3143   const TypeKlassPtr* klsptr = kls-&gt;bottom_type()-&gt;isa_klassptr();
3144   if (klsptr == NULL) {
3145     return true;
3146   }
3147   ciInstanceKlass* ik = klsptr-&gt;klass()-&gt;as_instance_klass();
3148   // don't need a guard for a klass that is already initialized
3149   return !ik-&gt;is_initialized();
3150 }
3151 
3152 //----------------------------inline_unsafe_allocate---------------------------
3153 // public native Object Unsafe.allocateInstance(Class&lt;?&gt; cls);
3154 bool LibraryCallKit::inline_unsafe_allocate() {
3155   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
3156 
3157   null_check_receiver();  // null-check, then ignore
3158   Node* cls = null_check(argument(1));
3159   if (stopped())  return true;
3160 
3161   Node* kls = load_klass_from_mirror(cls, false, NULL, 0);
3162   kls = null_check(kls);
3163   if (stopped())  return true;  // argument was like int.class
3164 
3165   Node* test = NULL;
3166   if (LibraryCallKit::klass_needs_init_guard(kls)) {
3167     // Note:  The argument might still be an illegal value like
3168     // Serializable.class or Object[].class.   The runtime will handle it.
3169     // But we must make an explicit check for initialization.
3170     Node* insp = basic_plus_adr(kls, in_bytes(InstanceKlass::init_state_offset()));
3171     // Use T_BOOLEAN for InstanceKlass::_init_state so the compiler
3172     // can generate code to load it as unsigned byte.
3173     Node* inst = make_load(NULL, insp, TypeInt::UBYTE, T_BOOLEAN, MemNode::unordered);
3174     Node* bits = intcon(InstanceKlass::fully_initialized);
3175     test = _gvn.transform(new SubINode(inst, bits));
3176     // The 'test' is non-zero if we need to take a slow path.
3177   }
3178 
3179   Node* obj = new_instance(kls, test);
3180   set_result(obj);
3181   return true;
3182 }
3183 
3184 //------------------------inline_native_time_funcs--------------
3185 // inline code for System.currentTimeMillis() and System.nanoTime()
3186 // these have the same type and signature
3187 bool LibraryCallKit::inline_native_time_funcs(address funcAddr, const char* funcName) {
3188   const TypeFunc* tf = OptoRuntime::void_long_Type();
3189   const TypePtr* no_memory_effects = NULL;
3190   Node* time = make_runtime_call(RC_LEAF, tf, funcAddr, funcName, no_memory_effects);
3191   Node* value = _gvn.transform(new ProjNode(time, TypeFunc::Parms+0));
3192 #ifdef ASSERT
3193   Node* value_top = _gvn.transform(new ProjNode(time, TypeFunc::Parms+1));
3194   assert(value_top == top(), "second value must be top");
3195 #endif
3196   set_result(value);
3197   return true;
3198 }
3199 
3200 //------------------------inline_native_currentThread------------------
3201 bool LibraryCallKit::inline_native_currentThread() {
3202   Node* junk = NULL;
3203   set_result(generate_current_thread(junk));
3204   return true;
3205 }
3206 
3207 //------------------------inline_native_isInterrupted------------------
3208 // private native boolean java.lang.Thread.isInterrupted(boolean ClearInterrupted);
3209 bool LibraryCallKit::inline_native_isInterrupted() {
3210   // Add a fast path to t.isInterrupted(clear_int):
3211   //   (t == Thread.current() &amp;&amp;
3212   //    (!TLS._osthread._interrupted || WINDOWS_ONLY(false) NOT_WINDOWS(!clear_int)))
3213   //   ? TLS._osthread._interrupted : /*slow path:*/ t.isInterrupted(clear_int)
3214   // So, in the common case that the interrupt bit is false,
3215   // we avoid making a call into the VM.  Even if the interrupt bit
3216   // is true, if the clear_int argument is false, we avoid the VM call.
3217   // However, if the receiver is not currentThread, we must call the VM,
3218   // because there must be some locking done around the operation.
3219 
3220   // We only go to the fast case code if we pass two guards.
3221   // Paths which do not pass are accumulated in the slow_region.
3222 
3223   enum {
3224     no_int_result_path   = 1, // t == Thread.current() &amp;&amp; !TLS._osthread._interrupted
3225     no_clear_result_path = 2, // t == Thread.current() &amp;&amp;  TLS._osthread._interrupted &amp;&amp; !clear_int
3226     slow_result_path     = 3, // slow path: t.isInterrupted(clear_int)
3227     PATH_LIMIT
3228   };
3229 
3230   // Ensure that it's not possible to move the load of TLS._osthread._interrupted flag
3231   // out of the function.
3232   insert_mem_bar(Op_MemBarCPUOrder);
3233 
3234   RegionNode* result_rgn = new RegionNode(PATH_LIMIT);
3235   PhiNode*    result_val = new PhiNode(result_rgn, TypeInt::BOOL);
3236 
3237   RegionNode* slow_region = new RegionNode(1);
3238   record_for_igvn(slow_region);
3239 
3240   // (a) Receiving thread must be the current thread.
3241   Node* rec_thr = argument(0);
3242   Node* tls_ptr = NULL;
3243   Node* cur_thr = generate_current_thread(tls_ptr);
3244   Node* cmp_thr = _gvn.transform(new CmpPNode(cur_thr, rec_thr));
3245   Node* bol_thr = _gvn.transform(new BoolNode(cmp_thr, BoolTest::ne));
3246 
3247   generate_slow_guard(bol_thr, slow_region);
3248 
3249   // (b) Interrupt bit on TLS must be false.
3250   Node* p = basic_plus_adr(top()/*!oop*/, tls_ptr, in_bytes(JavaThread::osthread_offset()));
3251   Node* osthread = make_load(NULL, p, TypeRawPtr::NOTNULL, T_ADDRESS, MemNode::unordered);
3252   p = basic_plus_adr(top()/*!oop*/, osthread, in_bytes(OSThread::interrupted_offset()));
3253 
3254   // Set the control input on the field _interrupted read to prevent it floating up.
3255   Node* int_bit = make_load(control(), p, TypeInt::BOOL, T_INT, MemNode::unordered);
3256   Node* cmp_bit = _gvn.transform(new CmpINode(int_bit, intcon(0)));
3257   Node* bol_bit = _gvn.transform(new BoolNode(cmp_bit, BoolTest::ne));
3258 
3259   IfNode* iff_bit = create_and_map_if(control(), bol_bit, PROB_UNLIKELY_MAG(3), COUNT_UNKNOWN);
3260 
3261   // First fast path:  if (!TLS._interrupted) return false;
3262   Node* false_bit = _gvn.transform(new IfFalseNode(iff_bit));
3263   result_rgn-&gt;init_req(no_int_result_path, false_bit);
3264   result_val-&gt;init_req(no_int_result_path, intcon(0));
3265 
3266   // drop through to next case
3267   set_control( _gvn.transform(new IfTrueNode(iff_bit)));
3268 
3269 #ifndef TARGET_OS_FAMILY_windows
3270   // (c) Or, if interrupt bit is set and clear_int is false, use 2nd fast path.
3271   Node* clr_arg = argument(1);
3272   Node* cmp_arg = _gvn.transform(new CmpINode(clr_arg, intcon(0)));
3273   Node* bol_arg = _gvn.transform(new BoolNode(cmp_arg, BoolTest::ne));
3274   IfNode* iff_arg = create_and_map_if(control(), bol_arg, PROB_FAIR, COUNT_UNKNOWN);
3275 
3276   // Second fast path:  ... else if (!clear_int) return true;
3277   Node* false_arg = _gvn.transform(new IfFalseNode(iff_arg));
3278   result_rgn-&gt;init_req(no_clear_result_path, false_arg);
3279   result_val-&gt;init_req(no_clear_result_path, intcon(1));
3280 
3281   // drop through to next case
3282   set_control( _gvn.transform(new IfTrueNode(iff_arg)));
3283 #else
3284   // To return true on Windows you must read the _interrupted field
3285   // and check the event state i.e. take the slow path.
3286 #endif // TARGET_OS_FAMILY_windows
3287 
3288   // (d) Otherwise, go to the slow path.
3289   slow_region-&gt;add_req(control());
3290   set_control( _gvn.transform(slow_region));
3291 
3292   if (stopped()) {
3293     // There is no slow path.
3294     result_rgn-&gt;init_req(slow_result_path, top());
3295     result_val-&gt;init_req(slow_result_path, top());
3296   } else {
3297     // non-virtual because it is a private non-static
3298     CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_isInterrupted);
3299 
3300     Node* slow_val = set_results_for_java_call(slow_call);
3301     // this-&gt;control() comes from set_results_for_java_call
3302 
3303     Node* fast_io  = slow_call-&gt;in(TypeFunc::I_O);
3304     Node* fast_mem = slow_call-&gt;in(TypeFunc::Memory);
3305 
3306     // These two phis are pre-filled with copies of of the fast IO and Memory
3307     PhiNode* result_mem  = PhiNode::make(result_rgn, fast_mem, Type::MEMORY, TypePtr::BOTTOM);
3308     PhiNode* result_io   = PhiNode::make(result_rgn, fast_io,  Type::ABIO);
3309 
3310     result_rgn-&gt;init_req(slow_result_path, control());
3311     result_io -&gt;init_req(slow_result_path, i_o());
3312     result_mem-&gt;init_req(slow_result_path, reset_memory());
3313     result_val-&gt;init_req(slow_result_path, slow_val);
3314 
3315     set_all_memory(_gvn.transform(result_mem));
3316     set_i_o(       _gvn.transform(result_io));
3317   }
3318 
3319   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3320   set_result(result_rgn, result_val);
3321   return true;
3322 }
3323 
3324 //---------------------------load_mirror_from_klass----------------------------
3325 // Given a klass oop, load its java mirror (a java.lang.Class oop).
3326 Node* LibraryCallKit::load_mirror_from_klass(Node* klass) {
3327   Node* p = basic_plus_adr(klass, in_bytes(Klass::java_mirror_offset()));
3328   return make_load(NULL, p, TypeInstPtr::MIRROR, T_OBJECT, MemNode::unordered);
3329 }
3330 
3331 //-----------------------load_klass_from_mirror_common-------------------------
3332 // Given a java mirror (a java.lang.Class oop), load its corresponding klass oop.
3333 // Test the klass oop for null (signifying a primitive Class like Integer.TYPE),
3334 // and branch to the given path on the region.
3335 // If never_see_null, take an uncommon trap on null, so we can optimistically
3336 // compile for the non-null case.
3337 // If the region is NULL, force never_see_null = true.
3338 Node* LibraryCallKit::load_klass_from_mirror_common(Node* mirror,
3339                                                     bool never_see_null,
3340                                                     RegionNode* region,
3341                                                     int null_path,
3342                                                     int offset) {
3343   if (region == NULL)  never_see_null = true;
3344   Node* p = basic_plus_adr(mirror, offset);
3345   const TypeKlassPtr*  kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3346   Node* kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, kls_type));
3347   Node* null_ctl = top();
3348   kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3349   if (region != NULL) {
3350     // Set region-&gt;in(null_path) if the mirror is a primitive (e.g, int.class).
3351     region-&gt;init_req(null_path, null_ctl);
3352   } else {
3353     assert(null_ctl == top(), "no loose ends");
3354   }
3355   return kls;
3356 }
3357 
3358 //--------------------(inline_native_Class_query helpers)---------------------
3359 // Use this for JVM_ACC_INTERFACE, JVM_ACC_IS_CLONEABLE_FAST, JVM_ACC_HAS_FINALIZER.
3360 // Fall through if (mods &amp; mask) == bits, take the guard otherwise.
3361 Node* LibraryCallKit::generate_access_flags_guard(Node* kls, int modifier_mask, int modifier_bits, RegionNode* region) {
3362   // Branch around if the given klass has the given modifier bit set.
3363   // Like generate_guard, adds a new path onto the region.
3364   Node* modp = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3365   Node* mods = make_load(NULL, modp, TypeInt::INT, T_INT, MemNode::unordered);
3366   Node* mask = intcon(modifier_mask);
3367   Node* bits = intcon(modifier_bits);
3368   Node* mbit = _gvn.transform(new AndINode(mods, mask));
3369   Node* cmp  = _gvn.transform(new CmpINode(mbit, bits));
3370   Node* bol  = _gvn.transform(new BoolNode(cmp, BoolTest::ne));
3371   return generate_fair_guard(bol, region);
3372 }
3373 Node* LibraryCallKit::generate_interface_guard(Node* kls, RegionNode* region) {
3374   return generate_access_flags_guard(kls, JVM_ACC_INTERFACE, 0, region);
3375 }
3376 
3377 //-------------------------inline_native_Class_query-------------------
3378 bool LibraryCallKit::inline_native_Class_query(vmIntrinsics::ID id) {
3379   const Type* return_type = TypeInt::BOOL;
3380   Node* prim_return_value = top();  // what happens if it's a primitive class?
3381   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3382   bool expect_prim = false;     // most of these guys expect to work on refs
3383 
3384   enum { _normal_path = 1, _prim_path = 2, PATH_LIMIT };
3385 
3386   Node* mirror = argument(0);
3387   Node* obj    = top();
3388 
3389   switch (id) {
3390   case vmIntrinsics::_isInstance:
3391     // nothing is an instance of a primitive type
3392     prim_return_value = intcon(0);
3393     obj = argument(1);
3394     break;
3395   case vmIntrinsics::_getModifiers:
3396     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3397     assert(is_power_of_2((int)JVM_ACC_WRITTEN_FLAGS+1), "change next line");
3398     return_type = TypeInt::make(0, JVM_ACC_WRITTEN_FLAGS, Type::WidenMin);
3399     break;
3400   case vmIntrinsics::_isInterface:
3401     prim_return_value = intcon(0);
3402     break;
3403   case vmIntrinsics::_isArray:
3404     prim_return_value = intcon(0);
3405     expect_prim = true;  // cf. ObjectStreamClass.getClassSignature
3406     break;
3407   case vmIntrinsics::_isPrimitive:
3408     prim_return_value = intcon(1);
3409     expect_prim = true;  // obviously
3410     break;
3411   case vmIntrinsics::_getSuperclass:
3412     prim_return_value = null();
3413     return_type = TypeInstPtr::MIRROR-&gt;cast_to_ptr_type(TypePtr::BotPTR);
3414     break;
3415   case vmIntrinsics::_getClassAccessFlags:
3416     prim_return_value = intcon(JVM_ACC_ABSTRACT | JVM_ACC_FINAL | JVM_ACC_PUBLIC);
3417     return_type = TypeInt::INT;  // not bool!  6297094
3418     break;
3419   default:
3420     fatal_unexpected_iid(id);
3421     break;
3422   }
3423 
3424   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3425   if (mirror_con == NULL)  return false;  // cannot happen?
3426 
3427 #ifndef PRODUCT
3428   if (C-&gt;print_intrinsics() || C-&gt;print_inlining()) {
3429     ciType* k = mirror_con-&gt;java_mirror_type();
3430     if (k) {
3431       tty-&gt;print("Inlining %s on constant Class ", vmIntrinsics::name_at(intrinsic_id()));
3432       k-&gt;print_name();
3433       tty-&gt;cr();
3434     }
3435   }
3436 #endif
3437 
3438   // Null-check the mirror, and the mirror's klass ptr (in case it is a primitive).
3439   RegionNode* region = new RegionNode(PATH_LIMIT);
3440   record_for_igvn(region);
3441   PhiNode* phi = new PhiNode(region, return_type);
3442 
3443   // The mirror will never be null of Reflection.getClassAccessFlags, however
3444   // it may be null for Class.isInstance or Class.getModifiers. Throw a NPE
3445   // if it is. See bug 4774291.
3446 
3447   // For Reflection.getClassAccessFlags(), the null check occurs in
3448   // the wrong place; see inline_unsafe_access(), above, for a similar
3449   // situation.
3450   mirror = null_check(mirror);
3451   // If mirror or obj is dead, only null-path is taken.
3452   if (stopped())  return true;
3453 
3454   if (expect_prim)  never_see_null = false;  // expect nulls (meaning prims)
3455 
3456   // Now load the mirror's klass metaobject, and null-check it.
3457   // Side-effects region with the control path if the klass is null.
3458   Node* kls = load_klass_from_mirror(mirror, never_see_null, region, _prim_path);
3459   // If kls is null, we have a primitive mirror.
3460   phi-&gt;init_req(_prim_path, prim_return_value);
3461   if (stopped()) { set_result(region, phi); return true; }
3462   bool safe_for_replace = (region-&gt;in(_prim_path) == top());
3463 
3464   Node* p;  // handy temp
3465   Node* null_ctl;
3466 
3467   // Now that we have the non-null klass, we can perform the real query.
3468   // For constant classes, the query will constant-fold in LoadNode::Value.
3469   Node* query_value = top();
3470   switch (id) {
3471   case vmIntrinsics::_isInstance:
3472     // nothing is an instance of a primitive type
3473     query_value = gen_instanceof(obj, kls, safe_for_replace);
3474     break;
3475 
3476   case vmIntrinsics::_getModifiers:
3477     p = basic_plus_adr(kls, in_bytes(Klass::modifier_flags_offset()));
3478     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3479     break;
3480 
3481   case vmIntrinsics::_isInterface:
3482     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3483     if (generate_interface_guard(kls, region) != NULL)
3484       // A guard was added.  If the guard is taken, it was an interface.
3485       phi-&gt;add_req(intcon(1));
3486     // If we fall through, it's a plain class.
3487     query_value = intcon(0);
3488     break;
3489 
3490   case vmIntrinsics::_isArray:
3491     // (To verify this code sequence, check the asserts in JVM_IsArrayClass.)
3492     if (generate_array_guard(kls, region) != NULL)
3493       // A guard was added.  If the guard is taken, it was an array.
3494       phi-&gt;add_req(intcon(1));
3495     // If we fall through, it's a plain class.
3496     query_value = intcon(0);
3497     break;
3498 
3499   case vmIntrinsics::_isPrimitive:
3500     query_value = intcon(0); // "normal" path produces false
3501     break;
3502 
3503   case vmIntrinsics::_getSuperclass:
3504     // The rules here are somewhat unfortunate, but we can still do better
3505     // with random logic than with a JNI call.
3506     // Interfaces store null or Object as _super, but must report null.
3507     // Arrays store an intermediate super as _super, but must report Object.
3508     // Other types can report the actual _super.
3509     // (To verify this code sequence, check the asserts in JVM_IsInterface.)
3510     if (generate_interface_guard(kls, region) != NULL)
3511       // A guard was added.  If the guard is taken, it was an interface.
3512       phi-&gt;add_req(null());
3513     if (generate_array_guard(kls, region) != NULL)
3514       // A guard was added.  If the guard is taken, it was an array.
3515       phi-&gt;add_req(makecon(TypeInstPtr::make(env()-&gt;Object_klass()-&gt;java_mirror())));
3516     // If we fall through, it's a plain class.  Get its _super.
3517     p = basic_plus_adr(kls, in_bytes(Klass::super_offset()));
3518     kls = _gvn.transform(LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, TypeRawPtr::BOTTOM, TypeKlassPtr::OBJECT_OR_NULL));
3519     null_ctl = top();
3520     kls = null_check_oop(kls, &amp;null_ctl);
3521     if (null_ctl != top()) {
3522       // If the guard is taken, Object.superClass is null (both klass and mirror).
3523       region-&gt;add_req(null_ctl);
3524       phi   -&gt;add_req(null());
3525     }
3526     if (!stopped()) {
3527       query_value = load_mirror_from_klass(kls);
3528     }
3529     break;
3530 
3531   case vmIntrinsics::_getClassAccessFlags:
3532     p = basic_plus_adr(kls, in_bytes(Klass::access_flags_offset()));
3533     query_value = make_load(NULL, p, TypeInt::INT, T_INT, MemNode::unordered);
3534     break;
3535 
3536   default:
3537     fatal_unexpected_iid(id);
3538     break;
3539   }
3540 
3541   // Fall-through is the normal case of a query to a real class.
3542   phi-&gt;init_req(1, query_value);
3543   region-&gt;init_req(1, control());
3544 
3545   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3546   set_result(region, phi);
3547   return true;
3548 }
3549 
3550 //-------------------------inline_Class_cast-------------------
3551 bool LibraryCallKit::inline_Class_cast() {
3552   Node* mirror = argument(0); // Class
3553   Node* obj    = argument(1);
3554   const TypeInstPtr* mirror_con = _gvn.type(mirror)-&gt;isa_instptr();
3555   if (mirror_con == NULL) {
3556     return false;  // dead path (mirror-&gt;is_top()).
3557   }
3558   if (obj == NULL || obj-&gt;is_top()) {
3559     return false;  // dead path
3560   }
3561   const TypeOopPtr* tp = _gvn.type(obj)-&gt;isa_oopptr();
3562 
3563   // First, see if Class.cast() can be folded statically.
3564   // java_mirror_type() returns non-null for compile-time Class constants.
3565   ciType* tm = mirror_con-&gt;java_mirror_type();
3566   if (tm != NULL &amp;&amp; tm-&gt;is_klass() &amp;&amp;
3567       tp != NULL &amp;&amp; tp-&gt;klass() != NULL) {
3568     if (!tp-&gt;klass()-&gt;is_loaded()) {
3569       // Don't use intrinsic when class is not loaded.
3570       return false;
3571     } else {
3572       int static_res = C-&gt;static_subtype_check(tm-&gt;as_klass(), tp-&gt;klass());
3573       if (static_res == Compile::SSC_always_true) {
3574         // isInstance() is true - fold the code.
3575         set_result(obj);
3576         return true;
3577       } else if (static_res == Compile::SSC_always_false) {
3578         // Don't use intrinsic, have to throw ClassCastException.
3579         // If the reference is null, the non-intrinsic bytecode will
3580         // be optimized appropriately.
3581         return false;
3582       }
3583     }
3584   }
3585 
3586   // Bailout intrinsic and do normal inlining if exception path is frequent.
3587   if (too_many_traps(Deoptimization::Reason_intrinsic)) {
3588     return false;
3589   }
3590 
3591   // Generate dynamic checks.
3592   // Class.cast() is java implementation of _checkcast bytecode.
3593   // Do checkcast (Parse::do_checkcast()) optimizations here.
3594 
3595   mirror = null_check(mirror);
3596   // If mirror is dead, only null-path is taken.
3597   if (stopped()) {
3598     return true;
3599   }
3600 
3601   // Not-subtype or the mirror's klass ptr is NULL (in case it is a primitive).
3602   enum { _bad_type_path = 1, _prim_path = 2, PATH_LIMIT };
3603   RegionNode* region = new RegionNode(PATH_LIMIT);
3604   record_for_igvn(region);
3605 
3606   // Now load the mirror's klass metaobject, and null-check it.
3607   // If kls is null, we have a primitive mirror and
3608   // nothing is an instance of a primitive type.
3609   Node* kls = load_klass_from_mirror(mirror, false, region, _prim_path);
3610 
3611   Node* res = top();
3612   if (!stopped()) {
3613     Node* bad_type_ctrl = top();
3614     // Do checkcast optimizations.
3615     res = gen_checkcast(obj, kls, &amp;bad_type_ctrl);
3616     region-&gt;init_req(_bad_type_path, bad_type_ctrl);
3617   }
3618   if (region-&gt;in(_prim_path) != top() ||
3619       region-&gt;in(_bad_type_path) != top()) {
3620     // Let Interpreter throw ClassCastException.
3621     PreserveJVMState pjvms(this);
3622     set_control(_gvn.transform(region));
3623     uncommon_trap(Deoptimization::Reason_intrinsic,
3624                   Deoptimization::Action_maybe_recompile);
3625   }
3626   if (!stopped()) {
3627     set_result(res);
3628   }
3629   return true;
3630 }
3631 
3632 
3633 //--------------------------inline_native_subtype_check------------------------
3634 // This intrinsic takes the JNI calls out of the heart of
3635 // UnsafeFieldAccessorImpl.set, which improves Field.set, readObject, etc.
3636 bool LibraryCallKit::inline_native_subtype_check() {
3637   // Pull both arguments off the stack.
3638   Node* args[2];                // two java.lang.Class mirrors: superc, subc
3639   args[0] = argument(0);
3640   args[1] = argument(1);
3641   Node* klasses[2];             // corresponding Klasses: superk, subk
3642   klasses[0] = klasses[1] = top();
3643 
3644   enum {
3645     // A full decision tree on {superc is prim, subc is prim}:
3646     _prim_0_path = 1,           // {P,N} =&gt; false
3647                                 // {P,P} &amp; superc!=subc =&gt; false
3648     _prim_same_path,            // {P,P} &amp; superc==subc =&gt; true
3649     _prim_1_path,               // {N,P} =&gt; false
3650     _ref_subtype_path,          // {N,N} &amp; subtype check wins =&gt; true
3651     _both_ref_path,             // {N,N} &amp; subtype check loses =&gt; false
3652     PATH_LIMIT
3653   };
3654 
3655   RegionNode* region = new RegionNode(PATH_LIMIT);
3656   Node*       phi    = new PhiNode(region, TypeInt::BOOL);
3657   record_for_igvn(region);
3658 
3659   const TypePtr* adr_type = TypeRawPtr::BOTTOM;   // memory type of loads
3660   const TypeKlassPtr* kls_type = TypeKlassPtr::OBJECT_OR_NULL;
3661   int class_klass_offset = java_lang_Class::klass_offset_in_bytes();
3662 
3663   // First null-check both mirrors and load each mirror's klass metaobject.
3664   int which_arg;
3665   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3666     Node* arg = args[which_arg];
3667     arg = null_check(arg);
3668     if (stopped())  break;
3669     args[which_arg] = arg;
3670 
3671     Node* p = basic_plus_adr(arg, class_klass_offset);
3672     Node* kls = LoadKlassNode::make(_gvn, NULL, immutable_memory(), p, adr_type, kls_type);
3673     klasses[which_arg] = _gvn.transform(kls);
3674   }
3675 
3676   // Having loaded both klasses, test each for null.
3677   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3678   for (which_arg = 0; which_arg &lt;= 1; which_arg++) {
3679     Node* kls = klasses[which_arg];
3680     Node* null_ctl = top();
3681     kls = null_check_oop(kls, &amp;null_ctl, never_see_null);
3682     int prim_path = (which_arg == 0 ? _prim_0_path : _prim_1_path);
3683     region-&gt;init_req(prim_path, null_ctl);
3684     if (stopped())  break;
3685     klasses[which_arg] = kls;
3686   }
3687 
3688   if (!stopped()) {
3689     // now we have two reference types, in klasses[0..1]
3690     Node* subk   = klasses[1];  // the argument to isAssignableFrom
3691     Node* superk = klasses[0];  // the receiver
3692     region-&gt;set_req(_both_ref_path, gen_subtype_check(subk, superk));
3693     // now we have a successful reference subtype check
3694     region-&gt;set_req(_ref_subtype_path, control());
3695   }
3696 
3697   // If both operands are primitive (both klasses null), then
3698   // we must return true when they are identical primitives.
3699   // It is convenient to test this after the first null klass check.
3700   set_control(region-&gt;in(_prim_0_path)); // go back to first null check
3701   if (!stopped()) {
3702     // Since superc is primitive, make a guard for the superc==subc case.
3703     Node* cmp_eq = _gvn.transform(new CmpPNode(args[0], args[1]));
3704     Node* bol_eq = _gvn.transform(new BoolNode(cmp_eq, BoolTest::eq));
3705     generate_guard(bol_eq, region, PROB_FAIR);
3706     if (region-&gt;req() == PATH_LIMIT+1) {
3707       // A guard was added.  If the added guard is taken, superc==subc.
3708       region-&gt;swap_edges(PATH_LIMIT, _prim_same_path);
3709       region-&gt;del_req(PATH_LIMIT);
3710     }
3711     region-&gt;set_req(_prim_0_path, control()); // Not equal after all.
3712   }
3713 
3714   // these are the only paths that produce 'true':
3715   phi-&gt;set_req(_prim_same_path,   intcon(1));
3716   phi-&gt;set_req(_ref_subtype_path, intcon(1));
3717 
3718   // pull together the cases:
3719   assert(region-&gt;req() == PATH_LIMIT, "sane region");
3720   for (uint i = 1; i &lt; region-&gt;req(); i++) {
3721     Node* ctl = region-&gt;in(i);
3722     if (ctl == NULL || ctl == top()) {
3723       region-&gt;set_req(i, top());
3724       phi   -&gt;set_req(i, top());
3725     } else if (phi-&gt;in(i) == NULL) {
3726       phi-&gt;set_req(i, intcon(0)); // all other paths produce 'false'
3727     }
3728   }
3729 
3730   set_control(_gvn.transform(region));
3731   set_result(_gvn.transform(phi));
3732   return true;
3733 }
3734 
3735 //---------------------generate_array_guard_common------------------------
3736 Node* LibraryCallKit::generate_array_guard_common(Node* kls, RegionNode* region,
3737                                                   bool obj_array, bool not_array) {
3738 
3739   if (stopped()) {
3740     return NULL;
3741   }
3742 
3743   // If obj_array/non_array==false/false:
3744   // Branch around if the given klass is in fact an array (either obj or prim).
3745   // If obj_array/non_array==false/true:
3746   // Branch around if the given klass is not an array klass of any kind.
3747   // If obj_array/non_array==true/true:
3748   // Branch around if the kls is not an oop array (kls is int[], String, etc.)
3749   // If obj_array/non_array==true/false:
3750   // Branch around if the kls is an oop array (Object[] or subtype)
3751   //
3752   // Like generate_guard, adds a new path onto the region.
3753   jint  layout_con = 0;
3754   Node* layout_val = get_layout_helper(kls, layout_con);
3755   if (layout_val == NULL) {
3756     bool query = (obj_array
3757                   ? Klass::layout_helper_is_objArray(layout_con)
3758                   : Klass::layout_helper_is_array(layout_con));
3759     if (query == not_array) {
3760       return NULL;                       // never a branch
3761     } else {                             // always a branch
3762       Node* always_branch = control();
3763       if (region != NULL)
3764         region-&gt;add_req(always_branch);
3765       set_control(top());
3766       return always_branch;
3767     }
3768   }
3769   // Now test the correct condition.
3770   jint  nval = (obj_array
3771                 ? (jint)(Klass::_lh_array_tag_type_value
3772                    &lt;&lt;    Klass::_lh_array_tag_shift)
3773                 : Klass::_lh_neutral_value);
3774   Node* cmp = _gvn.transform(new CmpINode(layout_val, intcon(nval)));
3775   BoolTest::mask btest = BoolTest::lt;  // correct for testing is_[obj]array
3776   // invert the test if we are looking for a non-array
3777   if (not_array)  btest = BoolTest(btest).negate();
3778   Node* bol = _gvn.transform(new BoolNode(cmp, btest));
3779   return generate_fair_guard(bol, region);
3780 }
3781 
3782 
3783 //-----------------------inline_native_newArray--------------------------
3784 // private static native Object java.lang.reflect.newArray(Class&lt;?&gt; componentType, int length);
3785 bool LibraryCallKit::inline_native_newArray() {
3786   Node* mirror    = argument(0);
3787   Node* count_val = argument(1);
3788 
3789   mirror = null_check(mirror);
3790   // If mirror or obj is dead, only null-path is taken.
3791   if (stopped())  return true;
3792 
3793   enum { _normal_path = 1, _slow_path = 2, PATH_LIMIT };
3794   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
3795   PhiNode*    result_val = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
3796   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
3797   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
3798 
3799   bool never_see_null = !too_many_traps(Deoptimization::Reason_null_check);
3800   Node* klass_node = load_array_klass_from_mirror(mirror, never_see_null,
3801                                                   result_reg, _slow_path);
3802   Node* normal_ctl   = control();
3803   Node* no_array_ctl = result_reg-&gt;in(_slow_path);
3804 
3805   // Generate code for the slow case.  We make a call to newArray().
3806   set_control(no_array_ctl);
3807   if (!stopped()) {
3808     // Either the input type is void.class, or else the
3809     // array klass has not yet been cached.  Either the
3810     // ensuing call will throw an exception, or else it
3811     // will cache the array klass for next time.
3812     PreserveJVMState pjvms(this);
3813     CallJavaNode* slow_call = generate_method_call_static(vmIntrinsics::_newArray);
3814     Node* slow_result = set_results_for_java_call(slow_call);
3815     // this-&gt;control() comes from set_results_for_java_call
3816     result_reg-&gt;set_req(_slow_path, control());
3817     result_val-&gt;set_req(_slow_path, slow_result);
3818     result_io -&gt;set_req(_slow_path, i_o());
3819     result_mem-&gt;set_req(_slow_path, reset_memory());
3820   }
3821 
3822   set_control(normal_ctl);
3823   if (!stopped()) {
3824     // Normal case:  The array type has been cached in the java.lang.Class.
3825     // The following call works fine even if the array type is polymorphic.
3826     // It could be a dynamic mix of int[], boolean[], Object[], etc.
3827     Node* obj = new_array(klass_node, count_val, 0);  // no arguments to push
3828     result_reg-&gt;init_req(_normal_path, control());
3829     result_val-&gt;init_req(_normal_path, obj);
3830     result_io -&gt;init_req(_normal_path, i_o());
3831     result_mem-&gt;init_req(_normal_path, reset_memory());
3832   }
3833 
3834   // Return the combined state.
3835   set_i_o(        _gvn.transform(result_io)  );
3836   set_all_memory( _gvn.transform(result_mem));
3837 
3838   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
3839   set_result(result_reg, result_val);
3840   return true;
3841 }
3842 
3843 //----------------------inline_native_getLength--------------------------
3844 // public static native int java.lang.reflect.Array.getLength(Object array);
3845 bool LibraryCallKit::inline_native_getLength() {
3846   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3847 
3848   Node* array = null_check(argument(0));
3849   // If array is dead, only null-path is taken.
3850   if (stopped())  return true;
3851 
3852   // Deoptimize if it is a non-array.
3853   Node* non_array = generate_non_array_guard(load_object_klass(array), NULL);
3854 
3855   if (non_array != NULL) {
3856     PreserveJVMState pjvms(this);
3857     set_control(non_array);
3858     uncommon_trap(Deoptimization::Reason_intrinsic,
3859                   Deoptimization::Action_maybe_recompile);
3860   }
3861 
3862   // If control is dead, only non-array-path is taken.
3863   if (stopped())  return true;
3864 
3865   // The works fine even if the array type is polymorphic.
3866   // It could be a dynamic mix of int[], boolean[], Object[], etc.
3867   Node* result = load_array_length(array);
3868 
3869   C-&gt;set_has_split_ifs(true);  // Has chance for split-if optimization
3870   set_result(result);
3871   return true;
3872 }
3873 
3874 //------------------------inline_array_copyOf----------------------------
3875 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOf(     U[] original, int newLength,         Class&lt;? extends T[]&gt; newType);
3876 // public static &lt;T,U&gt; T[] java.util.Arrays.copyOfRange(U[] original, int from,      int to, Class&lt;? extends T[]&gt; newType);
3877 bool LibraryCallKit::inline_array_copyOf(bool is_copyOfRange) {
3878   if (too_many_traps(Deoptimization::Reason_intrinsic))  return false;
3879 
3880   // Get the arguments.
3881   Node* original          = argument(0);
3882   Node* start             = is_copyOfRange? argument(1): intcon(0);
3883   Node* end               = is_copyOfRange? argument(2): argument(1);
3884   Node* array_type_mirror = is_copyOfRange? argument(3): argument(2);
3885 
3886   Node* newcopy = NULL;
3887 
3888   // Set the original stack and the reexecute bit for the interpreter to reexecute
3889   // the bytecode that invokes Arrays.copyOf if deoptimization happens.
3890   { PreserveReexecuteState preexecs(this);
3891     jvms()-&gt;set_should_reexecute(true);
3892 
3893     array_type_mirror = null_check(array_type_mirror);
3894     original          = null_check(original);
3895 
3896     // Check if a null path was taken unconditionally.
3897     if (stopped())  return true;
3898 
3899     Node* orig_length = load_array_length(original);
3900 
3901     Node* klass_node = load_klass_from_mirror(array_type_mirror, false, NULL, 0);
3902     klass_node = null_check(klass_node);
3903 
3904     RegionNode* bailout = new RegionNode(1);
3905     record_for_igvn(bailout);
3906 
3907     // Despite the generic type of Arrays.copyOf, the mirror might be int, int[], etc.
3908     // Bail out if that is so.
3909     Node* not_objArray = generate_non_objArray_guard(klass_node, bailout);
3910     if (not_objArray != NULL) {
3911       // Improve the klass node's type from the new optimistic assumption:
3912       ciKlass* ak = ciArrayKlass::make(env()-&gt;Object_klass());
3913       const Type* akls = TypeKlassPtr::make(TypePtr::NotNull, ak, 0/*offset*/);
3914       Node* cast = new CastPPNode(klass_node, akls);
3915       cast-&gt;init_req(0, control());
3916       klass_node = _gvn.transform(cast);
3917     }
3918 
3919     // Bail out if either start or end is negative.
3920     generate_negative_guard(start, bailout, &amp;start);
3921     generate_negative_guard(end,   bailout, &amp;end);
3922 
3923     Node* length = end;
3924     if (_gvn.type(start) != TypeInt::ZERO) {
3925       length = _gvn.transform(new SubINode(end, start));
3926     }
3927 
3928     // Bail out if length is negative.
3929     // Without this the new_array would throw
3930     // NegativeArraySizeException but IllegalArgumentException is what
3931     // should be thrown
3932     generate_negative_guard(length, bailout, &amp;length);
3933 
3934     if (bailout-&gt;req() &gt; 1) {
3935       PreserveJVMState pjvms(this);
3936       set_control(_gvn.transform(bailout));
3937       uncommon_trap(Deoptimization::Reason_intrinsic,
3938                     Deoptimization::Action_maybe_recompile);
3939     }
3940 
3941     if (!stopped()) {
3942       // How many elements will we copy from the original?
3943       // The answer is MinI(orig_length - start, length).
3944       Node* orig_tail = _gvn.transform(new SubINode(orig_length, start));
3945       Node* moved = generate_min_max(vmIntrinsics::_min, orig_tail, length);
3946 
3947       // Generate a direct call to the right arraycopy function(s).
3948       // We know the copy is disjoint but we might not know if the
3949       // oop stores need checking.
3950       // Extreme case:  Arrays.copyOf((Integer[])x, 10, String[].class).
3951       // This will fail a store-check if x contains any non-nulls.
3952 
3953       // ArrayCopyNode:Ideal may transform the ArrayCopyNode to
3954       // loads/stores but it is legal only if we're sure the
3955       // Arrays.copyOf would succeed. So we need all input arguments
3956       // to the copyOf to be validated, including that the copy to the
3957       // new array won't trigger an ArrayStoreException. That subtype
3958       // check can be optimized if we know something on the type of
3959       // the input array from type speculation.
3960       if (_gvn.type(klass_node)-&gt;singleton()) {
3961         ciKlass* subk   = _gvn.type(load_object_klass(original))-&gt;is_klassptr()-&gt;klass();
3962         ciKlass* superk = _gvn.type(klass_node)-&gt;is_klassptr()-&gt;klass();
3963 
3964         int test = C-&gt;static_subtype_check(superk, subk);
3965         if (test != Compile::SSC_always_true &amp;&amp; test != Compile::SSC_always_false) {
3966           const TypeOopPtr* t_original = _gvn.type(original)-&gt;is_oopptr();
3967           if (t_original-&gt;speculative_type() != NULL) {
3968             original = maybe_cast_profiled_obj(original, t_original-&gt;speculative_type(), true);
3969           }
3970         }
3971       }
3972 
3973       bool validated = false;
3974       // Reason_class_check rather than Reason_intrinsic because we
3975       // want to intrinsify even if this traps.
3976       if (!too_many_traps(Deoptimization::Reason_class_check)) {
3977         Node* not_subtype_ctrl = gen_subtype_check(load_object_klass(original),
3978                                                    klass_node);
3979 
3980         if (not_subtype_ctrl != top()) {
3981           PreserveJVMState pjvms(this);
3982           set_control(not_subtype_ctrl);
3983           uncommon_trap(Deoptimization::Reason_class_check,
3984                         Deoptimization::Action_make_not_entrant);
3985           assert(stopped(), "Should be stopped");
3986         }
3987         validated = true;
3988       }
3989 
3990       if (!stopped()) {
3991         newcopy = new_array(klass_node, length, 0);  // no arguments to push
3992 
3993         ArrayCopyNode* ac = ArrayCopyNode::make(this, true, original, start, newcopy, intcon(0), moved, true,
3994                                                 load_object_klass(original), klass_node);
3995         if (!is_copyOfRange) {
3996           ac-&gt;set_copyof(validated);
3997         } else {
3998           ac-&gt;set_copyofrange(validated);
3999         }
4000         Node* n = _gvn.transform(ac);
4001         if (n == ac) {
4002           ac-&gt;connect_outputs(this);
4003         } else {
4004           assert(validated, "shouldn't transform if all arguments not validated");
4005           set_all_memory(n);
4006         }
4007       }
4008     }
4009   } // original reexecute is set back here
4010 
4011   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4012   if (!stopped()) {
4013     set_result(newcopy);
4014   }
4015   return true;
4016 }
4017 
4018 
4019 //----------------------generate_virtual_guard---------------------------
4020 // Helper for hashCode and clone.  Peeks inside the vtable to avoid a call.
4021 Node* LibraryCallKit::generate_virtual_guard(Node* obj_klass,
4022                                              RegionNode* slow_region) {
4023   ciMethod* method = callee();
4024   int vtable_index = method-&gt;vtable_index();
4025   assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4026          "bad index %d", vtable_index);
4027   // Get the Method* out of the appropriate vtable entry.
4028   int entry_offset  = in_bytes(Klass::vtable_start_offset()) +
4029                      vtable_index*vtableEntry::size_in_bytes() +
4030                      vtableEntry::method_offset_in_bytes();
4031   Node* entry_addr  = basic_plus_adr(obj_klass, entry_offset);
4032   Node* target_call = make_load(NULL, entry_addr, TypePtr::NOTNULL, T_ADDRESS, MemNode::unordered);
4033 
4034   // Compare the target method with the expected method (e.g., Object.hashCode).
4035   const TypePtr* native_call_addr = TypeMetadataPtr::make(method);
4036 
4037   Node* native_call = makecon(native_call_addr);
4038   Node* chk_native  = _gvn.transform(new CmpPNode(target_call, native_call));
4039   Node* test_native = _gvn.transform(new BoolNode(chk_native, BoolTest::ne));
4040 
4041   return generate_slow_guard(test_native, slow_region);
4042 }
4043 
4044 //-----------------------generate_method_call----------------------------
4045 // Use generate_method_call to make a slow-call to the real
4046 // method if the fast path fails.  An alternative would be to
4047 // use a stub like OptoRuntime::slow_arraycopy_Java.
4048 // This only works for expanding the current library call,
4049 // not another intrinsic.  (E.g., don't use this for making an
4050 // arraycopy call inside of the copyOf intrinsic.)
4051 CallJavaNode*
4052 LibraryCallKit::generate_method_call(vmIntrinsics::ID method_id, bool is_virtual, bool is_static) {
4053   // When compiling the intrinsic method itself, do not use this technique.
4054   guarantee(callee() != C-&gt;method(), "cannot make slow-call to self");
4055 
4056   ciMethod* method = callee();
4057   // ensure the JVMS we have will be correct for this call
4058   guarantee(method_id == method-&gt;intrinsic_id(), "must match");
4059 
4060   const TypeFunc* tf = TypeFunc::make(method);
4061   CallJavaNode* slow_call;
4062   if (is_static) {
4063     assert(!is_virtual, "");
4064     slow_call = new CallStaticJavaNode(C, tf,
4065                            SharedRuntime::get_resolve_static_call_stub(),
4066                            method, bci());
4067   } else if (is_virtual) {
4068     null_check_receiver();
4069     int vtable_index = Method::invalid_vtable_index;
4070     if (UseInlineCaches) {
4071       // Suppress the vtable call
4072     } else {
4073       // hashCode and clone are not a miranda methods,
4074       // so the vtable index is fixed.
4075       // No need to use the linkResolver to get it.
4076        vtable_index = method-&gt;vtable_index();
4077        assert(vtable_index &gt;= 0 || vtable_index == Method::nonvirtual_vtable_index,
4078               "bad index %d", vtable_index);
4079     }
4080     slow_call = new CallDynamicJavaNode(tf,
4081                           SharedRuntime::get_resolve_virtual_call_stub(),
4082                           method, vtable_index, bci());
4083   } else {  // neither virtual nor static:  opt_virtual
4084     null_check_receiver();
4085     slow_call = new CallStaticJavaNode(C, tf,
4086                                 SharedRuntime::get_resolve_opt_virtual_call_stub(),
4087                                 method, bci());
4088     slow_call-&gt;set_optimized_virtual(true);
4089   }
4090   set_arguments_for_java_call(slow_call);
4091   set_edges_for_java_call(slow_call);
4092   return slow_call;
4093 }
4094 
4095 
4096 /**
4097  * Build special case code for calls to hashCode on an object. This call may
4098  * be virtual (invokevirtual) or bound (invokespecial). For each case we generate
4099  * slightly different code.
4100  */
4101 bool LibraryCallKit::inline_native_hashcode(bool is_virtual, bool is_static) {
4102   assert(is_static == callee()-&gt;is_static(), "correct intrinsic selection");
4103   assert(!(is_virtual &amp;&amp; is_static), "either virtual, special, or static");
4104 
4105   enum { _slow_path = 1, _fast_path, _null_path, PATH_LIMIT };
4106 
4107   RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4108   PhiNode*    result_val = new PhiNode(result_reg, TypeInt::INT);
4109   PhiNode*    result_io  = new PhiNode(result_reg, Type::ABIO);
4110   PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4111   Node* obj = NULL;
4112   if (!is_static) {
4113     // Check for hashing null object
4114     obj = null_check_receiver();
4115     if (stopped())  return true;        // unconditionally null
4116     result_reg-&gt;init_req(_null_path, top());
4117     result_val-&gt;init_req(_null_path, top());
4118   } else {
4119     // Do a null check, and return zero if null.
4120     // System.identityHashCode(null) == 0
4121     obj = argument(0);
4122     Node* null_ctl = top();
4123     obj = null_check_oop(obj, &amp;null_ctl);
4124     result_reg-&gt;init_req(_null_path, null_ctl);
4125     result_val-&gt;init_req(_null_path, _gvn.intcon(0));
4126   }
4127 
4128   // Unconditionally null?  Then return right away.
4129   if (stopped()) {
4130     set_control( result_reg-&gt;in(_null_path));
4131     if (!stopped())
4132       set_result(result_val-&gt;in(_null_path));
4133     return true;
4134   }
4135 
4136   // We only go to the fast case code if we pass a number of guards.  The
4137   // paths which do not pass are accumulated in the slow_region.
4138   RegionNode* slow_region = new RegionNode(1);
4139   record_for_igvn(slow_region);
4140 
4141   // If this is a virtual call, we generate a funny guard.  We pull out
4142   // the vtable entry corresponding to hashCode() from the target object.
4143   // If the target method which we are calling happens to be the native
4144   // Object hashCode() method, we pass the guard.  We do not need this
4145   // guard for non-virtual calls -- the caller is known to be the native
4146   // Object hashCode().
4147   if (is_virtual) {
4148     // After null check, get the object's klass.
4149     Node* obj_klass = load_object_klass(obj);
4150     generate_virtual_guard(obj_klass, slow_region);
4151   }
4152 
4153   // Get the header out of the object, use LoadMarkNode when available
4154   Node* header_addr = basic_plus_adr(obj, oopDesc::mark_offset_in_bytes());
4155   // The control of the load must be NULL. Otherwise, the load can move before
4156   // the null check after castPP removal.
4157   Node* no_ctrl = NULL;
4158   Node* header = make_load(no_ctrl, header_addr, TypeX_X, TypeX_X-&gt;basic_type(), MemNode::unordered);
4159 
4160   // Test the header to see if it is unlocked.
4161   Node *lock_mask      = _gvn.MakeConX(markOopDesc::biased_lock_mask_in_place);
4162   Node *lmasked_header = _gvn.transform(new AndXNode(header, lock_mask));
4163   Node *unlocked_val   = _gvn.MakeConX(markOopDesc::unlocked_value);
4164   Node *chk_unlocked   = _gvn.transform(new CmpXNode( lmasked_header, unlocked_val));
4165   Node *test_unlocked  = _gvn.transform(new BoolNode( chk_unlocked, BoolTest::ne));
4166 
4167   generate_slow_guard(test_unlocked, slow_region);
4168 
4169   // Get the hash value and check to see that it has been properly assigned.
4170   // We depend on hash_mask being at most 32 bits and avoid the use of
4171   // hash_mask_in_place because it could be larger than 32 bits in a 64-bit
4172   // vm: see markOop.hpp.
4173   Node *hash_mask      = _gvn.intcon(markOopDesc::hash_mask);
4174   Node *hash_shift     = _gvn.intcon(markOopDesc::hash_shift);
4175   Node *hshifted_header= _gvn.transform(new URShiftXNode(header, hash_shift));
4176   // This hack lets the hash bits live anywhere in the mark object now, as long
4177   // as the shift drops the relevant bits into the low 32 bits.  Note that
4178   // Java spec says that HashCode is an int so there's no point in capturing
4179   // an 'X'-sized hashcode (32 in 32-bit build or 64 in 64-bit build).
4180   hshifted_header      = ConvX2I(hshifted_header);
4181   Node *hash_val       = _gvn.transform(new AndINode(hshifted_header, hash_mask));
4182 
4183   Node *no_hash_val    = _gvn.intcon(markOopDesc::no_hash);
4184   Node *chk_assigned   = _gvn.transform(new CmpINode( hash_val, no_hash_val));
4185   Node *test_assigned  = _gvn.transform(new BoolNode( chk_assigned, BoolTest::eq));
4186 
4187   generate_slow_guard(test_assigned, slow_region);
4188 
4189   Node* init_mem = reset_memory();
4190   // fill in the rest of the null path:
4191   result_io -&gt;init_req(_null_path, i_o());
4192   result_mem-&gt;init_req(_null_path, init_mem);
4193 
4194   result_val-&gt;init_req(_fast_path, hash_val);
4195   result_reg-&gt;init_req(_fast_path, control());
4196   result_io -&gt;init_req(_fast_path, i_o());
4197   result_mem-&gt;init_req(_fast_path, init_mem);
4198 
4199   // Generate code for the slow case.  We make a call to hashCode().
4200   set_control(_gvn.transform(slow_region));
4201   if (!stopped()) {
4202     // No need for PreserveJVMState, because we're using up the present state.
4203     set_all_memory(init_mem);
4204     vmIntrinsics::ID hashCode_id = is_static ? vmIntrinsics::_identityHashCode : vmIntrinsics::_hashCode;
4205     CallJavaNode* slow_call = generate_method_call(hashCode_id, is_virtual, is_static);
4206     Node* slow_result = set_results_for_java_call(slow_call);
4207     // this-&gt;control() comes from set_results_for_java_call
4208     result_reg-&gt;init_req(_slow_path, control());
4209     result_val-&gt;init_req(_slow_path, slow_result);
4210     result_io  -&gt;set_req(_slow_path, i_o());
4211     result_mem -&gt;set_req(_slow_path, reset_memory());
4212   }
4213 
4214   // Return the combined state.
4215   set_i_o(        _gvn.transform(result_io)  );
4216   set_all_memory( _gvn.transform(result_mem));
4217 
4218   set_result(result_reg, result_val);
4219   return true;
4220 }
4221 
4222 //---------------------------inline_native_getClass----------------------------
4223 // public final native Class&lt;?&gt; java.lang.Object.getClass();
4224 //
4225 // Build special case code for calls to getClass on an object.
4226 bool LibraryCallKit::inline_native_getClass() {
4227   Node* obj = null_check_receiver();
4228   if (stopped())  return true;
4229   set_result(load_mirror_from_klass(load_object_klass(obj)));
4230   return true;
4231 }
4232 
4233 //-----------------inline_native_Reflection_getCallerClass---------------------
4234 // public static native Class&lt;?&gt; sun.reflect.Reflection.getCallerClass();
4235 //
4236 // In the presence of deep enough inlining, getCallerClass() becomes a no-op.
4237 //
4238 // NOTE: This code must perform the same logic as JVM_GetCallerClass
4239 // in that it must skip particular security frames and checks for
4240 // caller sensitive methods.
4241 bool LibraryCallKit::inline_native_Reflection_getCallerClass() {
4242 #ifndef PRODUCT
4243   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4244     tty-&gt;print_cr("Attempting to inline sun.reflect.Reflection.getCallerClass");
4245   }
4246 #endif
4247 
4248   if (!jvms()-&gt;has_method()) {
4249 #ifndef PRODUCT
4250     if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4251       tty-&gt;print_cr("  Bailing out because intrinsic was inlined at top level");
4252     }
4253 #endif
4254     return false;
4255   }
4256 
4257   // Walk back up the JVM state to find the caller at the required
4258   // depth.
4259   JVMState* caller_jvms = jvms();
4260 
4261   // Cf. JVM_GetCallerClass
4262   // NOTE: Start the loop at depth 1 because the current JVM state does
4263   // not include the Reflection.getCallerClass() frame.
4264   for (int n = 1; caller_jvms != NULL; caller_jvms = caller_jvms-&gt;caller(), n++) {
4265     ciMethod* m = caller_jvms-&gt;method();
4266     switch (n) {
4267     case 0:
4268       fatal("current JVM state does not include the Reflection.getCallerClass frame");
4269       break;
4270     case 1:
4271       // Frame 0 and 1 must be caller sensitive (see JVM_GetCallerClass).
4272       if (!m-&gt;caller_sensitive()) {
4273 #ifndef PRODUCT
4274         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4275           tty-&gt;print_cr("  Bailing out: CallerSensitive annotation expected at frame %d", n);
4276         }
4277 #endif
4278         return false;  // bail-out; let JVM_GetCallerClass do the work
4279       }
4280       break;
4281     default:
4282       if (!m-&gt;is_ignored_by_security_stack_walk()) {
4283         // We have reached the desired frame; return the holder class.
4284         // Acquire method holder as java.lang.Class and push as constant.
4285         ciInstanceKlass* caller_klass = caller_jvms-&gt;method()-&gt;holder();
4286         ciInstance* caller_mirror = caller_klass-&gt;java_mirror();
4287         set_result(makecon(TypeInstPtr::make(caller_mirror)));
4288 
4289 #ifndef PRODUCT
4290         if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4291           tty-&gt;print_cr("  Succeeded: caller = %d) %s.%s, JVMS depth = %d", n, caller_klass-&gt;name()-&gt;as_utf8(), caller_jvms-&gt;method()-&gt;name()-&gt;as_utf8(), jvms()-&gt;depth());
4292           tty-&gt;print_cr("  JVM state at this point:");
4293           for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4294             ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4295             tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4296           }
4297         }
4298 #endif
4299         return true;
4300       }
4301       break;
4302     }
4303   }
4304 
4305 #ifndef PRODUCT
4306   if ((C-&gt;print_intrinsics() || C-&gt;print_inlining()) &amp;&amp; Verbose) {
4307     tty-&gt;print_cr("  Bailing out because caller depth exceeded inlining depth = %d", jvms()-&gt;depth());
4308     tty-&gt;print_cr("  JVM state at this point:");
4309     for (int i = jvms()-&gt;depth(), n = 1; i &gt;= 1; i--, n++) {
4310       ciMethod* m = jvms()-&gt;of_depth(i)-&gt;method();
4311       tty-&gt;print_cr("   %d) %s.%s", n, m-&gt;holder()-&gt;name()-&gt;as_utf8(), m-&gt;name()-&gt;as_utf8());
4312     }
4313   }
4314 #endif
4315 
4316   return false;  // bail-out; let JVM_GetCallerClass do the work
4317 }
4318 
4319 bool LibraryCallKit::inline_fp_conversions(vmIntrinsics::ID id) {
4320   Node* arg = argument(0);
4321   Node* result = NULL;
4322 
4323   switch (id) {
4324   case vmIntrinsics::_floatToRawIntBits:    result = new MoveF2INode(arg);  break;
4325   case vmIntrinsics::_intBitsToFloat:       result = new MoveI2FNode(arg);  break;
4326   case vmIntrinsics::_doubleToRawLongBits:  result = new MoveD2LNode(arg);  break;
4327   case vmIntrinsics::_longBitsToDouble:     result = new MoveL2DNode(arg);  break;
4328 
4329   case vmIntrinsics::_doubleToLongBits: {
4330     // two paths (plus control) merge in a wood
4331     RegionNode *r = new RegionNode(3);
4332     Node *phi = new PhiNode(r, TypeLong::LONG);
4333 
4334     Node *cmpisnan = _gvn.transform(new CmpDNode(arg, arg));
4335     // Build the boolean node
4336     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4337 
4338     // Branch either way.
4339     // NaN case is less traveled, which makes all the difference.
4340     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4341     Node *opt_isnan = _gvn.transform(ifisnan);
4342     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4343     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4344     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4345 
4346     set_control(iftrue);
4347 
4348     static const jlong nan_bits = CONST64(0x7ff8000000000000);
4349     Node *slow_result = longcon(nan_bits); // return NaN
4350     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4351     r-&gt;init_req(1, iftrue);
4352 
4353     // Else fall through
4354     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4355     set_control(iffalse);
4356 
4357     phi-&gt;init_req(2, _gvn.transform(new MoveD2LNode(arg)));
4358     r-&gt;init_req(2, iffalse);
4359 
4360     // Post merge
4361     set_control(_gvn.transform(r));
4362     record_for_igvn(r);
4363 
4364     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4365     result = phi;
4366     assert(result-&gt;bottom_type()-&gt;isa_long(), "must be");
4367     break;
4368   }
4369 
4370   case vmIntrinsics::_floatToIntBits: {
4371     // two paths (plus control) merge in a wood
4372     RegionNode *r = new RegionNode(3);
4373     Node *phi = new PhiNode(r, TypeInt::INT);
4374 
4375     Node *cmpisnan = _gvn.transform(new CmpFNode(arg, arg));
4376     // Build the boolean node
4377     Node *bolisnan = _gvn.transform(new BoolNode(cmpisnan, BoolTest::ne));
4378 
4379     // Branch either way.
4380     // NaN case is less traveled, which makes all the difference.
4381     IfNode *ifisnan = create_and_xform_if(control(), bolisnan, PROB_STATIC_FREQUENT, COUNT_UNKNOWN);
4382     Node *opt_isnan = _gvn.transform(ifisnan);
4383     assert( opt_isnan-&gt;is_If(), "Expect an IfNode");
4384     IfNode *opt_ifisnan = (IfNode*)opt_isnan;
4385     Node *iftrue = _gvn.transform(new IfTrueNode(opt_ifisnan));
4386 
4387     set_control(iftrue);
4388 
4389     static const jint nan_bits = 0x7fc00000;
4390     Node *slow_result = makecon(TypeInt::make(nan_bits)); // return NaN
4391     phi-&gt;init_req(1, _gvn.transform( slow_result ));
4392     r-&gt;init_req(1, iftrue);
4393 
4394     // Else fall through
4395     Node *iffalse = _gvn.transform(new IfFalseNode(opt_ifisnan));
4396     set_control(iffalse);
4397 
4398     phi-&gt;init_req(2, _gvn.transform(new MoveF2INode(arg)));
4399     r-&gt;init_req(2, iffalse);
4400 
4401     // Post merge
4402     set_control(_gvn.transform(r));
4403     record_for_igvn(r);
4404 
4405     C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
4406     result = phi;
4407     assert(result-&gt;bottom_type()-&gt;isa_int(), "must be");
4408     break;
4409   }
4410 
4411   default:
4412     fatal_unexpected_iid(id);
4413     break;
4414   }
4415   set_result(_gvn.transform(result));
4416   return true;
4417 }
4418 
4419 //----------------------inline_unsafe_copyMemory-------------------------
4420 // public native void Unsafe.copyMemory(Object srcBase, long srcOffset, Object destBase, long destOffset, long bytes);
4421 bool LibraryCallKit::inline_unsafe_copyMemory() {
4422   if (callee()-&gt;is_static())  return false;  // caller must have the capability!
4423   null_check_receiver();  // null-check receiver
4424   if (stopped())  return true;
4425 
4426   C-&gt;set_has_unsafe_access(true);  // Mark eventual nmethod as "unsafe".
4427 
4428   Node* src_ptr =         argument(1);   // type: oop
4429   Node* src_off = ConvL2X(argument(2));  // type: long
4430   Node* dst_ptr =         argument(4);   // type: oop
4431   Node* dst_off = ConvL2X(argument(5));  // type: long
4432   Node* size    = ConvL2X(argument(7));  // type: long
4433 
4434   assert(Unsafe_field_offset_to_byte_offset(11) == 11,
4435          "fieldOffset must be byte-scaled");
4436 
4437   Node* src = make_unsafe_address(src_ptr, src_off);
4438   Node* dst = make_unsafe_address(dst_ptr, dst_off);
4439 
4440   // Conservatively insert a memory barrier on all memory slices.
4441   // Do not let writes of the copy source or destination float below the copy.
4442   insert_mem_bar(Op_MemBarCPUOrder);
4443 
4444   // Call it.  Note that the length argument is not scaled.
4445   make_runtime_call(RC_LEAF|RC_NO_FP,
4446                     OptoRuntime::fast_arraycopy_Type(),
4447                     StubRoutines::unsafe_arraycopy(),
4448                     "unsafe_arraycopy",
4449                     TypeRawPtr::BOTTOM,
4450                     src, dst, size XTOP);
4451 
4452   // Do not let reads of the copy destination float above the copy.
4453   insert_mem_bar(Op_MemBarCPUOrder);
4454 
4455   return true;
4456 }
4457 
4458 //------------------------clone_coping-----------------------------------
4459 // Helper function for inline_native_clone.
4460 void LibraryCallKit::copy_to_clone(Node* obj, Node* alloc_obj, Node* obj_size, bool is_array, bool card_mark) {
4461   assert(obj_size != NULL, "");
4462   Node* raw_obj = alloc_obj-&gt;in(1);
4463   assert(alloc_obj-&gt;is_CheckCastPP() &amp;&amp; raw_obj-&gt;is_Proj() &amp;&amp; raw_obj-&gt;in(0)-&gt;is_Allocate(), "");
4464 
4465   AllocateNode* alloc = NULL;
4466   if (ReduceBulkZeroing) {
4467     // We will be completely responsible for initializing this object -
4468     // mark Initialize node as complete.
4469     alloc = AllocateNode::Ideal_allocation(alloc_obj, &amp;_gvn);
4470     // The object was just allocated - there should be no any stores!
4471     guarantee(alloc != NULL &amp;&amp; alloc-&gt;maybe_set_complete(&amp;_gvn), "");
4472     // Mark as complete_with_arraycopy so that on AllocateNode
4473     // expansion, we know this AllocateNode is initialized by an array
4474     // copy and a StoreStore barrier exists after the array copy.
4475     alloc-&gt;initialization()-&gt;set_complete_with_arraycopy();
4476   }
4477 
4478   // Copy the fastest available way.
4479   // TODO: generate fields copies for small objects instead.
4480   Node* src  = obj;
4481   Node* dest = alloc_obj;
4482   Node* size = _gvn.transform(obj_size);
4483 
4484   // Exclude the header but include array length to copy by 8 bytes words.
4485   // Can't use base_offset_in_bytes(bt) since basic type is unknown.
4486   int base_off = is_array ? arrayOopDesc::length_offset_in_bytes() :
4487                             instanceOopDesc::base_offset_in_bytes();
4488   // base_off:
4489   // 8  - 32-bit VM
4490   // 12 - 64-bit VM, compressed klass
4491   // 16 - 64-bit VM, normal klass
4492   if (base_off % BytesPerLong != 0) {
4493     assert(UseCompressedClassPointers, "");
4494     if (is_array) {
4495       // Exclude length to copy by 8 bytes words.
4496       base_off += sizeof(int);
4497     } else {
4498       // Include klass to copy by 8 bytes words.
4499       base_off = instanceOopDesc::klass_offset_in_bytes();
4500     }
4501     assert(base_off % BytesPerLong == 0, "expect 8 bytes alignment");
4502   }
4503   src  = basic_plus_adr(src,  base_off);
4504   dest = basic_plus_adr(dest, base_off);
4505 
4506   // Compute the length also, if needed:
4507   Node* countx = size;
4508   countx = _gvn.transform(new SubXNode(countx, MakeConX(base_off)));
4509   countx = _gvn.transform(new URShiftXNode(countx, intcon(LogBytesPerLong) ));
4510 
4511   const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4512 
4513   ArrayCopyNode* ac = ArrayCopyNode::make(this, false, src, NULL, dest, NULL, countx, false);
4514   ac-&gt;set_clonebasic();
4515   Node* n = _gvn.transform(ac);
4516   if (n == ac) {
4517     set_predefined_output_for_runtime_call(ac, ac-&gt;in(TypeFunc::Memory), raw_adr_type);
4518   } else {
4519     set_all_memory(n);
4520   }
4521 
4522   // If necessary, emit some card marks afterwards.  (Non-arrays only.)
4523   if (card_mark) {
4524     assert(!is_array, "");
4525     // Put in store barrier for any and all oops we are sticking
4526     // into this object.  (We could avoid this if we could prove
4527     // that the object type contains no oop fields at all.)
4528     Node* no_particular_value = NULL;
4529     Node* no_particular_field = NULL;
4530     int raw_adr_idx = Compile::AliasIdxRaw;
4531     post_barrier(control(),
4532                  memory(raw_adr_type),
4533                  alloc_obj,
4534                  no_particular_field,
4535                  raw_adr_idx,
4536                  no_particular_value,
4537                  T_OBJECT,
4538                  false);
4539   }
4540 
4541   // Do not let reads from the cloned object float above the arraycopy.
4542   if (alloc != NULL) {
4543     // Do not let stores that initialize this object be reordered with
4544     // a subsequent store that would make this object accessible by
4545     // other threads.
4546     // Record what AllocateNode this StoreStore protects so that
4547     // escape analysis can go from the MemBarStoreStoreNode to the
4548     // AllocateNode and eliminate the MemBarStoreStoreNode if possible
4549     // based on the escape status of the AllocateNode.
4550     insert_mem_bar(Op_MemBarStoreStore, alloc-&gt;proj_out(AllocateNode::RawAddress));
4551   } else {
4552     insert_mem_bar(Op_MemBarCPUOrder);
4553   }
4554 }
4555 
4556 //------------------------inline_native_clone----------------------------
4557 // protected native Object java.lang.Object.clone();
4558 //
4559 // Here are the simple edge cases:
4560 //  null receiver =&gt; normal trap
4561 //  virtual and clone was overridden =&gt; slow path to out-of-line clone
4562 //  not cloneable or finalizer =&gt; slow path to out-of-line Object.clone
4563 //
4564 // The general case has two steps, allocation and copying.
4565 // Allocation has two cases, and uses GraphKit::new_instance or new_array.
4566 //
4567 // Copying also has two cases, oop arrays and everything else.
4568 // Oop arrays use arrayof_oop_arraycopy (same as System.arraycopy).
4569 // Everything else uses the tight inline loop supplied by CopyArrayNode.
4570 //
4571 // These steps fold up nicely if and when the cloned object's klass
4572 // can be sharply typed as an object array, a type array, or an instance.
4573 //
4574 bool LibraryCallKit::inline_native_clone(bool is_virtual) {
4575   PhiNode* result_val;
4576 
4577   // Set the reexecute bit for the interpreter to reexecute
4578   // the bytecode that invokes Object.clone if deoptimization happens.
4579   { PreserveReexecuteState preexecs(this);
4580     jvms()-&gt;set_should_reexecute(true);
4581 
4582     Node* obj = null_check_receiver();
4583     if (stopped())  return true;
4584 
4585     const TypeOopPtr* obj_type = _gvn.type(obj)-&gt;is_oopptr();
4586 
4587     // If we are going to clone an instance, we need its exact type to
4588     // know the number and types of fields to convert the clone to
4589     // loads/stores. Maybe a speculative type can help us.
4590     if (!obj_type-&gt;klass_is_exact() &amp;&amp;
4591         obj_type-&gt;speculative_type() != NULL &amp;&amp;
4592         obj_type-&gt;speculative_type()-&gt;is_instance_klass()) {
4593       ciInstanceKlass* spec_ik = obj_type-&gt;speculative_type()-&gt;as_instance_klass();
4594       if (spec_ik-&gt;nof_nonstatic_fields() &lt;= ArrayCopyLoadStoreMaxElem &amp;&amp;
4595           !spec_ik-&gt;has_injected_fields()) {
4596         ciKlass* k = obj_type-&gt;klass();
4597         if (!k-&gt;is_instance_klass() ||
4598             k-&gt;as_instance_klass()-&gt;is_interface() ||
4599             k-&gt;as_instance_klass()-&gt;has_subklass()) {
4600           obj = maybe_cast_profiled_obj(obj, obj_type-&gt;speculative_type(), false);
4601         }
4602       }
4603     }
4604 
4605     Node* obj_klass = load_object_klass(obj);
4606     const TypeKlassPtr* tklass = _gvn.type(obj_klass)-&gt;isa_klassptr();
4607     const TypeOopPtr*   toop   = ((tklass != NULL)
4608                                 ? tklass-&gt;as_instance_type()
4609                                 : TypeInstPtr::NOTNULL);
4610 
4611     // Conservatively insert a memory barrier on all memory slices.
4612     // Do not let writes into the original float below the clone.
4613     insert_mem_bar(Op_MemBarCPUOrder);
4614 
4615     // paths into result_reg:
4616     enum {
4617       _slow_path = 1,     // out-of-line call to clone method (virtual or not)
4618       _objArray_path,     // plain array allocation, plus arrayof_oop_arraycopy
4619       _array_path,        // plain array allocation, plus arrayof_long_arraycopy
4620       _instance_path,     // plain instance allocation, plus arrayof_long_arraycopy
4621       PATH_LIMIT
4622     };
4623     RegionNode* result_reg = new RegionNode(PATH_LIMIT);
4624     result_val             = new PhiNode(result_reg, TypeInstPtr::NOTNULL);
4625     PhiNode*    result_i_o = new PhiNode(result_reg, Type::ABIO);
4626     PhiNode*    result_mem = new PhiNode(result_reg, Type::MEMORY, TypePtr::BOTTOM);
4627     record_for_igvn(result_reg);
4628 
4629     const TypePtr* raw_adr_type = TypeRawPtr::BOTTOM;
4630     int raw_adr_idx = Compile::AliasIdxRaw;
4631 
4632     Node* array_ctl = generate_array_guard(obj_klass, (RegionNode*)NULL);
4633     if (array_ctl != NULL) {
4634       // It's an array.
4635       PreserveJVMState pjvms(this);
4636       set_control(array_ctl);
4637       Node* obj_length = load_array_length(obj);
4638       Node* obj_size  = NULL;
4639       Node* alloc_obj = new_array(obj_klass, obj_length, 0, &amp;obj_size);  // no arguments to push
4640 
4641       if (!use_ReduceInitialCardMarks()) {
4642         // If it is an oop array, it requires very special treatment,
4643         // because card marking is required on each card of the array.
4644         Node* is_obja = generate_objArray_guard(obj_klass, (RegionNode*)NULL);
4645         if (is_obja != NULL) {
4646           PreserveJVMState pjvms2(this);
4647           set_control(is_obja);
4648           // Generate a direct call to the right arraycopy function(s).
4649           Node* alloc = tightly_coupled_allocation(alloc_obj, NULL);
4650           ArrayCopyNode* ac = ArrayCopyNode::make(this, true, obj, intcon(0), alloc_obj, intcon(0), obj_length, alloc != NULL);
4651           ac-&gt;set_cloneoop();
4652           Node* n = _gvn.transform(ac);
4653           assert(n == ac, "cannot disappear");
4654           ac-&gt;connect_outputs(this);
4655 
4656           result_reg-&gt;init_req(_objArray_path, control());
4657           result_val-&gt;init_req(_objArray_path, alloc_obj);
4658           result_i_o -&gt;set_req(_objArray_path, i_o());
4659           result_mem -&gt;set_req(_objArray_path, reset_memory());
4660         }
4661       }
4662       // Otherwise, there are no card marks to worry about.
4663       // (We can dispense with card marks if we know the allocation
4664       //  comes out of eden (TLAB)...  In fact, ReduceInitialCardMarks
4665       //  causes the non-eden paths to take compensating steps to
4666       //  simulate a fresh allocation, so that no further
4667       //  card marks are required in compiled code to initialize
4668       //  the object.)
4669 
4670       if (!stopped()) {
4671         copy_to_clone(obj, alloc_obj, obj_size, true, false);
4672 
4673         // Present the results of the copy.
4674         result_reg-&gt;init_req(_array_path, control());
4675         result_val-&gt;init_req(_array_path, alloc_obj);
4676         result_i_o -&gt;set_req(_array_path, i_o());
4677         result_mem -&gt;set_req(_array_path, reset_memory());
4678       }
4679     }
4680 
4681     // We only go to the instance fast case code if we pass a number of guards.
4682     // The paths which do not pass are accumulated in the slow_region.
4683     RegionNode* slow_region = new RegionNode(1);
4684     record_for_igvn(slow_region);
4685     if (!stopped()) {
4686       // It's an instance (we did array above).  Make the slow-path tests.
4687       // If this is a virtual call, we generate a funny guard.  We grab
4688       // the vtable entry corresponding to clone() from the target object.
4689       // If the target method which we are calling happens to be the
4690       // Object clone() method, we pass the guard.  We do not need this
4691       // guard for non-virtual calls; the caller is known to be the native
4692       // Object clone().
4693       if (is_virtual) {
4694         generate_virtual_guard(obj_klass, slow_region);
4695       }
4696 
4697       // The object must be easily cloneable and must not have a finalizer.
4698       // Both of these conditions may be checked in a single test.
4699       // We could optimize the test further, but we don't care.
4700       generate_access_flags_guard(obj_klass,
4701                                   // Test both conditions:
4702                                   JVM_ACC_IS_CLONEABLE_FAST | JVM_ACC_HAS_FINALIZER,
4703                                   // Must be cloneable but not finalizer:
4704                                   JVM_ACC_IS_CLONEABLE_FAST,
4705                                   slow_region);
4706     }
4707 
4708     if (!stopped()) {
4709       // It's an instance, and it passed the slow-path tests.
4710       PreserveJVMState pjvms(this);
4711       Node* obj_size  = NULL;
4712       // Need to deoptimize on exception from allocation since Object.clone intrinsic
4713       // is reexecuted if deoptimization occurs and there could be problems when merging
4714       // exception state between multiple Object.clone versions (reexecute=true vs reexecute=false).
4715       Node* alloc_obj = new_instance(obj_klass, NULL, &amp;obj_size, /*deoptimize_on_exception=*/true);
4716 
4717       copy_to_clone(obj, alloc_obj, obj_size, false, !use_ReduceInitialCardMarks());
4718 
4719       // Present the results of the slow call.
4720       result_reg-&gt;init_req(_instance_path, control());
4721       result_val-&gt;init_req(_instance_path, alloc_obj);
4722       result_i_o -&gt;set_req(_instance_path, i_o());
4723       result_mem -&gt;set_req(_instance_path, reset_memory());
4724     }
4725 
4726     // Generate code for the slow case.  We make a call to clone().
4727     set_control(_gvn.transform(slow_region));
4728     if (!stopped()) {
4729       PreserveJVMState pjvms(this);
4730       CallJavaNode* slow_call = generate_method_call(vmIntrinsics::_clone, is_virtual);
4731       Node* slow_result = set_results_for_java_call(slow_call);
4732       // this-&gt;control() comes from set_results_for_java_call
4733       result_reg-&gt;init_req(_slow_path, control());
4734       result_val-&gt;init_req(_slow_path, slow_result);
4735       result_i_o -&gt;set_req(_slow_path, i_o());
4736       result_mem -&gt;set_req(_slow_path, reset_memory());
4737     }
4738 
4739     // Return the combined state.
4740     set_control(    _gvn.transform(result_reg));
4741     set_i_o(        _gvn.transform(result_i_o));
4742     set_all_memory( _gvn.transform(result_mem));
4743   } // original reexecute is set back here
4744 
4745   set_result(_gvn.transform(result_val));
4746   return true;
4747 }
4748 
4749 // If we have a tighly coupled allocation, the arraycopy may take care
4750 // of the array initialization. If one of the guards we insert between
4751 // the allocation and the arraycopy causes a deoptimization, an
4752 // unitialized array will escape the compiled method. To prevent that
4753 // we set the JVM state for uncommon traps between the allocation and
4754 // the arraycopy to the state before the allocation so, in case of
4755 // deoptimization, we'll reexecute the allocation and the
4756 // initialization.
4757 JVMState* LibraryCallKit::arraycopy_restore_alloc_state(AllocateArrayNode* alloc, int&amp; saved_reexecute_sp) {
4758   if (alloc != NULL) {
4759     ciMethod* trap_method = alloc-&gt;jvms()-&gt;method();
4760     int trap_bci = alloc-&gt;jvms()-&gt;bci();
4761 
4762     if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;
4763           !C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_null_check)) {
4764       // Make sure there's no store between the allocation and the
4765       // arraycopy otherwise visible side effects could be rexecuted
4766       // in case of deoptimization and cause incorrect execution.
4767       bool no_interfering_store = true;
4768       Node* mem = alloc-&gt;in(TypeFunc::Memory);
4769       if (mem-&gt;is_MergeMem()) {
4770         for (MergeMemStream mms(merged_memory(), mem-&gt;as_MergeMem()); mms.next_non_empty2(); ) {
4771           Node* n = mms.memory();
4772           if (n != mms.memory2() &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4773             assert(n-&gt;is_Store(), "what else?");
4774             no_interfering_store = false;
4775             break;
4776           }
4777         }
4778       } else {
4779         for (MergeMemStream mms(merged_memory()); mms.next_non_empty(); ) {
4780           Node* n = mms.memory();
4781           if (n != mem &amp;&amp; !(n-&gt;is_Proj() &amp;&amp; n-&gt;in(0) == alloc-&gt;initialization())) {
4782             assert(n-&gt;is_Store(), "what else?");
4783             no_interfering_store = false;
4784             break;
4785           }
4786         }
4787       }
4788 
4789       if (no_interfering_store) {
4790         JVMState* old_jvms = alloc-&gt;jvms()-&gt;clone_shallow(C);
4791         uint size = alloc-&gt;req();
4792         SafePointNode* sfpt = new SafePointNode(size, old_jvms);
4793         old_jvms-&gt;set_map(sfpt);
4794         for (uint i = 0; i &lt; size; i++) {
4795           sfpt-&gt;init_req(i, alloc-&gt;in(i));
4796         }
4797         // re-push array length for deoptimization
4798         sfpt-&gt;ins_req(old_jvms-&gt;stkoff() + old_jvms-&gt;sp(), alloc-&gt;in(AllocateNode::ALength));
4799         old_jvms-&gt;set_sp(old_jvms-&gt;sp()+1);
4800         old_jvms-&gt;set_monoff(old_jvms-&gt;monoff()+1);
4801         old_jvms-&gt;set_scloff(old_jvms-&gt;scloff()+1);
4802         old_jvms-&gt;set_endoff(old_jvms-&gt;endoff()+1);
4803         old_jvms-&gt;set_should_reexecute(true);
4804 
4805         sfpt-&gt;set_i_o(map()-&gt;i_o());
4806         sfpt-&gt;set_memory(map()-&gt;memory());
4807         sfpt-&gt;set_control(map()-&gt;control());
4808 
4809         JVMState* saved_jvms = jvms();
4810         saved_reexecute_sp = _reexecute_sp;
4811 
4812         set_jvms(sfpt-&gt;jvms());
4813         _reexecute_sp = jvms()-&gt;sp();
4814 
4815         return saved_jvms;
4816       }
4817     }
4818   }
4819   return NULL;
4820 }
4821 
4822 // In case of a deoptimization, we restart execution at the
4823 // allocation, allocating a new array. We would leave an uninitialized
4824 // array in the heap that GCs wouldn't expect. Move the allocation
4825 // after the traps so we don't allocate the array if we
4826 // deoptimize. This is possible because tightly_coupled_allocation()
4827 // guarantees there's no observer of the allocated array at this point
4828 // and the control flow is simple enough.
4829 void LibraryCallKit::arraycopy_move_allocation_here(AllocateArrayNode* alloc, Node* dest, JVMState* saved_jvms, int saved_reexecute_sp) {
4830   if (saved_jvms != NULL &amp;&amp; !stopped()) {
4831     assert(alloc != NULL, "only with a tightly coupled allocation");
4832     // restore JVM state to the state at the arraycopy
4833     saved_jvms-&gt;map()-&gt;set_control(map()-&gt;control());
4834     assert(saved_jvms-&gt;map()-&gt;memory() == map()-&gt;memory(), "memory state changed?");
4835     assert(saved_jvms-&gt;map()-&gt;i_o() == map()-&gt;i_o(), "IO state changed?");
4836     // If we've improved the types of some nodes (null check) while
4837     // emitting the guards, propagate them to the current state
4838     map()-&gt;replaced_nodes().apply(saved_jvms-&gt;map());
4839     set_jvms(saved_jvms);
4840     _reexecute_sp = saved_reexecute_sp;
4841 
4842     // Remove the allocation from above the guards
4843     CallProjections callprojs;
4844     alloc-&gt;extract_projections(&amp;callprojs, true);
4845     InitializeNode* init = alloc-&gt;initialization();
4846     Node* alloc_mem = alloc-&gt;in(TypeFunc::Memory);
4847     C-&gt;gvn_replace_by(callprojs.fallthrough_ioproj, alloc-&gt;in(TypeFunc::I_O));
4848     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Memory), alloc_mem);
4849     C-&gt;gvn_replace_by(init-&gt;proj_out(TypeFunc::Control), alloc-&gt;in(0));
4850 
4851     // move the allocation here (after the guards)
4852     _gvn.hash_delete(alloc);
4853     alloc-&gt;set_req(TypeFunc::Control, control());
4854     alloc-&gt;set_req(TypeFunc::I_O, i_o());
4855     Node *mem = reset_memory();
4856     set_all_memory(mem);
4857     alloc-&gt;set_req(TypeFunc::Memory, mem);
4858     set_control(init-&gt;proj_out(TypeFunc::Control));
4859     set_i_o(callprojs.fallthrough_ioproj);
4860 
4861     // Update memory as done in GraphKit::set_output_for_allocation()
4862     const TypeInt* length_type = _gvn.find_int_type(alloc-&gt;in(AllocateNode::ALength));
4863     const TypeOopPtr* ary_type = _gvn.type(alloc-&gt;in(AllocateNode::KlassNode))-&gt;is_klassptr()-&gt;as_instance_type();
4864     if (ary_type-&gt;isa_aryptr() &amp;&amp; length_type != NULL) {
4865       ary_type = ary_type-&gt;is_aryptr()-&gt;cast_to_size(length_type);
4866     }
4867     const TypePtr* telemref = ary_type-&gt;add_offset(Type::OffsetBot);
4868     int            elemidx  = C-&gt;get_alias_index(telemref);
4869     set_memory(init-&gt;proj_out(TypeFunc::Memory), Compile::AliasIdxRaw);
4870     set_memory(init-&gt;proj_out(TypeFunc::Memory), elemidx);
4871 
4872     Node* allocx = _gvn.transform(alloc);
4873     assert(allocx == alloc, "where has the allocation gone?");
4874     assert(dest-&gt;is_CheckCastPP(), "not an allocation result?");
4875 
4876     _gvn.hash_delete(dest);
4877     dest-&gt;set_req(0, control());
4878     Node* destx = _gvn.transform(dest);
4879     assert(destx == dest, "where has the allocation result gone?");
4880   }
4881 }
4882 
4883 
4884 //------------------------------inline_arraycopy-----------------------
4885 // public static native void java.lang.System.arraycopy(Object src,  int  srcPos,
4886 //                                                      Object dest, int destPos,
4887 //                                                      int length);
4888 bool LibraryCallKit::inline_arraycopy() {
4889   // Get the arguments.
4890   Node* src         = argument(0);  // type: oop
4891   Node* src_offset  = argument(1);  // type: int
4892   Node* dest        = argument(2);  // type: oop
4893   Node* dest_offset = argument(3);  // type: int
4894   Node* length      = argument(4);  // type: int
4895 
4896 
4897   // Check for allocation before we add nodes that would confuse
4898   // tightly_coupled_allocation()
4899   AllocateArrayNode* alloc = tightly_coupled_allocation(dest, NULL);
4900 
4901   int saved_reexecute_sp = -1;
4902   JVMState* saved_jvms = arraycopy_restore_alloc_state(alloc, saved_reexecute_sp);
4903   // See arraycopy_restore_alloc_state() comment
4904   // if alloc == NULL we don't have to worry about a tightly coupled allocation so we can emit all needed guards
4905   // if saved_jvms != NULL (then alloc != NULL) then we can handle guards and a tightly coupled allocation
4906   // if saved_jvms == NULL and alloc != NULL, we cant emit any guards
4907   bool can_emit_guards = (alloc == NULL || saved_jvms != NULL);
4908 
4909   // The following tests must be performed
4910   // (1) src and dest are arrays.
4911   // (2) src and dest arrays must have elements of the same BasicType
4912   // (3) src and dest must not be null.
4913   // (4) src_offset must not be negative.
4914   // (5) dest_offset must not be negative.
4915   // (6) length must not be negative.
4916   // (7) src_offset + length must not exceed length of src.
4917   // (8) dest_offset + length must not exceed length of dest.
4918   // (9) each element of an oop array must be assignable
4919 
4920   // (3) src and dest must not be null.
4921   // always do this here because we need the JVM state for uncommon traps
4922   Node* null_ctl = top();
4923   src  = saved_jvms != NULL ? null_check_oop(src, &amp;null_ctl, true, true) : null_check(src,  T_ARRAY);
4924   assert(null_ctl-&gt;is_top(), "no null control here");
4925   dest = null_check(dest, T_ARRAY);
4926 
4927   if (!can_emit_guards) {
4928     // if saved_jvms == NULL and alloc != NULL, we don't emit any
4929     // guards but the arraycopy node could still take advantage of a
4930     // tightly allocated allocation. tightly_coupled_allocation() is
4931     // called again to make sure it takes the null check above into
4932     // account: the null check is mandatory and if it caused an
4933     // uncommon trap to be emitted then the allocation can't be
4934     // considered tightly coupled in this context.
4935     alloc = tightly_coupled_allocation(dest, NULL);
4936   }
4937 
4938   bool validated = false;
4939 
4940   const Type* src_type  = _gvn.type(src);
4941   const Type* dest_type = _gvn.type(dest);
4942   const TypeAryPtr* top_src  = src_type-&gt;isa_aryptr();
4943   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
4944 
4945   // Do we have the type of src?
4946   bool has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4947   // Do we have the type of dest?
4948   bool has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4949   // Is the type for src from speculation?
4950   bool src_spec = false;
4951   // Is the type for dest from speculation?
4952   bool dest_spec = false;
4953 
4954   if ((!has_src || !has_dest) &amp;&amp; can_emit_guards) {
4955     // We don't have sufficient type information, let's see if
4956     // speculative types can help. We need to have types for both src
4957     // and dest so that it pays off.
4958 
4959     // Do we already have or could we have type information for src
4960     bool could_have_src = has_src;
4961     // Do we already have or could we have type information for dest
4962     bool could_have_dest = has_dest;
4963 
4964     ciKlass* src_k = NULL;
4965     if (!has_src) {
4966       src_k = src_type-&gt;speculative_type_not_null();
4967       if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
4968         could_have_src = true;
4969       }
4970     }
4971 
4972     ciKlass* dest_k = NULL;
4973     if (!has_dest) {
4974       dest_k = dest_type-&gt;speculative_type_not_null();
4975       if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
4976         could_have_dest = true;
4977       }
4978     }
4979 
4980     if (could_have_src &amp;&amp; could_have_dest) {
4981       // This is going to pay off so emit the required guards
4982       if (!has_src) {
4983         src = maybe_cast_profiled_obj(src, src_k, true);
4984         src_type  = _gvn.type(src);
4985         top_src  = src_type-&gt;isa_aryptr();
4986         has_src = (top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL);
4987         src_spec = true;
4988       }
4989       if (!has_dest) {
4990         dest = maybe_cast_profiled_obj(dest, dest_k, true);
4991         dest_type  = _gvn.type(dest);
4992         top_dest  = dest_type-&gt;isa_aryptr();
4993         has_dest = (top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL);
4994         dest_spec = true;
4995       }
4996     }
4997   }
4998 
4999   if (has_src &amp;&amp; has_dest &amp;&amp; can_emit_guards) {
5000     BasicType src_elem  = top_src-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5001     BasicType dest_elem = top_dest-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5002     if (src_elem  == T_ARRAY)  src_elem  = T_OBJECT;
5003     if (dest_elem == T_ARRAY)  dest_elem = T_OBJECT;
5004 
5005     if (src_elem == dest_elem &amp;&amp; src_elem == T_OBJECT) {
5006       // If both arrays are object arrays then having the exact types
5007       // for both will remove the need for a subtype check at runtime
5008       // before the call and may make it possible to pick a faster copy
5009       // routine (without a subtype check on every element)
5010       // Do we have the exact type of src?
5011       bool could_have_src = src_spec;
5012       // Do we have the exact type of dest?
5013       bool could_have_dest = dest_spec;
5014       ciKlass* src_k = top_src-&gt;klass();
5015       ciKlass* dest_k = top_dest-&gt;klass();
5016       if (!src_spec) {
5017         src_k = src_type-&gt;speculative_type_not_null();
5018         if (src_k != NULL &amp;&amp; src_k-&gt;is_array_klass()) {
5019           could_have_src = true;
5020         }
5021       }
5022       if (!dest_spec) {
5023         dest_k = dest_type-&gt;speculative_type_not_null();
5024         if (dest_k != NULL &amp;&amp; dest_k-&gt;is_array_klass()) {
5025           could_have_dest = true;
5026         }
5027       }
5028       if (could_have_src &amp;&amp; could_have_dest) {
5029         // If we can have both exact types, emit the missing guards
5030         if (could_have_src &amp;&amp; !src_spec) {
5031           src = maybe_cast_profiled_obj(src, src_k, true);
5032         }
5033         if (could_have_dest &amp;&amp; !dest_spec) {
5034           dest = maybe_cast_profiled_obj(dest, dest_k, true);
5035         }
5036       }
5037     }
5038   }
5039 
5040   ciMethod* trap_method = method();
5041   int trap_bci = bci();
5042   if (saved_jvms != NULL) {
5043     trap_method = alloc-&gt;jvms()-&gt;method();
5044     trap_bci = alloc-&gt;jvms()-&gt;bci();
5045   }
5046 
5047   if (!C-&gt;too_many_traps(trap_method, trap_bci, Deoptimization::Reason_intrinsic) &amp;&amp;
5048       can_emit_guards &amp;&amp;
5049       !src-&gt;is_top() &amp;&amp; !dest-&gt;is_top()) {
5050     // validate arguments: enables transformation the ArrayCopyNode
5051     validated = true;
5052 
5053     RegionNode* slow_region = new RegionNode(1);
5054     record_for_igvn(slow_region);
5055 
5056     // (1) src and dest are arrays.
5057     generate_non_array_guard(load_object_klass(src), slow_region);
5058     generate_non_array_guard(load_object_klass(dest), slow_region);
5059 
5060     // (2) src and dest arrays must have elements of the same BasicType
5061     // done at macro expansion or at Ideal transformation time
5062 
5063     // (4) src_offset must not be negative.
5064     generate_negative_guard(src_offset, slow_region);
5065 
5066     // (5) dest_offset must not be negative.
5067     generate_negative_guard(dest_offset, slow_region);
5068 
5069     // (7) src_offset + length must not exceed length of src.
5070     generate_limit_guard(src_offset, length,
5071                          load_array_length(src),
5072                          slow_region);
5073 
5074     // (8) dest_offset + length must not exceed length of dest.
5075     generate_limit_guard(dest_offset, length,
5076                          load_array_length(dest),
5077                          slow_region);
5078 
5079     // (9) each element of an oop array must be assignable
5080     Node* src_klass  = load_object_klass(src);
5081     Node* dest_klass = load_object_klass(dest);
5082     Node* not_subtype_ctrl = gen_subtype_check(src_klass, dest_klass);
5083 
5084     if (not_subtype_ctrl != top()) {
5085       PreserveJVMState pjvms(this);
5086       set_control(not_subtype_ctrl);
5087       uncommon_trap(Deoptimization::Reason_intrinsic,
5088                     Deoptimization::Action_make_not_entrant);
5089       assert(stopped(), "Should be stopped");
5090     }
5091     {
5092       PreserveJVMState pjvms(this);
5093       set_control(_gvn.transform(slow_region));
5094       uncommon_trap(Deoptimization::Reason_intrinsic,
5095                     Deoptimization::Action_make_not_entrant);
5096       assert(stopped(), "Should be stopped");
5097     }
5098   }
5099 
5100   arraycopy_move_allocation_here(alloc, dest, saved_jvms, saved_reexecute_sp);
5101 
5102   if (stopped()) {
5103     return true;
5104   }
5105 
5106   ArrayCopyNode* ac = ArrayCopyNode::make(this, true, src, src_offset, dest, dest_offset, length, alloc != NULL,
5107                                           // Create LoadRange and LoadKlass nodes for use during macro expansion here
5108                                           // so the compiler has a chance to eliminate them: during macro expansion,
5109                                           // we have to set their control (CastPP nodes are eliminated).
5110                                           load_object_klass(src), load_object_klass(dest),
5111                                           load_array_length(src), load_array_length(dest));
5112 
5113   ac-&gt;set_arraycopy(validated);
5114 
5115   Node* n = _gvn.transform(ac);
5116   if (n == ac) {
5117     ac-&gt;connect_outputs(this);
5118   } else {
5119     assert(validated, "shouldn't transform if all arguments not validated");
5120     set_all_memory(n);
5121   }
5122 
5123   return true;
5124 }
5125 
5126 
5127 // Helper function which determines if an arraycopy immediately follows
5128 // an allocation, with no intervening tests or other escapes for the object.
5129 AllocateArrayNode*
5130 LibraryCallKit::tightly_coupled_allocation(Node* ptr,
5131                                            RegionNode* slow_region) {
5132   if (stopped())             return NULL;  // no fast path
5133   if (C-&gt;AliasLevel() == 0)  return NULL;  // no MergeMems around
5134 
5135   AllocateArrayNode* alloc = AllocateArrayNode::Ideal_array_allocation(ptr, &amp;_gvn);
5136   if (alloc == NULL)  return NULL;
5137 
5138   Node* rawmem = memory(Compile::AliasIdxRaw);
5139   // Is the allocation's memory state untouched?
5140   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0)-&gt;is_Initialize())) {
5141     // Bail out if there have been raw-memory effects since the allocation.
5142     // (Example:  There might have been a call or safepoint.)
5143     return NULL;
5144   }
5145   rawmem = rawmem-&gt;in(0)-&gt;as_Initialize()-&gt;memory(Compile::AliasIdxRaw);
5146   if (!(rawmem-&gt;is_Proj() &amp;&amp; rawmem-&gt;in(0) == alloc)) {
5147     return NULL;
5148   }
5149 
5150   // There must be no unexpected observers of this allocation.
5151   for (DUIterator_Fast imax, i = ptr-&gt;fast_outs(imax); i &lt; imax; i++) {
5152     Node* obs = ptr-&gt;fast_out(i);
5153     if (obs != this-&gt;map()) {
5154       return NULL;
5155     }
5156   }
5157 
5158   // This arraycopy must unconditionally follow the allocation of the ptr.
5159   Node* alloc_ctl = ptr-&gt;in(0);
5160   assert(just_allocated_object(alloc_ctl) == ptr, "most recent allo");
5161 
5162   Node* ctl = control();
5163   while (ctl != alloc_ctl) {
5164     // There may be guards which feed into the slow_region.
5165     // Any other control flow means that we might not get a chance
5166     // to finish initializing the allocated object.
5167     if ((ctl-&gt;is_IfFalse() || ctl-&gt;is_IfTrue()) &amp;&amp; ctl-&gt;in(0)-&gt;is_If()) {
5168       IfNode* iff = ctl-&gt;in(0)-&gt;as_If();
5169       Node* not_ctl = iff-&gt;proj_out(1 - ctl-&gt;as_Proj()-&gt;_con);
5170       assert(not_ctl != NULL &amp;&amp; not_ctl != ctl, "found alternate");
5171       if (slow_region != NULL &amp;&amp; slow_region-&gt;find_edge(not_ctl) &gt;= 1) {
5172         ctl = iff-&gt;in(0);       // This test feeds the known slow_region.
5173         continue;
5174       }
5175       // One more try:  Various low-level checks bottom out in
5176       // uncommon traps.  If the debug-info of the trap omits
5177       // any reference to the allocation, as we've already
5178       // observed, then there can be no objection to the trap.
5179       bool found_trap = false;
5180       for (DUIterator_Fast jmax, j = not_ctl-&gt;fast_outs(jmax); j &lt; jmax; j++) {
5181         Node* obs = not_ctl-&gt;fast_out(j);
5182         if (obs-&gt;in(0) == not_ctl &amp;&amp; obs-&gt;is_Call() &amp;&amp;
5183             (obs-&gt;as_Call()-&gt;entry_point() == SharedRuntime::uncommon_trap_blob()-&gt;entry_point())) {
5184           found_trap = true; break;
5185         }
5186       }
5187       if (found_trap) {
5188         ctl = iff-&gt;in(0);       // This test feeds a harmless uncommon trap.
5189         continue;
5190       }
5191     }
5192     return NULL;
5193   }
5194 
5195   // If we get this far, we have an allocation which immediately
5196   // precedes the arraycopy, and we can take over zeroing the new object.
5197   // The arraycopy will finish the initialization, and provide
5198   // a new control state to which we will anchor the destination pointer.
5199 
5200   return alloc;
5201 }
5202 
5203 //-------------inline_encodeISOArray-----------------------------------
5204 // encode char[] to byte[] in ISO_8859_1
5205 bool LibraryCallKit::inline_encodeISOArray() {
5206   assert(callee()-&gt;signature()-&gt;size() == 5, "encodeISOArray has 5 parameters");
5207   // no receiver since it is static method
5208   Node *src         = argument(0);
5209   Node *src_offset  = argument(1);
5210   Node *dst         = argument(2);
5211   Node *dst_offset  = argument(3);
5212   Node *length      = argument(4);
5213 
5214   const Type* src_type = src-&gt;Value(&amp;_gvn);
5215   const Type* dst_type = dst-&gt;Value(&amp;_gvn);
5216   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5217   const TypeAryPtr* top_dest = dst_type-&gt;isa_aryptr();
5218   if (top_src  == NULL || top_src-&gt;klass()  == NULL ||
5219       top_dest == NULL || top_dest-&gt;klass() == NULL) {
5220     // failed array check
5221     return false;
5222   }
5223 
5224   // Figure out the size and type of the elements we will be copying.
5225   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5226   BasicType dst_elem = dst_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5227   if (!((src_elem == T_CHAR) || (src_elem== T_BYTE)) || dst_elem != T_BYTE) {
5228     return false;
5229   }
5230 
5231   Node* src_start = array_element_address(src, src_offset, T_CHAR);
5232   Node* dst_start = array_element_address(dst, dst_offset, dst_elem);
5233   // 'src_start' points to src array + scaled offset
5234   // 'dst_start' points to dst array + scaled offset
5235 
5236   const TypeAryPtr* mtype = TypeAryPtr::BYTES;
5237   Node* enc = new EncodeISOArrayNode(control(), memory(mtype), src_start, dst_start, length);
5238   enc = _gvn.transform(enc);
5239   Node* res_mem = _gvn.transform(new SCMemProjNode(enc));
5240   set_memory(res_mem, mtype);
5241   set_result(enc);
5242   return true;
5243 }
5244 
5245 //-------------inline_multiplyToLen-----------------------------------
5246 bool LibraryCallKit::inline_multiplyToLen() {
5247   assert(UseMultiplyToLenIntrinsic, "not implemented on this platform");
5248 
5249   address stubAddr = StubRoutines::multiplyToLen();
5250   if (stubAddr == NULL) {
5251     return false; // Intrinsic's stub is not implemented on this platform
5252   }
5253   const char* stubName = "multiplyToLen";
5254 
5255   assert(callee()-&gt;signature()-&gt;size() == 5, "multiplyToLen has 5 parameters");
5256 
5257   // no receiver because it is a static method
5258   Node* x    = argument(0);
5259   Node* xlen = argument(1);
5260   Node* y    = argument(2);
5261   Node* ylen = argument(3);
5262   Node* z    = argument(4);
5263 
5264   const Type* x_type = x-&gt;Value(&amp;_gvn);
5265   const Type* y_type = y-&gt;Value(&amp;_gvn);
5266   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5267   const TypeAryPtr* top_y = y_type-&gt;isa_aryptr();
5268   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5269       top_y == NULL || top_y-&gt;klass() == NULL) {
5270     // failed array check
5271     return false;
5272   }
5273 
5274   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5275   BasicType y_elem = y_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5276   if (x_elem != T_INT || y_elem != T_INT) {
5277     return false;
5278   }
5279 
5280   // Set the original stack and the reexecute bit for the interpreter to reexecute
5281   // the bytecode that invokes BigInteger.multiplyToLen() if deoptimization happens
5282   // on the return from z array allocation in runtime.
5283   { PreserveReexecuteState preexecs(this);
5284     jvms()-&gt;set_should_reexecute(true);
5285 
5286     Node* x_start = array_element_address(x, intcon(0), x_elem);
5287     Node* y_start = array_element_address(y, intcon(0), y_elem);
5288     // 'x_start' points to x array + scaled xlen
5289     // 'y_start' points to y array + scaled ylen
5290 
5291     // Allocate the result array
5292     Node* zlen = _gvn.transform(new AddINode(xlen, ylen));
5293     ciKlass* klass = ciTypeArrayKlass::make(T_INT);
5294     Node* klass_node = makecon(TypeKlassPtr::make(klass));
5295 
5296     IdealKit ideal(this);
5297 
5298 #define __ ideal.
5299      Node* one = __ ConI(1);
5300      Node* zero = __ ConI(0);
5301      IdealVariable need_alloc(ideal), z_alloc(ideal);  __ declarations_done();
5302      __ set(need_alloc, zero);
5303      __ set(z_alloc, z);
5304      __ if_then(z, BoolTest::eq, null()); {
5305        __ increment (need_alloc, one);
5306      } __ else_(); {
5307        // Update graphKit memory and control from IdealKit.
5308        sync_kit(ideal);
5309        Node* zlen_arg = load_array_length(z);
5310        // Update IdealKit memory and control from graphKit.
5311        __ sync_kit(this);
5312        __ if_then(zlen_arg, BoolTest::lt, zlen); {
5313          __ increment (need_alloc, one);
5314        } __ end_if();
5315      } __ end_if();
5316 
5317      __ if_then(__ value(need_alloc), BoolTest::ne, zero); {
5318        // Update graphKit memory and control from IdealKit.
5319        sync_kit(ideal);
5320        Node * narr = new_array(klass_node, zlen, 1);
5321        // Update IdealKit memory and control from graphKit.
5322        __ sync_kit(this);
5323        __ set(z_alloc, narr);
5324      } __ end_if();
5325 
5326      sync_kit(ideal);
5327      z = __ value(z_alloc);
5328      // Can't use TypeAryPtr::INTS which uses Bottom offset.
5329      _gvn.set_type(z, TypeOopPtr::make_from_klass(klass));
5330      // Final sync IdealKit and GraphKit.
5331      final_sync(ideal);
5332 #undef __
5333 
5334     Node* z_start = array_element_address(z, intcon(0), T_INT);
5335 
5336     Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
5337                                    OptoRuntime::multiplyToLen_Type(),
5338                                    stubAddr, stubName, TypePtr::BOTTOM,
5339                                    x_start, xlen, y_start, ylen, z_start, zlen);
5340   } // original reexecute is set back here
5341 
5342   C-&gt;set_has_split_ifs(true); // Has chance for split-if optimization
5343   set_result(z);
5344   return true;
5345 }
5346 
5347 //-------------inline_squareToLen------------------------------------
5348 bool LibraryCallKit::inline_squareToLen() {
5349   assert(UseSquareToLenIntrinsic, "not implemented on this platform");
5350 
5351   address stubAddr = StubRoutines::squareToLen();
5352   if (stubAddr == NULL) {
5353     return false; // Intrinsic's stub is not implemented on this platform
5354   }
5355   const char* stubName = "squareToLen";
5356 
5357   assert(callee()-&gt;signature()-&gt;size() == 4, "implSquareToLen has 4 parameters");
5358 
5359   Node* x    = argument(0);
5360   Node* len  = argument(1);
5361   Node* z    = argument(2);
5362   Node* zlen = argument(3);
5363 
5364   const Type* x_type = x-&gt;Value(&amp;_gvn);
5365   const Type* z_type = z-&gt;Value(&amp;_gvn);
5366   const TypeAryPtr* top_x = x_type-&gt;isa_aryptr();
5367   const TypeAryPtr* top_z = z_type-&gt;isa_aryptr();
5368   if (top_x  == NULL || top_x-&gt;klass()  == NULL ||
5369       top_z  == NULL || top_z-&gt;klass()  == NULL) {
5370     // failed array check
5371     return false;
5372   }
5373 
5374   BasicType x_elem = x_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5375   BasicType z_elem = z_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5376   if (x_elem != T_INT || z_elem != T_INT) {
5377     return false;
5378   }
5379 
5380 
5381   Node* x_start = array_element_address(x, intcon(0), x_elem);
5382   Node* z_start = array_element_address(z, intcon(0), z_elem);
5383 
5384   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5385                                   OptoRuntime::squareToLen_Type(),
5386                                   stubAddr, stubName, TypePtr::BOTTOM,
5387                                   x_start, len, z_start, zlen);
5388 
5389   set_result(z);
5390   return true;
5391 }
5392 
5393 //-------------inline_mulAdd------------------------------------------
5394 bool LibraryCallKit::inline_mulAdd() {
5395   assert(UseMulAddIntrinsic, "not implemented on this platform");
5396 
5397   address stubAddr = StubRoutines::mulAdd();
5398   if (stubAddr == NULL) {
5399     return false; // Intrinsic's stub is not implemented on this platform
5400   }
5401   const char* stubName = "mulAdd";
5402 
5403   assert(callee()-&gt;signature()-&gt;size() == 5, "mulAdd has 5 parameters");
5404 
5405   Node* out      = argument(0);
5406   Node* in       = argument(1);
5407   Node* offset   = argument(2);
5408   Node* len      = argument(3);
5409   Node* k        = argument(4);
5410 
5411   const Type* out_type = out-&gt;Value(&amp;_gvn);
5412   const Type* in_type = in-&gt;Value(&amp;_gvn);
5413   const TypeAryPtr* top_out = out_type-&gt;isa_aryptr();
5414   const TypeAryPtr* top_in = in_type-&gt;isa_aryptr();
5415   if (top_out  == NULL || top_out-&gt;klass()  == NULL ||
5416       top_in == NULL || top_in-&gt;klass() == NULL) {
5417     // failed array check
5418     return false;
5419   }
5420 
5421   BasicType out_elem = out_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5422   BasicType in_elem = in_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5423   if (out_elem != T_INT || in_elem != T_INT) {
5424     return false;
5425   }
5426 
5427   Node* outlen = load_array_length(out);
5428   Node* new_offset = _gvn.transform(new SubINode(outlen, offset));
5429   Node* out_start = array_element_address(out, intcon(0), out_elem);
5430   Node* in_start = array_element_address(in, intcon(0), in_elem);
5431 
5432   Node*  call = make_runtime_call(RC_LEAF|RC_NO_FP,
5433                                   OptoRuntime::mulAdd_Type(),
5434                                   stubAddr, stubName, TypePtr::BOTTOM,
5435                                   out_start,in_start, new_offset, len, k);
5436   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5437   set_result(result);
5438   return true;
5439 }
5440 
5441 //-------------inline_montgomeryMultiply-----------------------------------
5442 bool LibraryCallKit::inline_montgomeryMultiply() {
5443   address stubAddr = StubRoutines::montgomeryMultiply();
5444   if (stubAddr == NULL) {
5445     return false; // Intrinsic's stub is not implemented on this platform
5446   }
5447 
5448   assert(UseMontgomeryMultiplyIntrinsic, "not implemented on this platform");
5449   const char* stubName = "montgomery_square";
5450 
5451   assert(callee()-&gt;signature()-&gt;size() == 7, "montgomeryMultiply has 7 parameters");
5452 
5453   Node* a    = argument(0);
5454   Node* b    = argument(1);
5455   Node* n    = argument(2);
5456   Node* len  = argument(3);
5457   Node* inv  = argument(4);
5458   Node* m    = argument(6);
5459 
5460   const Type* a_type = a-&gt;Value(&amp;_gvn);
5461   const TypeAryPtr* top_a = a_type-&gt;isa_aryptr();
5462   const Type* b_type = b-&gt;Value(&amp;_gvn);
5463   const TypeAryPtr* top_b = b_type-&gt;isa_aryptr();
5464   const Type* n_type = a-&gt;Value(&amp;_gvn);
5465   const TypeAryPtr* top_n = n_type-&gt;isa_aryptr();
5466   const Type* m_type = a-&gt;Value(&amp;_gvn);
5467   const TypeAryPtr* top_m = m_type-&gt;isa_aryptr();
5468   if (top_a  == NULL || top_a-&gt;klass()  == NULL ||
5469       top_b == NULL || top_b-&gt;klass()  == NULL ||
5470       top_n == NULL || top_n-&gt;klass()  == NULL ||
5471       top_m == NULL || top_m-&gt;klass()  == NULL) {
5472     // failed array check
5473     return false;
5474   }
5475 
5476   BasicType a_elem = a_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5477   BasicType b_elem = b_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5478   BasicType n_elem = n_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5479   BasicType m_elem = m_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5480   if (a_elem != T_INT || b_elem != T_INT || n_elem != T_INT || m_elem != T_INT) {
5481     return false;
5482   }
5483 
5484   // Make the call
5485   {
5486     Node* a_start = array_element_address(a, intcon(0), a_elem);
5487     Node* b_start = array_element_address(b, intcon(0), b_elem);
5488     Node* n_start = array_element_address(n, intcon(0), n_elem);
5489     Node* m_start = array_element_address(m, intcon(0), m_elem);
5490 
5491     Node* call = make_runtime_call(RC_LEAF,
5492                                    OptoRuntime::montgomeryMultiply_Type(),
5493                                    stubAddr, stubName, TypePtr::BOTTOM,
5494                                    a_start, b_start, n_start, len, inv, top(),
5495                                    m_start);
5496     set_result(m);
5497   }
5498 
5499   return true;
5500 }
5501 
5502 bool LibraryCallKit::inline_montgomerySquare() {
5503   address stubAddr = StubRoutines::montgomerySquare();
5504   if (stubAddr == NULL) {
5505     return false; // Intrinsic's stub is not implemented on this platform
5506   }
5507 
5508   assert(UseMontgomerySquareIntrinsic, "not implemented on this platform");
5509   const char* stubName = "montgomery_square";
5510 
5511   assert(callee()-&gt;signature()-&gt;size() == 6, "montgomerySquare has 6 parameters");
5512 
5513   Node* a    = argument(0);
5514   Node* n    = argument(1);
5515   Node* len  = argument(2);
5516   Node* inv  = argument(3);
5517   Node* m    = argument(5);
5518 
5519   const Type* a_type = a-&gt;Value(&amp;_gvn);
5520   const TypeAryPtr* top_a = a_type-&gt;isa_aryptr();
5521   const Type* n_type = a-&gt;Value(&amp;_gvn);
5522   const TypeAryPtr* top_n = n_type-&gt;isa_aryptr();
5523   const Type* m_type = a-&gt;Value(&amp;_gvn);
5524   const TypeAryPtr* top_m = m_type-&gt;isa_aryptr();
5525   if (top_a  == NULL || top_a-&gt;klass()  == NULL ||
5526       top_n == NULL || top_n-&gt;klass()  == NULL ||
5527       top_m == NULL || top_m-&gt;klass()  == NULL) {
5528     // failed array check
5529     return false;
5530   }
5531 
5532   BasicType a_elem = a_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5533   BasicType n_elem = n_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5534   BasicType m_elem = m_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5535   if (a_elem != T_INT || n_elem != T_INT || m_elem != T_INT) {
5536     return false;
5537   }
5538 
5539   // Make the call
5540   {
5541     Node* a_start = array_element_address(a, intcon(0), a_elem);
5542     Node* n_start = array_element_address(n, intcon(0), n_elem);
5543     Node* m_start = array_element_address(m, intcon(0), m_elem);
5544 
5545     Node* call = make_runtime_call(RC_LEAF,
5546                                    OptoRuntime::montgomerySquare_Type(),
5547                                    stubAddr, stubName, TypePtr::BOTTOM,
5548                                    a_start, n_start, len, inv, top(),
5549                                    m_start);
5550     set_result(m);
5551   }
5552 
5553   return true;
5554 }
5555 
5556 //-------------inline_vectorizedMismatch------------------------------
5557 bool LibraryCallKit::inline_vectorizedMismatch() {
5558   assert(UseVectorizedMismatchIntrinsic, "not implementated on this platform");
5559 
5560   address stubAddr = StubRoutines::vectorizedMismatch();
5561   if (stubAddr == NULL) {
5562     return false; // Intrinsic's stub is not implemented on this platform
5563   }
5564   const char* stubName = "vectorizedMismatch";
5565   int size_l = callee()-&gt;signature()-&gt;size();
5566   assert(callee()-&gt;signature()-&gt;size() == 8, "vectorizedMismatch has 6 parameters");
5567 
5568   Node* obja = argument(0);
5569   Node* aoffset = argument(1);
5570   Node* objb = argument(3);
5571   Node* boffset = argument(4);
5572   Node* length = argument(6);
5573   Node* scale = argument(7);
5574 
5575   const Type* a_type = obja-&gt;Value(&amp;_gvn);
5576   const Type* b_type = objb-&gt;Value(&amp;_gvn);
5577   const TypeAryPtr* top_a = a_type-&gt;isa_aryptr();
5578   const TypeAryPtr* top_b = b_type-&gt;isa_aryptr();
5579   if (top_a == NULL || top_a-&gt;klass() == NULL ||
5580     top_b == NULL || top_b-&gt;klass() == NULL) {
5581     // failed array check
5582     return false;
5583   }
5584 
5585   Node* call;
5586   jvms()-&gt;set_should_reexecute(true);
5587 
5588   Node* obja_adr = make_unsafe_address(obja, aoffset);
5589   Node* objb_adr = make_unsafe_address(objb, boffset);
5590 
5591   call = make_runtime_call(RC_LEAF,
5592     OptoRuntime::vectorizedMismatch_Type(),
5593     stubAddr, stubName, TypePtr::BOTTOM,
5594     obja_adr, objb_adr, length, scale);
5595 
5596   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5597   set_result(result);
5598   return true;
5599 }
5600 
5601 /**
5602  * Calculate CRC32 for byte.
5603  * int java.util.zip.CRC32.update(int crc, int b)
5604  */
5605 bool LibraryCallKit::inline_updateCRC32() {
5606   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5607   assert(callee()-&gt;signature()-&gt;size() == 2, "update has 2 parameters");
5608   // no receiver since it is static method
5609   Node* crc  = argument(0); // type: int
5610   Node* b    = argument(1); // type: int
5611 
5612   /*
5613    *    int c = ~ crc;
5614    *    b = timesXtoThe32[(b ^ c) &amp; 0xFF];
5615    *    b = b ^ (c &gt;&gt;&gt; 8);
5616    *    crc = ~b;
5617    */
5618 
5619   Node* M1 = intcon(-1);
5620   crc = _gvn.transform(new XorINode(crc, M1));
5621   Node* result = _gvn.transform(new XorINode(crc, b));
5622   result = _gvn.transform(new AndINode(result, intcon(0xFF)));
5623 
5624   Node* base = makecon(TypeRawPtr::make(StubRoutines::crc_table_addr()));
5625   Node* offset = _gvn.transform(new LShiftINode(result, intcon(0x2)));
5626   Node* adr = basic_plus_adr(top(), base, ConvI2X(offset));
5627   result = make_load(control(), adr, TypeInt::INT, T_INT, MemNode::unordered);
5628 
5629   crc = _gvn.transform(new URShiftINode(crc, intcon(8)));
5630   result = _gvn.transform(new XorINode(crc, result));
5631   result = _gvn.transform(new XorINode(result, M1));
5632   set_result(result);
5633   return true;
5634 }
5635 
5636 /**
5637  * Calculate CRC32 for byte[] array.
5638  * int java.util.zip.CRC32.updateBytes(int crc, byte[] buf, int off, int len)
5639  */
5640 bool LibraryCallKit::inline_updateBytesCRC32() {
5641   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5642   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5643   // no receiver since it is static method
5644   Node* crc     = argument(0); // type: int
5645   Node* src     = argument(1); // type: oop
5646   Node* offset  = argument(2); // type: int
5647   Node* length  = argument(3); // type: int
5648 
5649   const Type* src_type = src-&gt;Value(&amp;_gvn);
5650   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5651   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5652     // failed array check
5653     return false;
5654   }
5655 
5656   // Figure out the size and type of the elements we will be copying.
5657   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5658   if (src_elem != T_BYTE) {
5659     return false;
5660   }
5661 
5662   // 'src_start' points to src array + scaled offset
5663   Node* src_start = array_element_address(src, offset, src_elem);
5664 
5665   // We assume that range check is done by caller.
5666   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5667 
5668   // Call the stub.
5669   address stubAddr = StubRoutines::updateBytesCRC32();
5670   const char *stubName = "updateBytesCRC32";
5671 
5672   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5673                                  stubAddr, stubName, TypePtr::BOTTOM,
5674                                  crc, src_start, length);
5675   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5676   set_result(result);
5677   return true;
5678 }
5679 
5680 /**
5681  * Calculate CRC32 for ByteBuffer.
5682  * int java.util.zip.CRC32.updateByteBuffer(int crc, long buf, int off, int len)
5683  */
5684 bool LibraryCallKit::inline_updateByteBufferCRC32() {
5685   assert(UseCRC32Intrinsics, "need AVX and LCMUL instructions support");
5686   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5687   // no receiver since it is static method
5688   Node* crc     = argument(0); // type: int
5689   Node* src     = argument(1); // type: long
5690   Node* offset  = argument(3); // type: int
5691   Node* length  = argument(4); // type: int
5692 
5693   src = ConvL2X(src);  // adjust Java long to machine word
5694   Node* base = _gvn.transform(new CastX2PNode(src));
5695   offset = ConvI2X(offset);
5696 
5697   // 'src_start' points to src array + scaled offset
5698   Node* src_start = basic_plus_adr(top(), base, offset);
5699 
5700   // Call the stub.
5701   address stubAddr = StubRoutines::updateBytesCRC32();
5702   const char *stubName = "updateBytesCRC32";
5703 
5704   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::updateBytesCRC32_Type(),
5705                                  stubAddr, stubName, TypePtr::BOTTOM,
5706                                  crc, src_start, length);
5707   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5708   set_result(result);
5709   return true;
5710 }
5711 
5712 //------------------------------get_table_from_crc32c_class-----------------------
5713 Node * LibraryCallKit::get_table_from_crc32c_class(ciInstanceKlass *crc32c_class) {
5714   Node* table = load_field_from_object(NULL, "byteTable", "[I", /*is_exact*/ false, /*is_static*/ true, crc32c_class);
5715   assert (table != NULL, "wrong version of java.util.zip.CRC32C");
5716 
5717   return table;
5718 }
5719 
5720 //------------------------------inline_updateBytesCRC32C-----------------------
5721 //
5722 // Calculate CRC32C for byte[] array.
5723 // int java.util.zip.CRC32C.updateBytes(int crc, byte[] buf, int off, int end)
5724 //
5725 bool LibraryCallKit::inline_updateBytesCRC32C() {
5726   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5727   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5728   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5729   // no receiver since it is a static method
5730   Node* crc     = argument(0); // type: int
5731   Node* src     = argument(1); // type: oop
5732   Node* offset  = argument(2); // type: int
5733   Node* end     = argument(3); // type: int
5734 
5735   Node* length = _gvn.transform(new SubINode(end, offset));
5736 
5737   const Type* src_type = src-&gt;Value(&amp;_gvn);
5738   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5739   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5740     // failed array check
5741     return false;
5742   }
5743 
5744   // Figure out the size and type of the elements we will be copying.
5745   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5746   if (src_elem != T_BYTE) {
5747     return false;
5748   }
5749 
5750   // 'src_start' points to src array + scaled offset
5751   Node* src_start = array_element_address(src, offset, src_elem);
5752 
5753   // static final int[] byteTable in class CRC32C
5754   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5755   Node* table_start = array_element_address(table, intcon(0), T_INT);
5756 
5757   // We assume that range check is done by caller.
5758   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5759 
5760   // Call the stub.
5761   address stubAddr = StubRoutines::updateBytesCRC32C();
5762   const char *stubName = "updateBytesCRC32C";
5763 
5764   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5765                                  stubAddr, stubName, TypePtr::BOTTOM,
5766                                  crc, src_start, length, table_start);
5767   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5768   set_result(result);
5769   return true;
5770 }
5771 
5772 //------------------------------inline_updateDirectByteBufferCRC32C-----------------------
5773 //
5774 // Calculate CRC32C for DirectByteBuffer.
5775 // int java.util.zip.CRC32C.updateDirectByteBuffer(int crc, long buf, int off, int end)
5776 //
5777 bool LibraryCallKit::inline_updateDirectByteBufferCRC32C() {
5778   assert(UseCRC32CIntrinsics, "need CRC32C instruction support");
5779   assert(callee()-&gt;signature()-&gt;size() == 5, "updateDirectByteBuffer has 4 parameters and one is long");
5780   assert(callee()-&gt;holder()-&gt;is_loaded(), "CRC32C class must be loaded");
5781   // no receiver since it is a static method
5782   Node* crc     = argument(0); // type: int
5783   Node* src     = argument(1); // type: long
5784   Node* offset  = argument(3); // type: int
5785   Node* end     = argument(4); // type: int
5786 
5787   Node* length = _gvn.transform(new SubINode(end, offset));
5788 
5789   src = ConvL2X(src);  // adjust Java long to machine word
5790   Node* base = _gvn.transform(new CastX2PNode(src));
5791   offset = ConvI2X(offset);
5792 
5793   // 'src_start' points to src array + scaled offset
5794   Node* src_start = basic_plus_adr(top(), base, offset);
5795 
5796   // static final int[] byteTable in class CRC32C
5797   Node* table = get_table_from_crc32c_class(callee()-&gt;holder());
5798   Node* table_start = array_element_address(table, intcon(0), T_INT);
5799 
5800   // Call the stub.
5801   address stubAddr = StubRoutines::updateBytesCRC32C();
5802   const char *stubName = "updateBytesCRC32C";
5803 
5804   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesCRC32C_Type(),
5805                                  stubAddr, stubName, TypePtr::BOTTOM,
5806                                  crc, src_start, length, table_start);
5807   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5808   set_result(result);
5809   return true;
5810 }
5811 
5812 //------------------------------inline_updateBytesAdler32----------------------
5813 //
5814 // Calculate Adler32 checksum for byte[] array.
5815 // int java.util.zip.Adler32.updateBytes(int crc, byte[] buf, int off, int len)
5816 //
5817 bool LibraryCallKit::inline_updateBytesAdler32() {
5818   assert(UseAdler32Intrinsics, "Adler32 Instrinsic support need"); // check if we actually need to check this flag or check a different one
5819   assert(callee()-&gt;signature()-&gt;size() == 4, "updateBytes has 4 parameters");
5820   assert(callee()-&gt;holder()-&gt;is_loaded(), "Adler32 class must be loaded");
5821   // no receiver since it is static method
5822   Node* crc     = argument(0); // type: int
5823   Node* src     = argument(1); // type: oop
5824   Node* offset  = argument(2); // type: int
5825   Node* length  = argument(3); // type: int
5826 
5827   const Type* src_type = src-&gt;Value(&amp;_gvn);
5828   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
5829   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
5830     // failed array check
5831     return false;
5832   }
5833 
5834   // Figure out the size and type of the elements we will be copying.
5835   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
5836   if (src_elem != T_BYTE) {
5837     return false;
5838   }
5839 
5840   // 'src_start' points to src array + scaled offset
5841   Node* src_start = array_element_address(src, offset, src_elem);
5842 
5843   // We assume that range check is done by caller.
5844   // TODO: generate range check (offset+length &lt; src.length) in debug VM.
5845 
5846   // Call the stub.
5847   address stubAddr = StubRoutines::updateBytesAdler32();
5848   const char *stubName = "updateBytesAdler32";
5849 
5850   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesAdler32_Type(),
5851                                  stubAddr, stubName, TypePtr::BOTTOM,
5852                                  crc, src_start, length);
5853   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5854   set_result(result);
5855   return true;
5856 }
5857 
5858 //------------------------------inline_updateByteBufferAdler32---------------
5859 //
5860 // Calculate Adler32 checksum for DirectByteBuffer.
5861 // int java.util.zip.Adler32.updateByteBuffer(int crc, long buf, int off, int len)
5862 //
5863 bool LibraryCallKit::inline_updateByteBufferAdler32() {
5864   assert(UseAdler32Intrinsics, "Adler32 Instrinsic support need"); // check if we actually need to check this flag or check a different one
5865   assert(callee()-&gt;signature()-&gt;size() == 5, "updateByteBuffer has 4 parameters and one is long");
5866   assert(callee()-&gt;holder()-&gt;is_loaded(), "Adler32 class must be loaded");
5867   // no receiver since it is static method
5868   Node* crc     = argument(0); // type: int
5869   Node* src     = argument(1); // type: long
5870   Node* offset  = argument(3); // type: int
5871   Node* length  = argument(4); // type: int
5872 
5873   src = ConvL2X(src);  // adjust Java long to machine word
5874   Node* base = _gvn.transform(new CastX2PNode(src));
5875   offset = ConvI2X(offset);
5876 
5877   // 'src_start' points to src array + scaled offset
5878   Node* src_start = basic_plus_adr(top(), base, offset);
5879 
5880   // Call the stub.
5881   address stubAddr = StubRoutines::updateBytesAdler32();
5882   const char *stubName = "updateBytesAdler32";
5883 
5884   Node* call = make_runtime_call(RC_LEAF, OptoRuntime::updateBytesAdler32_Type(),
5885                                  stubAddr, stubName, TypePtr::BOTTOM,
5886                                  crc, src_start, length);
5887 
5888   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
5889   set_result(result);
5890   return true;
5891 }
5892 
5893 //----------------------------inline_reference_get----------------------------
5894 // public T java.lang.ref.Reference.get();
5895 bool LibraryCallKit::inline_reference_get() {
5896   const int referent_offset = java_lang_ref_Reference::referent_offset;
5897   guarantee(referent_offset &gt; 0, "should have already been set");
5898 
5899   // Get the argument:
5900   Node* reference_obj = null_check_receiver();
5901   if (stopped()) return true;
5902 
5903   Node* adr = basic_plus_adr(reference_obj, reference_obj, referent_offset);
5904 
5905   ciInstanceKlass* klass = env()-&gt;Object_klass();
5906   const TypeOopPtr* object_type = TypeOopPtr::make_from_klass(klass);
5907 
5908   Node* no_ctrl = NULL;
5909   Node* result = make_load(no_ctrl, adr, object_type, T_OBJECT, MemNode::unordered);
5910 
5911   // Use the pre-barrier to record the value in the referent field
5912   pre_barrier(false /* do_load */,
5913               control(),
5914               NULL /* obj */, NULL /* adr */, max_juint /* alias_idx */, NULL /* val */, NULL /* val_type */,
5915               result /* pre_val */,
5916               T_OBJECT);
5917 
5918   // Add memory barrier to prevent commoning reads from this field
5919   // across safepoint since GC can change its value.
5920   insert_mem_bar(Op_MemBarCPUOrder);
5921 
5922   set_result(result);
5923   return true;
5924 }
5925 
5926 
5927 Node * LibraryCallKit::load_field_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
5928                                               bool is_exact=true, bool is_static=false,
5929                                               ciInstanceKlass * fromKls=NULL) {
5930   if (fromKls == NULL) {
5931     const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
5932     assert(tinst != NULL, "obj is null");
5933     assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
5934     assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
5935     fromKls = tinst-&gt;klass()-&gt;as_instance_klass();
5936   } else {
5937     assert(is_static, "only for static field access");
5938   }
5939   ciField* field = fromKls-&gt;get_field_by_name(ciSymbol::make(fieldName),
5940                                               ciSymbol::make(fieldTypeString),
5941                                               is_static);
5942 
5943   assert (field != NULL, "undefined field");
5944   if (field == NULL) return (Node *) NULL;
5945 
5946   if (is_static) {
5947     const TypeInstPtr* tip = TypeInstPtr::make(fromKls-&gt;java_mirror());
5948     fromObj = makecon(tip);
5949   }
5950 
5951   // Next code  copied from Parse::do_get_xxx():
5952 
5953   // Compute address and memory type.
5954   int offset  = field-&gt;offset_in_bytes();
5955   bool is_vol = field-&gt;is_volatile();
5956   ciType* field_klass = field-&gt;type();
5957   assert(field_klass-&gt;is_loaded(), "should be loaded");
5958   const TypePtr* adr_type = C-&gt;alias_type(field)-&gt;adr_type();
5959   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
5960   BasicType bt = field-&gt;layout_type();
5961 
5962   // Build the resultant type of the load
5963   const Type *type;
5964   if (bt == T_OBJECT) {
5965     type = TypeOopPtr::make_from_klass(field_klass-&gt;as_klass());
5966   } else {
5967     type = Type::get_const_basic_type(bt);
5968   }
5969 
5970   if (support_IRIW_for_not_multiple_copy_atomic_cpu &amp;&amp; is_vol) {
5971     insert_mem_bar(Op_MemBarVolatile);   // StoreLoad barrier
5972   }
5973   // Build the load.
5974   MemNode::MemOrd mo = is_vol ? MemNode::acquire : MemNode::unordered;
5975   Node* loadedField = make_load(NULL, adr, type, bt, adr_type, mo, LoadNode::DependsOnlyOnTest, is_vol);
5976   // If reference is volatile, prevent following memory ops from
5977   // floating up past the volatile read.  Also prevents commoning
5978   // another volatile read.
5979   if (is_vol) {
5980     // Memory barrier includes bogus read of value to force load BEFORE membar
5981     insert_mem_bar(Op_MemBarAcquire, loadedField);
5982   }
5983   return loadedField;
5984 }
5985 
5986 Node * LibraryCallKit::field_address_from_object(Node * fromObj, const char * fieldName, const char * fieldTypeString,
5987                                                  bool is_exact = true, bool is_static = false,
5988                                                  ciInstanceKlass * fromKls = NULL) {
5989   if (fromKls == NULL) {
5990     const TypeInstPtr* tinst = _gvn.type(fromObj)-&gt;isa_instptr();
5991     assert(tinst != NULL, "obj is null");
5992     assert(tinst-&gt;klass()-&gt;is_loaded(), "obj is not loaded");
5993     assert(!is_exact || tinst-&gt;klass_is_exact(), "klass not exact");
5994     fromKls = tinst-&gt;klass()-&gt;as_instance_klass();
5995   }
5996   else {
5997     assert(is_static, "only for static field access");
5998   }
5999   ciField* field = fromKls-&gt;get_field_by_name(ciSymbol::make(fieldName),
6000     ciSymbol::make(fieldTypeString),
6001     is_static);
6002 
6003   assert(field != NULL, "undefined field");
6004   assert(!field-&gt;is_volatile(), "not defined for volatile fields");
6005 
6006   if (is_static) {
6007     const TypeInstPtr* tip = TypeInstPtr::make(fromKls-&gt;java_mirror());
6008     fromObj = makecon(tip);
6009   }
6010 
6011   // Next code  copied from Parse::do_get_xxx():
6012 
6013   // Compute address and memory type.
6014   int offset = field-&gt;offset_in_bytes();
6015   Node *adr = basic_plus_adr(fromObj, fromObj, offset);
6016 
6017   return adr;
6018 }
6019 
6020 //------------------------------inline_aescrypt_Block-----------------------
6021 bool LibraryCallKit::inline_aescrypt_Block(vmIntrinsics::ID id) {
6022   address stubAddr = NULL;
6023   const char *stubName;
6024   assert(UseAES, "need AES instruction support");
6025 
6026   switch(id) {
6027   case vmIntrinsics::_aescrypt_encryptBlock:
6028     stubAddr = StubRoutines::aescrypt_encryptBlock();
6029     stubName = "aescrypt_encryptBlock";
6030     break;
6031   case vmIntrinsics::_aescrypt_decryptBlock:
6032     stubAddr = StubRoutines::aescrypt_decryptBlock();
6033     stubName = "aescrypt_decryptBlock";
6034     break;
6035   }
6036   if (stubAddr == NULL) return false;
6037 
6038   Node* aescrypt_object = argument(0);
6039   Node* src             = argument(1);
6040   Node* src_offset      = argument(2);
6041   Node* dest            = argument(3);
6042   Node* dest_offset     = argument(4);
6043 
6044   // (1) src and dest are arrays.
6045   const Type* src_type = src-&gt;Value(&amp;_gvn);
6046   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
6047   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6048   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
6049   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
6050 
6051   // for the quick and dirty code we will skip all the checks.
6052   // we are just trying to get the call to be generated.
6053   Node* src_start  = src;
6054   Node* dest_start = dest;
6055   if (src_offset != NULL || dest_offset != NULL) {
6056     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
6057     src_start  = array_element_address(src,  src_offset,  T_BYTE);
6058     dest_start = array_element_address(dest, dest_offset, T_BYTE);
6059   }
6060 
6061   // now need to get the start of its expanded key array
6062   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
6063   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
6064   if (k_start == NULL) return false;
6065 
6066   if (Matcher::pass_original_key_for_aes()) {
6067     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
6068     // compatibility issues between Java key expansion and SPARC crypto instructions
6069     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
6070     if (original_k_start == NULL) return false;
6071 
6072     // Call the stub.
6073     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
6074                       stubAddr, stubName, TypePtr::BOTTOM,
6075                       src_start, dest_start, k_start, original_k_start);
6076   } else {
6077     // Call the stub.
6078     make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::aescrypt_block_Type(),
6079                       stubAddr, stubName, TypePtr::BOTTOM,
6080                       src_start, dest_start, k_start);
6081   }
6082 
6083   return true;
6084 }
6085 
6086 //------------------------------inline_cipherBlockChaining_AESCrypt-----------------------
6087 bool LibraryCallKit::inline_cipherBlockChaining_AESCrypt(vmIntrinsics::ID id) {
6088   address stubAddr = NULL;
6089   const char *stubName = NULL;
6090 
6091   assert(UseAES, "need AES instruction support");
6092 
6093   switch(id) {
6094   case vmIntrinsics::_cipherBlockChaining_encryptAESCrypt:
6095     stubAddr = StubRoutines::cipherBlockChaining_encryptAESCrypt();
6096     stubName = "cipherBlockChaining_encryptAESCrypt";
6097     break;
6098   case vmIntrinsics::_cipherBlockChaining_decryptAESCrypt:
6099     stubAddr = StubRoutines::cipherBlockChaining_decryptAESCrypt();
6100     stubName = "cipherBlockChaining_decryptAESCrypt";
6101     break;
6102   }
6103   if (stubAddr == NULL) return false;
6104 
6105   Node* cipherBlockChaining_object = argument(0);
6106   Node* src                        = argument(1);
6107   Node* src_offset                 = argument(2);
6108   Node* len                        = argument(3);
6109   Node* dest                       = argument(4);
6110   Node* dest_offset                = argument(5);
6111 
6112   // (1) src and dest are arrays.
6113   const Type* src_type = src-&gt;Value(&amp;_gvn);
6114   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
6115   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6116   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
6117   assert (top_src  != NULL &amp;&amp; top_src-&gt;klass()  != NULL
6118           &amp;&amp;  top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
6119 
6120   // checks are the responsibility of the caller
6121   Node* src_start  = src;
6122   Node* dest_start = dest;
6123   if (src_offset != NULL || dest_offset != NULL) {
6124     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
6125     src_start  = array_element_address(src,  src_offset,  T_BYTE);
6126     dest_start = array_element_address(dest, dest_offset, T_BYTE);
6127   }
6128 
6129   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
6130   // (because of the predicated logic executed earlier).
6131   // so we cast it here safely.
6132   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
6133 
6134   Node* embeddedCipherObj = load_field_from_object(cipherBlockChaining_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6135   if (embeddedCipherObj == NULL) return false;
6136 
6137   // cast it to what we know it will be at runtime
6138   const TypeInstPtr* tinst = _gvn.type(cipherBlockChaining_object)-&gt;isa_instptr();
6139   assert(tinst != NULL, "CBC obj is null");
6140   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBC obj is not loaded");
6141   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6142   assert(klass_AESCrypt-&gt;is_loaded(), "predicate checks that this class is loaded");
6143 
6144   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6145   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
6146   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6147   Node* aescrypt_object = new CheckCastPPNode(control(), embeddedCipherObj, xtype);
6148   aescrypt_object = _gvn.transform(aescrypt_object);
6149 
6150   // we need to get the start of the aescrypt_object's expanded key array
6151   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
6152   if (k_start == NULL) return false;
6153 
6154   // similarly, get the start address of the r vector
6155   Node* objRvec = load_field_from_object(cipherBlockChaining_object, "r", "[B", /*is_exact*/ false);
6156   if (objRvec == NULL) return false;
6157   Node* r_start = array_element_address(objRvec, intcon(0), T_BYTE);
6158 
6159   Node* cbcCrypt;
6160   if (Matcher::pass_original_key_for_aes()) {
6161     // on SPARC we need to pass the original key since key expansion needs to happen in intrinsics due to
6162     // compatibility issues between Java key expansion and SPARC crypto instructions
6163     Node* original_k_start = get_original_key_start_from_aescrypt_object(aescrypt_object);
6164     if (original_k_start == NULL) return false;
6165 
6166     // Call the stub, passing src_start, dest_start, k_start, r_start, src_len and original_k_start
6167     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6168                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6169                                  stubAddr, stubName, TypePtr::BOTTOM,
6170                                  src_start, dest_start, k_start, r_start, len, original_k_start);
6171   } else {
6172     // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
6173     cbcCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6174                                  OptoRuntime::cipherBlockChaining_aescrypt_Type(),
6175                                  stubAddr, stubName, TypePtr::BOTTOM,
6176                                  src_start, dest_start, k_start, r_start, len);
6177   }
6178 
6179   // return cipher length (int)
6180   Node* retvalue = _gvn.transform(new ProjNode(cbcCrypt, TypeFunc::Parms));
6181   set_result(retvalue);
6182   return true;
6183 }
6184 
6185 //------------------------------inline_counterMode_AESCrypt-----------------------
6186 bool LibraryCallKit::inline_counterMode_AESCrypt(vmIntrinsics::ID id) {
6187   assert(UseAES, "need AES instruction support");
6188   if (!UseAESCTRIntrinsics) return false;
6189 
6190   address stubAddr = NULL;
6191   const char *stubName = NULL;
6192   if (id == vmIntrinsics::_counterMode_AESCrypt) {
6193     stubAddr = StubRoutines::counterMode_AESCrypt();
6194     stubName = "counterMode_AESCrypt";
6195   }
6196   if (stubAddr == NULL) return false;
6197 
6198   Node* counterMode_object = argument(0);
6199   Node* src = argument(1);
6200   Node* src_offset = argument(2);
6201   Node* len = argument(3);
6202   Node* dest = argument(4);
6203   Node* dest_offset = argument(5);
6204 
6205   // (1) src and dest are arrays.
6206   const Type* src_type = src-&gt;Value(&amp;_gvn);
6207   const Type* dest_type = dest-&gt;Value(&amp;_gvn);
6208   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6209   const TypeAryPtr* top_dest = dest_type-&gt;isa_aryptr();
6210   assert(top_src != NULL &amp;&amp; top_src-&gt;klass() != NULL &amp;&amp;
6211          top_dest != NULL &amp;&amp; top_dest-&gt;klass() != NULL, "args are strange");
6212 
6213   // checks are the responsibility of the caller
6214   Node* src_start = src;
6215   Node* dest_start = dest;
6216   if (src_offset != NULL || dest_offset != NULL) {
6217     assert(src_offset != NULL &amp;&amp; dest_offset != NULL, "");
6218     src_start = array_element_address(src, src_offset, T_BYTE);
6219     dest_start = array_element_address(dest, dest_offset, T_BYTE);
6220   }
6221 
6222   // if we are in this set of code, we "know" the embeddedCipher is an AESCrypt object
6223   // (because of the predicated logic executed earlier).
6224   // so we cast it here safely.
6225   // this requires a newer class file that has this array as littleEndian ints, otherwise we revert to java
6226   Node* embeddedCipherObj = load_field_from_object(counterMode_object, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6227   if (embeddedCipherObj == NULL) return false;
6228   // cast it to what we know it will be at runtime
6229   const TypeInstPtr* tinst = _gvn.type(counterMode_object)-&gt;isa_instptr();
6230   assert(tinst != NULL, "CTR obj is null");
6231   assert(tinst-&gt;klass()-&gt;is_loaded(), "CTR obj is not loaded");
6232   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6233   assert(klass_AESCrypt-&gt;is_loaded(), "predicate checks that this class is loaded");
6234   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6235   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_AESCrypt);
6236   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6237   Node* aescrypt_object = new CheckCastPPNode(control(), embeddedCipherObj, xtype);
6238   aescrypt_object = _gvn.transform(aescrypt_object);
6239   // we need to get the start of the aescrypt_object's expanded key array
6240   Node* k_start = get_key_start_from_aescrypt_object(aescrypt_object);
6241   if (k_start == NULL) return false;
6242   // similarly, get the start address of the r vector
6243   Node* obj_counter = load_field_from_object(counterMode_object, "counter", "[B", /*is_exact*/ false);
6244   if (obj_counter == NULL) return false;
6245   Node* cnt_start = array_element_address(obj_counter, intcon(0), T_BYTE);
6246 
6247   Node* saved_encCounter = load_field_from_object(counterMode_object, "encryptedCounter", "[B", /*is_exact*/ false);
6248   if (saved_encCounter == NULL) return false;
6249   Node* saved_encCounter_start = array_element_address(saved_encCounter, intcon(0), T_BYTE);
6250   Node* used = field_address_from_object(counterMode_object, "used", "I", /*is_exact*/ false);
6251 
6252   Node* ctrCrypt;
6253   if (Matcher::pass_original_key_for_aes()) {
6254     // no SPARC version for AES/CTR intrinsics now.
6255     return false;
6256   }
6257   // Call the stub, passing src_start, dest_start, k_start, r_start and src_len
6258   ctrCrypt = make_runtime_call(RC_LEAF|RC_NO_FP,
6259                                OptoRuntime::counterMode_aescrypt_Type(),
6260                                stubAddr, stubName, TypePtr::BOTTOM,
6261                                src_start, dest_start, k_start, cnt_start, len, saved_encCounter_start, used);
6262 
6263   // return cipher length (int)
6264   Node* retvalue = _gvn.transform(new ProjNode(ctrCrypt, TypeFunc::Parms));
6265   set_result(retvalue);
6266   return true;
6267 }
6268 
6269 //------------------------------get_key_start_from_aescrypt_object-----------------------
6270 Node * LibraryCallKit::get_key_start_from_aescrypt_object(Node *aescrypt_object) {
6271   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "K", "[I", /*is_exact*/ false);
6272   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6273   if (objAESCryptKey == NULL) return (Node *) NULL;
6274 
6275   // now have the array, need to get the start address of the K array
6276   Node* k_start = array_element_address(objAESCryptKey, intcon(0), T_INT);
6277   return k_start;
6278 }
6279 
6280 //------------------------------get_original_key_start_from_aescrypt_object-----------------------
6281 Node * LibraryCallKit::get_original_key_start_from_aescrypt_object(Node *aescrypt_object) {
6282   Node* objAESCryptKey = load_field_from_object(aescrypt_object, "lastKey", "[B", /*is_exact*/ false);
6283   assert (objAESCryptKey != NULL, "wrong version of com.sun.crypto.provider.AESCrypt");
6284   if (objAESCryptKey == NULL) return (Node *) NULL;
6285 
6286   // now have the array, need to get the start address of the lastKey array
6287   Node* original_k_start = array_element_address(objAESCryptKey, intcon(0), T_BYTE);
6288   return original_k_start;
6289 }
6290 
6291 //----------------------------inline_cipherBlockChaining_AESCrypt_predicate----------------------------
6292 // Return node representing slow path of predicate check.
6293 // the pseudo code we want to emulate with this predicate is:
6294 // for encryption:
6295 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
6296 // for decryption:
6297 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
6298 //    note cipher==plain is more conservative than the original java code but that's OK
6299 //
6300 Node* LibraryCallKit::inline_cipherBlockChaining_AESCrypt_predicate(bool decrypting) {
6301   // The receiver was checked for NULL already.
6302   Node* objCBC = argument(0);
6303 
6304   // Load embeddedCipher field of CipherBlockChaining object.
6305   Node* embeddedCipherObj = load_field_from_object(objCBC, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6306 
6307   // get AESCrypt klass for instanceOf check
6308   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
6309   // will have same classloader as CipherBlockChaining object
6310   const TypeInstPtr* tinst = _gvn.type(objCBC)-&gt;isa_instptr();
6311   assert(tinst != NULL, "CBCobj is null");
6312   assert(tinst-&gt;klass()-&gt;is_loaded(), "CBCobj is not loaded");
6313 
6314   // we want to do an instanceof comparison against the AESCrypt class
6315   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6316   if (!klass_AESCrypt-&gt;is_loaded()) {
6317     // if AESCrypt is not even loaded, we never take the intrinsic fast path
6318     Node* ctrl = control();
6319     set_control(top()); // no regular fast path
6320     return ctrl;
6321   }
6322   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6323 
6324   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
6325   Node* cmp_instof  = _gvn.transform(new CmpINode(instof, intcon(1)));
6326   Node* bool_instof  = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
6327 
6328   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6329 
6330   // for encryption, we are done
6331   if (!decrypting)
6332     return instof_false;  // even if it is NULL
6333 
6334   // for decryption, we need to add a further check to avoid
6335   // taking the intrinsic path when cipher and plain are the same
6336   // see the original java code for why.
6337   RegionNode* region = new RegionNode(3);
6338   region-&gt;init_req(1, instof_false);
6339   Node* src = argument(1);
6340   Node* dest = argument(4);
6341   Node* cmp_src_dest = _gvn.transform(new CmpPNode(src, dest));
6342   Node* bool_src_dest = _gvn.transform(new BoolNode(cmp_src_dest, BoolTest::eq));
6343   Node* src_dest_conjoint = generate_guard(bool_src_dest, NULL, PROB_MIN);
6344   region-&gt;init_req(2, src_dest_conjoint);
6345 
6346   record_for_igvn(region);
6347   return _gvn.transform(region);
6348 }
6349 
6350 //----------------------------inline_counterMode_AESCrypt_predicate----------------------------
6351 // Return node representing slow path of predicate check.
6352 // the pseudo code we want to emulate with this predicate is:
6353 // for encryption:
6354 //    if (embeddedCipherObj instanceof AESCrypt) do_intrinsic, else do_javapath
6355 // for decryption:
6356 //    if ((embeddedCipherObj instanceof AESCrypt) &amp;&amp; (cipher!=plain)) do_intrinsic, else do_javapath
6357 //    note cipher==plain is more conservative than the original java code but that's OK
6358 //
6359 
6360 Node* LibraryCallKit::inline_counterMode_AESCrypt_predicate() {
6361   // The receiver was checked for NULL already.
6362   Node* objCTR = argument(0);
6363 
6364   // Load embeddedCipher field of CipherBlockChaining object.
6365   Node* embeddedCipherObj = load_field_from_object(objCTR, "embeddedCipher", "Lcom/sun/crypto/provider/SymmetricCipher;", /*is_exact*/ false);
6366 
6367   // get AESCrypt klass for instanceOf check
6368   // AESCrypt might not be loaded yet if some other SymmetricCipher got us to this compile point
6369   // will have same classloader as CipherBlockChaining object
6370   const TypeInstPtr* tinst = _gvn.type(objCTR)-&gt;isa_instptr();
6371   assert(tinst != NULL, "CTRobj is null");
6372   assert(tinst-&gt;klass()-&gt;is_loaded(), "CTRobj is not loaded");
6373 
6374   // we want to do an instanceof comparison against the AESCrypt class
6375   ciKlass* klass_AESCrypt = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make("com/sun/crypto/provider/AESCrypt"));
6376   if (!klass_AESCrypt-&gt;is_loaded()) {
6377     // if AESCrypt is not even loaded, we never take the intrinsic fast path
6378     Node* ctrl = control();
6379     set_control(top()); // no regular fast path
6380     return ctrl;
6381   }
6382 
6383   ciInstanceKlass* instklass_AESCrypt = klass_AESCrypt-&gt;as_instance_klass();
6384   Node* instof = gen_instanceof(embeddedCipherObj, makecon(TypeKlassPtr::make(instklass_AESCrypt)));
6385   Node* cmp_instof = _gvn.transform(new CmpINode(instof, intcon(1)));
6386   Node* bool_instof = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
6387   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6388 
6389   return instof_false; // even if it is NULL
6390 }
6391 
6392 //------------------------------inline_ghash_processBlocks
6393 bool LibraryCallKit::inline_ghash_processBlocks() {
6394   address stubAddr;
6395   const char *stubName;
6396   assert(UseGHASHIntrinsics, "need GHASH intrinsics support");
6397 
6398   stubAddr = StubRoutines::ghash_processBlocks();
6399   stubName = "ghash_processBlocks";
6400 
6401   Node* data           = argument(0);
6402   Node* offset         = argument(1);
6403   Node* len            = argument(2);
6404   Node* state          = argument(3);
6405   Node* subkeyH        = argument(4);
6406 
6407   Node* state_start  = array_element_address(state, intcon(0), T_LONG);
6408   assert(state_start, "state is NULL");
6409   Node* subkeyH_start  = array_element_address(subkeyH, intcon(0), T_LONG);
6410   assert(subkeyH_start, "subkeyH is NULL");
6411   Node* data_start  = array_element_address(data, offset, T_BYTE);
6412   assert(data_start, "data is NULL");
6413 
6414   Node* ghash = make_runtime_call(RC_LEAF|RC_NO_FP,
6415                                   OptoRuntime::ghash_processBlocks_Type(),
6416                                   stubAddr, stubName, TypePtr::BOTTOM,
6417                                   state_start, subkeyH_start, data_start, len);
6418   return true;
6419 }
6420 
6421 //------------------------------inline_sha_implCompress-----------------------
6422 //
6423 // Calculate SHA (i.e., SHA-1) for single-block byte[] array.
6424 // void com.sun.security.provider.SHA.implCompress(byte[] buf, int ofs)
6425 //
6426 // Calculate SHA2 (i.e., SHA-244 or SHA-256) for single-block byte[] array.
6427 // void com.sun.security.provider.SHA2.implCompress(byte[] buf, int ofs)
6428 //
6429 // Calculate SHA5 (i.e., SHA-384 or SHA-512) for single-block byte[] array.
6430 // void com.sun.security.provider.SHA5.implCompress(byte[] buf, int ofs)
6431 //
6432 bool LibraryCallKit::inline_sha_implCompress(vmIntrinsics::ID id) {
6433   assert(callee()-&gt;signature()-&gt;size() == 2, "sha_implCompress has 2 parameters");
6434 
6435   Node* sha_obj = argument(0);
6436   Node* src     = argument(1); // type oop
6437   Node* ofs     = argument(2); // type int
6438 
6439   const Type* src_type = src-&gt;Value(&amp;_gvn);
6440   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6441   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6442     // failed array check
6443     return false;
6444   }
6445   // Figure out the size and type of the elements we will be copying.
6446   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6447   if (src_elem != T_BYTE) {
6448     return false;
6449   }
6450   // 'src_start' points to src array + offset
6451   Node* src_start = array_element_address(src, ofs, src_elem);
6452   Node* state = NULL;
6453   address stubAddr;
6454   const char *stubName;
6455 
6456   switch(id) {
6457   case vmIntrinsics::_sha_implCompress:
6458     assert(UseSHA1Intrinsics, "need SHA1 instruction support");
6459     state = get_state_from_sha_object(sha_obj);
6460     stubAddr = StubRoutines::sha1_implCompress();
6461     stubName = "sha1_implCompress";
6462     break;
6463   case vmIntrinsics::_sha2_implCompress:
6464     assert(UseSHA256Intrinsics, "need SHA256 instruction support");
6465     state = get_state_from_sha_object(sha_obj);
6466     stubAddr = StubRoutines::sha256_implCompress();
6467     stubName = "sha256_implCompress";
6468     break;
6469   case vmIntrinsics::_sha5_implCompress:
6470     assert(UseSHA512Intrinsics, "need SHA512 instruction support");
6471     state = get_state_from_sha5_object(sha_obj);
6472     stubAddr = StubRoutines::sha512_implCompress();
6473     stubName = "sha512_implCompress";
6474     break;
6475   default:
6476     fatal_unexpected_iid(id);
6477     return false;
6478   }
6479   if (state == NULL) return false;
6480 
6481   // Call the stub.
6482   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP, OptoRuntime::sha_implCompress_Type(),
6483                                  stubAddr, stubName, TypePtr::BOTTOM,
6484                                  src_start, state);
6485 
6486   return true;
6487 }
6488 
6489 //------------------------------inline_digestBase_implCompressMB-----------------------
6490 //
6491 // Calculate SHA/SHA2/SHA5 for multi-block byte[] array.
6492 // int com.sun.security.provider.DigestBase.implCompressMultiBlock(byte[] b, int ofs, int limit)
6493 //
6494 bool LibraryCallKit::inline_digestBase_implCompressMB(int predicate) {
6495   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6496          "need SHA1/SHA256/SHA512 instruction support");
6497   assert((uint)predicate &lt; 3, "sanity");
6498   assert(callee()-&gt;signature()-&gt;size() == 3, "digestBase_implCompressMB has 3 parameters");
6499 
6500   Node* digestBase_obj = argument(0); // The receiver was checked for NULL already.
6501   Node* src            = argument(1); // byte[] array
6502   Node* ofs            = argument(2); // type int
6503   Node* limit          = argument(3); // type int
6504 
6505   const Type* src_type = src-&gt;Value(&amp;_gvn);
6506   const TypeAryPtr* top_src = src_type-&gt;isa_aryptr();
6507   if (top_src  == NULL || top_src-&gt;klass()  == NULL) {
6508     // failed array check
6509     return false;
6510   }
6511   // Figure out the size and type of the elements we will be copying.
6512   BasicType src_elem = src_type-&gt;isa_aryptr()-&gt;klass()-&gt;as_array_klass()-&gt;element_type()-&gt;basic_type();
6513   if (src_elem != T_BYTE) {
6514     return false;
6515   }
6516   // 'src_start' points to src array + offset
6517   Node* src_start = array_element_address(src, ofs, src_elem);
6518 
6519   const char* klass_SHA_name = NULL;
6520   const char* stub_name = NULL;
6521   address     stub_addr = NULL;
6522   bool        long_state = false;
6523 
6524   switch (predicate) {
6525   case 0:
6526     if (UseSHA1Intrinsics) {
6527       klass_SHA_name = "sun/security/provider/SHA";
6528       stub_name = "sha1_implCompressMB";
6529       stub_addr = StubRoutines::sha1_implCompressMB();
6530     }
6531     break;
6532   case 1:
6533     if (UseSHA256Intrinsics) {
6534       klass_SHA_name = "sun/security/provider/SHA2";
6535       stub_name = "sha256_implCompressMB";
6536       stub_addr = StubRoutines::sha256_implCompressMB();
6537     }
6538     break;
6539   case 2:
6540     if (UseSHA512Intrinsics) {
6541       klass_SHA_name = "sun/security/provider/SHA5";
6542       stub_name = "sha512_implCompressMB";
6543       stub_addr = StubRoutines::sha512_implCompressMB();
6544       long_state = true;
6545     }
6546     break;
6547   default:
6548     fatal("unknown SHA intrinsic predicate: %d", predicate);
6549   }
6550   if (klass_SHA_name != NULL) {
6551     // get DigestBase klass to lookup for SHA klass
6552     const TypeInstPtr* tinst = _gvn.type(digestBase_obj)-&gt;isa_instptr();
6553     assert(tinst != NULL, "digestBase_obj is not instance???");
6554     assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6555 
6556     ciKlass* klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6557     assert(klass_SHA-&gt;is_loaded(), "predicate checks that this class is loaded");
6558     ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6559     return inline_sha_implCompressMB(digestBase_obj, instklass_SHA, long_state, stub_addr, stub_name, src_start, ofs, limit);
6560   }
6561   return false;
6562 }
6563 //------------------------------inline_sha_implCompressMB-----------------------
6564 bool LibraryCallKit::inline_sha_implCompressMB(Node* digestBase_obj, ciInstanceKlass* instklass_SHA,
6565                                                bool long_state, address stubAddr, const char *stubName,
6566                                                Node* src_start, Node* ofs, Node* limit) {
6567   const TypeKlassPtr* aklass = TypeKlassPtr::make(instklass_SHA);
6568   const TypeOopPtr* xtype = aklass-&gt;as_instance_type();
6569   Node* sha_obj = new CheckCastPPNode(control(), digestBase_obj, xtype);
6570   sha_obj = _gvn.transform(sha_obj);
6571 
6572   Node* state;
6573   if (long_state) {
6574     state = get_state_from_sha5_object(sha_obj);
6575   } else {
6576     state = get_state_from_sha_object(sha_obj);
6577   }
6578   if (state == NULL) return false;
6579 
6580   // Call the stub.
6581   Node* call = make_runtime_call(RC_LEAF|RC_NO_FP,
6582                                  OptoRuntime::digestBase_implCompressMB_Type(),
6583                                  stubAddr, stubName, TypePtr::BOTTOM,
6584                                  src_start, state, ofs, limit);
6585   // return ofs (int)
6586   Node* result = _gvn.transform(new ProjNode(call, TypeFunc::Parms));
6587   set_result(result);
6588 
6589   return true;
6590 }
6591 
6592 //------------------------------get_state_from_sha_object-----------------------
6593 Node * LibraryCallKit::get_state_from_sha_object(Node *sha_object) {
6594   Node* sha_state = load_field_from_object(sha_object, "state", "[I", /*is_exact*/ false);
6595   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA/SHA2");
6596   if (sha_state == NULL) return (Node *) NULL;
6597 
6598   // now have the array, need to get the start address of the state array
6599   Node* state = array_element_address(sha_state, intcon(0), T_INT);
6600   return state;
6601 }
6602 
6603 //------------------------------get_state_from_sha5_object-----------------------
6604 Node * LibraryCallKit::get_state_from_sha5_object(Node *sha_object) {
6605   Node* sha_state = load_field_from_object(sha_object, "state", "[J", /*is_exact*/ false);
6606   assert (sha_state != NULL, "wrong version of sun.security.provider.SHA5");
6607   if (sha_state == NULL) return (Node *) NULL;
6608 
6609   // now have the array, need to get the start address of the state array
6610   Node* state = array_element_address(sha_state, intcon(0), T_LONG);
6611   return state;
6612 }
6613 
6614 //----------------------------inline_digestBase_implCompressMB_predicate----------------------------
6615 // Return node representing slow path of predicate check.
6616 // the pseudo code we want to emulate with this predicate is:
6617 //    if (digestBaseObj instanceof SHA/SHA2/SHA5) do_intrinsic, else do_javapath
6618 //
6619 Node* LibraryCallKit::inline_digestBase_implCompressMB_predicate(int predicate) {
6620   assert(UseSHA1Intrinsics || UseSHA256Intrinsics || UseSHA512Intrinsics,
6621          "need SHA1/SHA256/SHA512 instruction support");
6622   assert((uint)predicate &lt; 3, "sanity");
6623 
6624   // The receiver was checked for NULL already.
6625   Node* digestBaseObj = argument(0);
6626 
6627   // get DigestBase klass for instanceOf check
6628   const TypeInstPtr* tinst = _gvn.type(digestBaseObj)-&gt;isa_instptr();
6629   assert(tinst != NULL, "digestBaseObj is null");
6630   assert(tinst-&gt;klass()-&gt;is_loaded(), "DigestBase is not loaded");
6631 
6632   const char* klass_SHA_name = NULL;
6633   switch (predicate) {
6634   case 0:
6635     if (UseSHA1Intrinsics) {
6636       // we want to do an instanceof comparison against the SHA class
6637       klass_SHA_name = "sun/security/provider/SHA";
6638     }
6639     break;
6640   case 1:
6641     if (UseSHA256Intrinsics) {
6642       // we want to do an instanceof comparison against the SHA2 class
6643       klass_SHA_name = "sun/security/provider/SHA2";
6644     }
6645     break;
6646   case 2:
6647     if (UseSHA512Intrinsics) {
6648       // we want to do an instanceof comparison against the SHA5 class
6649       klass_SHA_name = "sun/security/provider/SHA5";
6650     }
6651     break;
6652   default:
6653     fatal("unknown SHA intrinsic predicate: %d", predicate);
6654   }
6655 
6656   ciKlass* klass_SHA = NULL;
6657   if (klass_SHA_name != NULL) {
6658     klass_SHA = tinst-&gt;klass()-&gt;as_instance_klass()-&gt;find_klass(ciSymbol::make(klass_SHA_name));
6659   }
6660   if ((klass_SHA == NULL) || !klass_SHA-&gt;is_loaded()) {
6661     // if none of SHA/SHA2/SHA5 is loaded, we never take the intrinsic fast path
6662     Node* ctrl = control();
6663     set_control(top()); // no intrinsic path
6664     return ctrl;
6665   }
6666   ciInstanceKlass* instklass_SHA = klass_SHA-&gt;as_instance_klass();
6667 
6668   Node* instofSHA = gen_instanceof(digestBaseObj, makecon(TypeKlassPtr::make(instklass_SHA)));
6669   Node* cmp_instof = _gvn.transform(new CmpINode(instofSHA, intcon(1)));
6670   Node* bool_instof = _gvn.transform(new BoolNode(cmp_instof, BoolTest::ne));
6671   Node* instof_false = generate_guard(bool_instof, NULL, PROB_MIN);
6672 
6673   return instof_false;  // even if it is NULL
6674 }
6675 
6676 bool LibraryCallKit::inline_profileBoolean() {
6677   Node* counts = argument(1);
6678   const TypeAryPtr* ary = NULL;
6679   ciArray* aobj = NULL;
6680   if (counts-&gt;is_Con()
6681       &amp;&amp; (ary = counts-&gt;bottom_type()-&gt;isa_aryptr()) != NULL
6682       &amp;&amp; (aobj = ary-&gt;const_oop()-&gt;as_array()) != NULL
6683       &amp;&amp; (aobj-&gt;length() == 2)) {
6684     // Profile is int[2] where [0] and [1] correspond to false and true value occurrences respectively.
6685     jint false_cnt = aobj-&gt;element_value(0).as_int();
6686     jint  true_cnt = aobj-&gt;element_value(1).as_int();
6687 
6688     if (C-&gt;log() != NULL) {
6689       C-&gt;log()-&gt;elem("observe source='profileBoolean' false='%d' true='%d'",
6690                      false_cnt, true_cnt);
6691     }
6692 
6693     if (false_cnt + true_cnt == 0) {
6694       // According to profile, never executed.
6695       uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6696                           Deoptimization::Action_reinterpret);
6697       return true;
6698     }
6699 
6700     // result is a boolean (0 or 1) and its profile (false_cnt &amp; true_cnt)
6701     // is a number of each value occurrences.
6702     Node* result = argument(0);
6703     if (false_cnt == 0 || true_cnt == 0) {
6704       // According to profile, one value has been never seen.
6705       int expected_val = (false_cnt == 0) ? 1 : 0;
6706 
6707       Node* cmp  = _gvn.transform(new CmpINode(result, intcon(expected_val)));
6708       Node* test = _gvn.transform(new BoolNode(cmp, BoolTest::eq));
6709 
6710       IfNode* check = create_and_map_if(control(), test, PROB_ALWAYS, COUNT_UNKNOWN);
6711       Node* fast_path = _gvn.transform(new IfTrueNode(check));
6712       Node* slow_path = _gvn.transform(new IfFalseNode(check));
6713 
6714       { // Slow path: uncommon trap for never seen value and then reexecute
6715         // MethodHandleImpl::profileBoolean() to bump the count, so JIT knows
6716         // the value has been seen at least once.
6717         PreserveJVMState pjvms(this);
6718         PreserveReexecuteState preexecs(this);
6719         jvms()-&gt;set_should_reexecute(true);
6720 
6721         set_control(slow_path);
6722         set_i_o(i_o());
6723 
6724         uncommon_trap_exact(Deoptimization::Reason_intrinsic,
6725                             Deoptimization::Action_reinterpret);
6726       }
6727       // The guard for never seen value enables sharpening of the result and
6728       // returning a constant. It allows to eliminate branches on the same value
6729       // later on.
6730       set_control(fast_path);
6731       result = intcon(expected_val);
6732     }
6733     // Stop profiling.
6734     // MethodHandleImpl::profileBoolean() has profiling logic in its bytecode.
6735     // By replacing method body with profile data (represented as ProfileBooleanNode
6736     // on IR level) we effectively disable profiling.
6737     // It enables full speed execution once optimized code is generated.
6738     Node* profile = _gvn.transform(new ProfileBooleanNode(result, false_cnt, true_cnt));
6739     C-&gt;record_for_igvn(profile);
6740     set_result(profile);
6741     return true;
6742   } else {
6743     // Continue profiling.
6744     // Profile data isn't available at the moment. So, execute method's bytecode version.
6745     // Usually, when GWT LambdaForms are profiled it means that a stand-alone nmethod
6746     // is compiled and counters aren't available since corresponding MethodHandle
6747     // isn't a compile-time constant.
6748     return false;
6749   }
6750 }
6751 
6752 bool LibraryCallKit::inline_isCompileConstant() {
6753   Node* n = argument(0);
6754   set_result(n-&gt;is_Con() ? intcon(1) : intcon(0));
6755   return true;
6756 }
6757 
6758 bool LibraryCallKit::inline_deoptimize() {
6759   assert(WhiteBoxAPI, "");
6760   PreserveReexecuteState preexecs(this);
6761   jvms()-&gt;set_should_reexecute(false);
6762   uncommon_trap(Deoptimization::Reason_intrinsic,
6763                 Deoptimization::Action_none);
6764   return true;
6765 }
</pre></body></html>
